{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Triplestore wrapper for Python providing a simple and consistent interface to a range of triplestore backends</p>"},{"location":"#tripper","title":"Tripper","text":""},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Tutorial</li> <li>Documentation</li> <li>Reference manual</li> </ul>"},{"location":"#basic-concepts","title":"Basic concepts","text":"<p>Tripper provides a simple and consistent interface to a range of triplestore backends. It strives for simplicity and is modelled after rdflib (with a few simplifications).</p> <p>In Tripper:</p> <ul> <li> <p>All IRIs are represented by Python strings.   Example: <code>https://w3id.org/emmo#Metre</code></p> </li> <li> <p>Blank nodes are strings starting with <code>_:</code>.   Example: <code>_:bnode1</code></p> </li> <li> <p>Literals are constructed with <code>tripper.Literal</code>.   Example: <code>tripper.Literal(3.14, datatype=XSD.float)</code></p> </li> </ul> <p>To make it easy to work with IRIs, provide Tripper a set of pre-defined namespaces, like <code>XSD.float</code>. New namespaces can be defined with the <code>tripper.Namespace</code> class.</p> <p>A triplestore wrapper is created with the <code>tripper.Triplestore</code> class.</p>"},{"location":"#sub-packages","title":"Sub-packages","text":"<p>Additional functionality beyond interfacing triplestore backends is provided by specialised sub-package:</p> <ul> <li>tripper.datadoc: An API for data documentation.</li> <li>tripper.units: Working with units and quantities defined in ontologies.</li> <li>tripper.mappings: Traverse mappings stored in the triplestore and find possible mapping routes.</li> <li>tripper.convert: Convert between RDF and other data representations.</li> </ul>"},{"location":"#available-backends","title":"Available backends","text":"<p>The following backends are currently available, either in Tripper or other packages.</p> Backend name Provided by Requirements Comment rdflib tripper rdflib In-memory rdflib triplestore supporting all features. ontopy tripper EMMOntoPy Backend for EMMOntoPy. In-memory. sparqlwrapper tripper sparqlwrapper Generic backend for all triplestores supported by sparqlwrapper. collection tripper DLite-Python Backend to a DLite collection. graphdb tripper sparqlwrapper Backend to GraphDB. fuseki PyBackTrip sparqlwrapper Backend to fuseki. stardog PyBackTrip sparqlwrapper,pystardog Backend to StarDog."},{"location":"#installation","title":"Installation","text":"<p>Tripper has by itself no dependencies outside the standard library, but the triplestore backends may have specific dependencies.</p> <p>The package can be installed from PyPI using <code>pip</code>. A minimal installation can be done with</p> <pre><code>pip install tripper\n</code></pre> <p>but typically, you would also like to install the requirements needed by the backends, the tripper sub-packages and extra features. The requirements for the backends are listed in the table above, while sub-packages and extra features are enabled by specifying one or more of the \"extras\" listed in the table below when installing Tripper.</p> <p>For example, the following command will install Tripper with the tripper.datadoc sub-package and the rdflib backend enabled:</p> <pre><code>pip install tripper[datadoc] rdflib\n</code></pre> <p>Developers should install Tripper with the <code>dev</code> extras and enable pre-commit:</p> <pre><code>pip install tripper[dev]\npre-commit install  # Enable pre-commit and installing hooks\n</code></pre> Extras Description of dependencies Implies units Required by the tripper.units sub-package. mappings Required by the tripper.mappings sub-package. units datadoc Required by the tripper.datadoc sub-package. mappings backends Installs dependencies for all backends testing For testing. datadoc,backends docs For generation of documentation testing dev For developers. Installs all dependencies, including pre-commit. dev"},{"location":"#license-and-copyright","title":"License and copyright","text":"<p>All files in this repository are licensed under the MIT license. If not stated otherwise in the top of the files, they have copyright \u00a9 2022 SINTEF.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We gratefully acknowledge the following projects for supporting the development of Tripper:</p> <ul> <li>OntoTrans (2020-2024) that receives funding from the European Union's Horizon 2020 Research and Innovation Programme, under Grant Agreement n. 862136.</li> <li>OpenModel (2021-2025) that receives funding from the European Union's Horizon 2020 Research and Innovation Programme, under Grant Agreement n. 953167.</li> <li>SFI PhysMet (2020-2028) funded by Forskningsr\u00e5det and Norwegian industry partners.</li> <li>DOME 4.0 (2021-2025) that receives funding from the European Union's Horizon 2020 Research and Innovation Programme, under Grant Agreement n. 953163.</li> <li>VIPCOAT (2021-2025) that receives funding from the European Union's Horizon 2020 Research and Innovation Programme, under Grant Agreement n. 952903.</li> <li>MEDIATE (2022-2025) that receives funding from the RCN, Norway; FNR, Luxenburg; SMWK Germany via the M-era.net programme, project 9557,</li> <li>MatCHMaker (2022-2026) that receives funding from the European Union's Horizon 2020 Research and Innovation Programme, under Grant Agreement n. 101091687.</li> <li>PINK (2024-2027) that receives funding from the European Union's Horizon 2020 Research and Innovation Programme, under Grant Agreement n. 101137809.</li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#unreleased-changes-2026-02-05","title":"Unreleased changes (2026-02-05)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Improve description of classes #460</li> </ul> <p>Merged pull requests:</p> <ul> <li>Annotation mappings for generated keywords  #497 (jesper-friis)</li> <li>Added --set-prefix option to keywords #496 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #495 (pre-commit-ci[bot])</li> <li>Fixed use of deprecated <code>yamlfile</code> argument to get_keywords() #494 (jesper-friis)</li> <li>Added prefix argument to keywords tool #492 (francescalb)</li> <li>[pre-commit.ci] pre-commit autoupdate #489 (pre-commit-ci[bot])</li> <li>[pre-commit.ci] pre-commit autoupdate #487 (pre-commit-ci[bot])</li> <li>Added reverse mapping to Namespace objects using the __call__() method. #483 (jesper-friis)</li> <li>Added --redefine option to datadoc tool and corrected Keywords._keywords_list() method #482 (jesper-friis)</li> <li>Update keywords for manufactured product #481 (jesper-friis)</li> <li>460 improve description of classes #479 (jesper-friis)</li> <li>Restore license #478 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #477 (pre-commit-ci[bot])</li> <li>Allow to filter generated markdown documentation on namespaces #476 (jesper-friis)</li> <li>Speedup #474 (jesper-friis)</li> <li>Make it possible to duplicate nested table headers #467 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v050-2025-11-22","title":"v0.5.0 (2025-11-22)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Add functions for storing/loading namespace prefixes and URIs to/from the KB #441</li> <li>Several tests are never run because of error in importsatetemtn fpr pyld #427</li> <li>Fix the safety workflow by creating a Safety account and add a key to the workflow #423</li> <li>Add methods to the Keywords class for adding new prefixes #422</li> <li>Why is the test wit the ontopy backend skipped? #334</li> </ul> <p>Merged pull requests:</p> <ul> <li>Fixed some bugs in Keywords  #473 (jesper-friis)</li> <li>Added missing keywords #470 (jesper-friis)</li> <li>Update keywords #469 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #468 (pre-commit-ci[bot])</li> <li>Remove default type for TableDoc #466 (jesper-friis)</li> <li>Allow keyword redefinition #465 (francescalb)</li> <li>[pre-commit.ci] pre-commit autoupdate #464 (pre-commit-ci[bot])</li> <li>Improved error message and fixed minor issue when reading data from PINK. #463 (jesper-friis)</li> <li>Ensure that SPARQL queries with the rdflib backend behaves similar to sparqlwrapper #456 (jesper-friis)</li> <li>Changed <code>check_url</code> to default to <code>base_iri</code> #455 (jesper-friis)</li> <li>Fixed some issues with sparql backend #453 (jesper-friis)</li> <li>Generate keywords file #451 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #450 (pre-commit-ci[bot])</li> <li>Added keyring tip #449 (jesper-friis)</li> <li>Fix deprecation warnings #448 (jesper-friis)</li> <li>Test against latest Python 3.14 instead of release candidate rc2 #447 (jesper-friis)</li> <li>Restore formatting #446 (jesper-friis)</li> <li>Fix formatting2 #445 (jesper-friis)</li> <li>Store prefixes to triplestore #443 (jesper-friis)</li> <li>Clean up keywords #442 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #439 (pre-commit-ci[bot])</li> <li>Added available() method and check_url to argument to Triplestore #437 (jesper-friis)</li> <li>Added substitute_query() function #434 (jesper-friis)</li> <li>Fixed minor bugs in keywords.py #432 (jesper-friis)</li> <li>Fix linting warnings for bound prefixes #431 (nameloCmaS)</li> <li>Corrected importorskip for PyLD (corrected to pyld) #428 (francescalb)</li> <li>[pre-commit.ci] pre-commit autoupdate #426 (pre-commit-ci[bot])</li> <li>Added add_prefix() method to keywords module. #424 (jesper-friis)</li> <li>Changed pypi status to production #418 (jesper-friis)</li> <li>Updated figure with basic relations #417 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v043-2025-08-19","title":"v0.4.3 (2025-08-19)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>inconsistent usage when filtering searches #396</li> <li>correct criterias to criteria #392</li> <li>Add <code>hasCurator</code> and <code>hasCurationDate</code> properties to resources #390</li> <li>Support for documenting not (yet) existing resources #369</li> <li>Make the generated keywords documentation look better #365</li> <li>Speed up tripper.dataset.save_data() #337</li> <li>Example in documentation of  search_iris iw wrong. #323</li> <li>get_jsonld_context does not support PosixPath #322</li> <li>Set up a triplestore on the PhysMet Portal #315</li> <li>Make converting a large table of datasets to RDF faster #305</li> <li>Ensure dataset is TEM schema compliant #293</li> <li>Define schema for TEM dataset queries #292</li> </ul> <p>Merged pull requests:</p> <ul> <li>Updated installation instructions on the readme page. #415 (jesper-friis)</li> <li>Add support for Python 3.14 #414 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #412 (pre-commit-ci[bot])</li> <li>Updated dataset figure #410 (jesper-friis)</li> <li>Include sparqlwrapper in dependencies for datadoc #409 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #408 (pre-commit-ci[bot])</li> <li>Updated figures #407 (jesper-friis)</li> <li>Date and time literals #406 (jesper-friis)</li> <li>Added keywords <code>hasCurator</code> and <code>hasCurationDate</code> #405 (jesper-friis)</li> <li>Build documentation with Python 3.12 #404 (jesper-friis)</li> <li>Skip running doctest on session.md if Fuseki is not running #403 (jesper-friis)</li> <li>Fix issue #396 #401 (jesper-friis)</li> <li>Updated figure with basic relations #400 (jesper-friis)</li> <li>Added __contains__() method to namespace #399 (jesper-friis)</li> <li>Run mkdocs with Python 3.13 instead of 3.9 #398 (jesper-friis)</li> <li>added a file about how to search and delete resources in the ts #397 (francescalb)</li> <li>Update using criteria in search #395 (francescalb)</li> <li>Keywords inverse #394 (jesper-friis)</li> <li>Keywords inverse #393 (jesper-friis)</li> <li>Added keywords for documenting characterisation processes #391 (jesper-friis)</li> <li>WIP: Update classes #388 (jesper-friis)</li> <li>Improved control when storing to the triplestore #387 (jesper-friis)</li> <li>Fix test_markdown_doctest #383 (jesper-friis)</li> <li>Added ureg.get_quantity() method to units module #381 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v042-2025-04-23","title":"v0.4.2 (2025-04-23)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Development branch that merges loose ends together #380 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v041-2025-04-19","title":"v0.4.1 (2025-04-19)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>ts.remove does not work in sparqlwrapper. #350</li> <li>exapand_iri, documentation formatted wrongly. #348</li> <li>Attrdict return non-expanded iris, is this correct? #341</li> <li>The CONSTRUCT sparql query in _load_sparql used in load_dict returns all datasets #336</li> <li>Adding text in graphDB trough save_datadoc is done incorrectly? #333</li> <li>datadoc.dataset._load_sparql uses a CONSTRUCT query, which is not implemented in the SPARQLWrapper #330</li> <li>Include the JSON-LD context in the installed package #326</li> <li>It is not possible to search for Literals in the graphdb backend #320</li> <li>Validator for schema #294</li> </ul> <p>Merged pull requests:</p> <ul> <li>Corrected DataSetvice to DataService #374 (francescalb)</li> <li>Fix typo in keywords documentation #373 (jesper-friis)</li> <li>Updated keywords documentation #371 (jesper-friis)</li> <li>Adding logos #368 (jesper-friis)</li> <li>Add resources (classes) to the JSON-LD context. #366 (jesper-friis)</li> <li>Added support for ASK and DESCRIBE queries in the sparqlwrapper backend #364 (jesper-friis)</li> <li>Improved formatting of keyword documentation table. #362 (jesper-friis)</li> <li>Test for the datadoc command-line tool #361 (jesper-friis)</li> <li>Fixed CONTEXT_URL after branch 294-validator-for-schema was merged to master #359 (jesper-friis)</li> <li>Fix formatting #358 (jesper-friis)</li> <li>Added session #357 (jesper-friis)</li> <li>Added update() method to sparqlwrapper backend #356 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #355 (pre-commit-ci[bot])</li> <li>[pre-commit.ci] pre-commit autoupdate #353 (pre-commit-ci[bot])</li> <li>Corrected sparqlquery for removing triples #351 (francescalb)</li> <li>Corrected formatting of example #349 (francescalb)</li> <li>Added test for Fuseki and fixed query to return TURTLE which works for both fuseki and graphdb #347 (francescalb)</li> <li>[pre-commit.ci] pre-commit autoupdate #346 (pre-commit-ci[bot])</li> <li>Correct syntax #344 (jesper-friis)</li> <li>Correct syntax #343 (jesper-friis)</li> <li>Flb/expand iri in sparql query #340 (francescalb)</li> <li>disable pylint errors on too-many-positional-arguments #339 (francescalb)</li> <li>Fixed CONSTRUCT to return s and not o #338 (francescalb)</li> <li>Corrected literals in SELECT and CONSTRUCT queries in the sparqlwrapper backend #335 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #332 (pre-commit-ci[bot])</li> <li>Added CONSTRUCT query to sparqlwrapper,  #331 (francescalb)</li> <li>Include context files in installation #328 (jesper-friis)</li> <li>Added PINK to contributing projects #327 (jesper-friis)</li> <li>Update dataset.py #325 (jesper-friis)</li> <li>Added units submodule for working with units and quantities defined in ontologies #324 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #321 (pre-commit-ci[bot])</li> <li>Schema validator #296 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v040-2025-02-10","title":"v0.4.0 (2025-02-10)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Clean up documentation #302</li> <li>Make datadoc windows compatible #301</li> </ul> <p>Merged pull requests:</p> <ul> <li>[pre-commit.ci] pre-commit autoupdate #317 (pre-commit-ci[bot])</li> <li>Add updateEndpoint to SPARQLwrapper #313 (torhaugl)</li> <li>Updated the readme file #311 (jesper-friis)</li> <li>Drop support for Python 3.7 #310 (jesper-friis)</li> <li>Include prefixes from context when populating a triplestore from a csv file #309 (jesper-friis)</li> <li>Added support for username/password for sparqlwrapper #308 (jesper-friis)</li> <li>Implementing regex search #307 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #306 (pre-commit-ci[bot])</li> <li>Rename dataset #304 (jesper-friis)</li> <li>Change pathlib to be pathlib and not as uri. This fixes opening in wi\u2026 #303 (francescalb)</li> <li>Added fixes for the demonstration at the PINK demo #299 (jesper-friis)</li> <li>Renamed the dataset subpackage to datadoc #298 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #287 (pre-commit-ci[bot])</li> <li>Make datasets with a datamodel an individual of the datamodel #286 (jesper-friis)</li> <li>Re-applied fixes from testing datadoc on a use case for PINK. #285 (jesper-friis)</li> <li>Command-line datadoc script #281 (jesper-friis)</li> <li>Added documentation for datasets #280 (jesper-friis)</li> <li>Dataset TODOs #279 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #277 (pre-commit-ci[bot])</li> <li>Added csv parser to Tabledoc  #275 (jesper-friis)</li> <li>Updated dataset figures #274 (jesper-friis)</li> <li>New TableDoc class providing a table interface for data documentation #273 (jesper-friis)</li> <li>Updated dataset module #272 (jesper-friis)</li> <li>Corrected dataset module #271 (jesper-friis)</li> <li>Add query to SPARQLwrapper strategy #269 (torhaugl)</li> <li>[pre-commit.ci] pre-commit autoupdate #268 (pre-commit-ci[bot])</li> <li>Add support for Python 3.13 #266 (jesper-friis)</li> <li>Updated requirements to allow Pint 0.24, which in turn opens for NumPy 2 #265 (jesper-friis)</li> <li>Updated documentation format for better rendering #262 (jesper-friis)</li> <li>Predefined EMMO namespace with checking and label lookup #261 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #259 (pre-commit-ci[bot])</li> <li>[pre-commit.ci] pre-commit autoupdate #258 (pre-commit-ci[bot])</li> <li>Dataset #256 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v034-2024-10-17","title":"v0.3.4 (2024-10-17)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Allowing backends to indicate whether they prefer the sparql interface #255 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #254 (pre-commit-ci[bot])</li> <li>Update n3 format for string with quotes. #253 (pintr)</li> <li>Added option to Triplestore.value() to return a generator over all matching values #252 (jesper-friis)</li> <li>Ignore safety vulnerabeility 72715 #251 (jesper-friis)</li> <li>Fix empty prefix #250 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #249 (pre-commit-ci[bot])</li> <li>Backend info to triplestore instance #248 (jesper-friis)</li> <li>Added argument <code>ignore_unrecognised</code> to tripper.convert() #247 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #244 (pre-commit-ci[bot])</li> </ul>"},{"location":"CHANGELOG/#v033-2024-08-26","title":"v0.3.3 (2024-08-26)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Use Trusted Publishers for PyPI #242</li> </ul> <p>Closed issues:</p> <ul> <li>Add instructions of how to release a new version #47</li> <li>Update Python API references in README #22</li> </ul> <p>Merged pull requests:</p> <ul> <li>Use Trusted Publishers on PyPI #243 (CasperWA)</li> </ul>"},{"location":"CHANGELOG/#v032-2024-08-19","title":"v0.3.2 (2024-08-19)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Minor fix for documentation layout  #238 (jesper-friis)</li> <li>Updated documentation navigation #237 (jesper-friis)</li> <li>Updated cd_release workflow #236 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v031-2024-08-16","title":"v0.3.1 (2024-08-16)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Avoid that tripper.Namespace crashes if the cache directory cannot be accessed #235 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #232 (pre-commit-ci[bot])</li> <li>[pre-commit.ci] pre-commit autoupdate #231 (pre-commit-ci[bot])</li> <li>Cleaned up backend tests #230 (jesper-friis)</li> <li>Added a fix for parsing rdflib literals. #229 (jesper-friis)</li> <li>Add better and more convenient support for rdf:JSON datatype #228 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v030-2024-06-24","title":"v0.3.0 (2024-06-24)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Tutorial update #227 (jesper-friis)</li> <li>Get rid of the last warning in the tests #226 (jesper-friis)</li> <li>Get restrictions as dicts #225 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #224 (pre-commit-ci[bot])</li> <li>Added an additional test for convert #223 (jesper-friis)</li> <li>Minor fix in tutorial #221 (jesper-friis)</li> <li>Updated the tutorial #220 (jesper-friis)</li> <li>Improved the handling of different return types from the query() method #219 (jesper-friis)</li> <li>Correct parsing literals from the rdflib backend. #217 (jesper-friis)</li> <li>Do not save cache while interpreter shotdown. #216 (jesper-friis)</li> <li>Allow string literal to compare equal to strings. #215 (jesper-friis)</li> <li>Also test for Python 3.12 #213 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v0216-2024-05-16","title":"v0.2.16 (2024-05-16)","text":"<p>Full Changelog</p> <p>Fixed bugs:</p> <ul> <li>cannot release #211</li> </ul> <p>Closed issues:</p> <ul> <li>Add caching and extension to tripper.Namespace #194</li> <li>Add support for adding and searching for restrictions #189</li> </ul> <p>Merged pull requests:</p> <ul> <li>In tripper.convert, commented out recognised keys for oteapi strategies #212 (jesper-friis)</li> <li>Use latest SINTEF/ci-cd version #210 (CasperWA)</li> <li>Allow prefix with digits #209 (jesper-friis)</li> <li>Added test for SPARQL CONSTRUCT query via tripper #207 (jesper-friis)</li> <li>Added more recognised keys to tripper.convert #206 (jesper-friis)</li> <li>[pre-commit.ci] pre-commit autoupdate #204 (pre-commit-ci[bot])</li> <li>Changed EMMO representation of function arguments from datasets to datum #203 (jesper-friis)</li> <li>Added comment suggested by Francesca #201 (jesper-friis)</li> <li>Enhanced the use of namespaces #195 (jesper-friis)</li> <li>Support for restrictions #190 (jesper-friis)</li> <li>Correctly convert rdflib bnodes back to tripper #187 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v0215-2024-03-12","title":"v0.2.15 (2024-03-12)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Documentation CD (still) fails due to old python version #193</li> <li>Publish workflow fails because it uses python 3.7 #191</li> <li>Allow untyped literals #182</li> <li>Search with ts.triples() doesn't recognise literals #179</li> <li>PyBackTrip - external tripper backends available #177</li> <li>Surprising literal comparisons #161</li> <li>Retain literal types in collection backend #160</li> <li>Document implemented backends  #157</li> </ul> <p>Merged pull requests:</p> <ul> <li>Loosen req on pint to include 0.23 (newest) #198 (francescalb)</li> <li>Use Python 3.9 in all CI/CD workflows #196 (CasperWA)</li> <li>Bump basic CI tests and CD to python 3.9 #192 (ajeklund)</li> <li>Cleaned up PR template #188 (jesper-friis)</li> <li>Bump mkdocs-material version for security #186 (ajeklund)</li> <li>Allow untyped literals #184 (jesper-friis)</li> <li>Added test for finding literal triples #183 (jesper-friis)</li> <li>Added reference to PyBackTrip #180 (jesper-friis)</li> <li>Added support for xsd:nonNegativeInteger literals #178 (jesper-friis)</li> <li>Added acknowledgements to readme file. #173 (jesper-friis)</li> <li>Retain literal types in collection backend #165 (jesper-friis)</li> <li>Improved comparing literals #164 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v0214-2024-01-25","title":"v0.2.14 (2024-01-25)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Literals are lost when listing triples with rdflib #162</li> <li>dependencies are too strict, at least for pint #149</li> <li>Function repo plugin system #128</li> </ul> <p>Merged pull requests:</p> <ul> <li>Retain datatype when listing literals from rdflib #163 (jesper-friis)</li> <li>Make it possible to expose an existing rdflib graph via tripper #156 (jesper-friis)</li> <li>Mapping function plugin system #152 (jesper-friis)</li> <li>Get rid of confusing UnusedArgumentWarning from working code #151 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v0213-2023-11-14","title":"v0.2.13 (2023-11-14)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Added UnknownUnitError #153 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v0212-2023-11-07","title":"v0.2.12 (2023-11-07)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Expand pint requirements to include more versions #150 (francescalb)</li> </ul>"},{"location":"CHANGELOG/#v0211-2023-10-30","title":"v0.2.11 (2023-10-30)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Added new section to README file #148 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v0210-2023-10-19","title":"v0.2.10 (2023-10-19)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Francescalb/testing dependencies #142 (francescalb)</li> <li>Clearified that tripper does not depend on DLite and Pydantic #136 (jesper-friis)</li> <li>Cleaned up mappings module #132 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v029-2023-09-29","title":"v0.2.9 (2023-09-29)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v028-2023-09-12","title":"v0.2.8 (2023-09-12)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Added support for lists in tripper.convert #129 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v027-2023-08-29","title":"v0.2.7 (2023-08-29)","text":"<p>Full Changelog</p> <p>Fixed bugs:</p> <ul> <li>Fix issues related to renaming of the default branch #116</li> </ul> <p>Merged pull requests:</p> <ul> <li>Added a module for converting to/from dicts #126 (jesper-friis)</li> <li>Update dependabot after changing master branch to 'master' #121 (jesper-friis)</li> <li>Also run CI tests on examples in documentation #118 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v026-2023-06-23","title":"v0.2.6 (2023-06-23)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Add CI/CD tests for Python 3.11 #102</li> </ul> <p>Merged pull requests:</p> <ul> <li>One letter words where not allowed in mappings, now they are #117 (francescalb)</li> <li>pylint settings #115 (jesper-friis)</li> <li>Fix issue with entry points for Python 3.6 and 3.7 #113 (jesper-friis)</li> <li>Added DOI badge #109 (jesper-friis)</li> <li>Fixed parsing rdf:HTML literals with the rdflib backend #106 (jesper-friis)</li> <li>Support Python 3.11 #103 (jesper-friis)</li> <li>92 new triplestore subclass #99 (alfredoisg)</li> </ul>"},{"location":"CHANGELOG/#v025-2023-05-24","title":"v0.2.5 (2023-05-24)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v024-2023-04-30","title":"v0.2.4 (2023-04-30)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Add entrypoint system to link external backend implementation #63</li> </ul> <p>Closed issues:</p> <ul> <li>Transformations based on data sources #90</li> <li>Add workflow example  #79</li> </ul> <p>Merged pull requests:</p> <ul> <li>Added argument <code>lang</code> to triplestore.value() #104 (jesper-friis)</li> <li>Run doctest from CI tests #101 (jesper-friis)</li> <li>Added add_data(), get_value() and add_interpolation_source() methods to Triplestore #91 (jesper-friis)</li> <li>Added tests for Python 3.11 and 3.6 #84 (jesper-friis)</li> <li>Remove some deprecation warnings #83 (jesper-friis)</li> <li>Workflow example #81 (jesper-friis)</li> <li>Support external backends #80 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v023-2023-02-05","title":"v0.2.3 (2023-02-05)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Add a to_yaml() method to MappingStep #66</li> </ul> <p>Merged pull requests:</p> <ul> <li>Add official support for Python 3.11 #82 (jesper-friis)</li> <li>added PR template #77 (alfredoisg)</li> </ul>"},{"location":"CHANGELOG/#v022-2023-01-30","title":"v0.2.2 (2023-01-30)","text":"<p>Full Changelog</p> <p>Fixed bugs:</p> <ul> <li>pre-commit failing hook installation #75</li> <li>ontopy backend failing tests #7</li> </ul> <p>Closed issues:</p> <ul> <li>Describe functions with EMMO instead of FnO #65</li> </ul> <p>Merged pull requests:</p> <ul> <li>Using isort 5.12.0 for pre-commit #76 (CasperWA)</li> <li>Visualise #74 (jesper-friis)</li> <li>Generate mapping routes from subclasses of Value and MappingStep #73 (jesper-friis)</li> <li>Fix deprecated calls syntax to Triplestore.triples() #71 (jesper-friis)</li> <li>Made the value optional + added some cleanup #70 (jesper-friis)</li> <li>Add map() method to Triplestore #69 (jesper-friis)</li> <li>Proper cost function #68 (jesper-friis)</li> <li>Updated Triplestore.add_function() to also support EMMO. #67 (jesper-friis)</li> <li>Added mappings #62 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v021-2022-12-18","title":"v0.2.1 (2022-12-18)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Simplify use of the Triplestore triples() and remove() methods #50</li> </ul> <p>Merged pull requests:</p> <ul> <li>Commented out __hash__() and __eq__() methods from Literal. #55 (jesper-friis)</li> <li>Simplify use of the Triplestore triples() and remove() methods #51 (jesper-friis)</li> <li>Separated <code>base_iri</code> argument from <code>triplestore_url</code> in rdflib backend #49 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v020-2022-12-13","title":"v0.2.0 (2022-12-13)","text":"<p>Full Changelog</p> <p>Fixed bugs:</p> <ul> <li>Deploy docs failing due to wrong dependency installation #53</li> </ul> <p>Closed issues:</p> <ul> <li>Remove backend packages from requirements #48</li> <li>Fix utils.parse_object()  #45</li> </ul> <p>Merged pull requests:</p> <ul> <li>Update <code>docs</code> extra #54 (CasperWA)</li> <li>Remove backend packages from requirements #52 (jesper-friis)</li> <li>Fix utils.parse_object() #46 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v012-2022-12-11","title":"v0.1.2 (2022-12-11)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>The return value of <code>query()</code> depends on the query #42</li> <li>Add DLite collection backend #41</li> <li>Implement create_database() and remove_database() methods #34</li> <li>SPARQLWrapper backend #10</li> <li>Refactor triplestore.py (triplestore module API) #3</li> </ul> <p>Closed issues:</p> <ul> <li>Revert to using the proper general version for SINTEF/ci-cd #35</li> <li>Write in type annotations #33</li> <li>Improve README file #18</li> <li>Add support for simple persistent storage in the rdflib backend #14</li> </ul> <p>Merged pull requests:</p> <ul> <li>Added collection backend #44 (jesper-friis)</li> <li>Fix return types #43 (CasperWA)</li> <li>Updated import statements in the tutorial #40 (jesper-friis)</li> <li>Added create_database() and remove_database() methods. #39 (jesper-friis)</li> <li>Use the proper version of SINTEF/ci-cd #36 (CasperWA)</li> <li>bumped ci-cd version to remove --strict mkdocs command #32 (daniel-sintef)</li> <li>3 refactor triplestorepy triplestore module api #31 (daniel-sintef)</li> <li>3 refactor triplestore #27 (jesper-friis)</li> <li>Corrected copyright #21 (jesper-friis)</li> <li>Add a useful description to README file. #19 (jesper-friis)</li> <li>Added support for simple persistent storage in the rdflib backend #15 (jesper-friis)</li> <li>sparqlwrapper backend #11 (jesper-friis)</li> <li>Documented return value of the Triplestore.query() and added a test for it #9 (jesper-friis)</li> </ul>"},{"location":"CHANGELOG/#v011-2022-10-13","title":"v0.1.1 (2022-10-13)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Change package name to <code>tripper</code> on PyPI #13</li> <li>Clean up newly initialized Python API #1</li> </ul> <p>Fixed bugs:</p> <ul> <li>Enable proper release workflow #8</li> <li>Fix workflows so they succeed #2</li> </ul> <p>Merged pull requests:</p> <ul> <li>Use the package name <code>tripper</code> (not <code>tripperpy</code>) #16 (CasperWA)</li> <li>Clean up repository &amp; fix workflows #5 (CasperWA)</li> <li>Added version number in __init__ to enable local pip install #4 (quaat)</li> </ul> <p>* This Changelog was automatically generated by github_changelog_generator</p>"},{"location":"LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright (c) 2022-2025 SINTEF</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"backend_discovery/","title":"Discovery of custom backends","text":"<p>A tripper backend is a normal importable Python module that defines the class <code>{Name}Strategy</code>, where <code>{Name}</code> is the name of the backend with the first letter capitalized (as it would be with the <code>str.title()</code> method). The methods they are supposed to define are documented in tripper/interface.py.</p> <p>Tripper support several use cases for discovery of custom backends.</p>"},{"location":"backend_discovery/#installed-backend-package","title":"Installed backend package","text":"<p>It is possible to create a pip installable Python package that provides new tripper backends that will be automatically discovered.</p> <p>The backend package should add the following to its <code>pyproject.toml</code> file:</p> <pre><code>[project.entry-points.\"tripper.backends\"]\nmybackend1 = \"subpackage.mybackend1\"\nmybackend2 = \"subpackage.mybackend2\"\n</code></pre> <p>When your package is installed, this would make <code>mybackend1</code> and <code>mybackend2</code> automatically discovarable by tripper, such that you can write</p> <pre><code>&gt;&gt;&gt; from tripper import Triplestore\n&gt;&gt;&gt; ts = Triplestore(backend=\"mybackend1\")  # doctest: +SKIP\n</code></pre>"},{"location":"backend_discovery/#backend-module","title":"Backend module","text":"<p>If you have a tripper backend that is specific to your application, or that you for some other reason don't want or feel the need to publish as a separate Python package, you can keep the backend as a module within your application.</p> <p>In this case you have two options, either specify explicitly backend module when you instantiate your triplestore or append your package to the <code>tripper.backend_packages</code> module variable:</p>"},{"location":"backend_discovery/#instantiate-triplestore-with-explicit-module-path","title":"Instantiate triplestore with explicit module path","text":"<p>An explicit module path can either be absolute or relative as shown in the example below:</p> <pre><code># Absolute\nts = Triplestore(backend=\"mypackage.backends.mybackend\")\n\n# Relative to the `package` argument\nts = Triplestore(backend=\"backends.mybackend\", package=\"mypackage\")\n</code></pre> <p>A backend is considered to be specified explicitly if the <code>backend</code> argument contain a dot (.) or if the <code>package</code> argument is provided.</p>"},{"location":"backend_discovery/#append-to-tripperbackend_packages","title":"Append to <code>tripper.backend_packages</code>","text":"<p>Finally you can insert/append the sub-package with your backend to the <code>tripper.backend_packages</code> list module variable:</p> <pre><code>import tripper\ntripper.backend_packages.append(\"mypackage.backends\")\nts = Triplestore(backend=\"mybackend\")\n</code></pre>"},{"location":"backend_discovery/#search-order-for-backends","title":"Search order for backends","text":"<p>Tripper backends are looked up in the following order: 1. explicit specified backend modules 2. backend packages 3. checking <code>tripper.backend_packages</code></p> <p>By default the built-in backends are looked up as the first element in <code>tripper.backend_packages</code> (but it is possible for the insert a custom backend sub-package before it). This means that backend packages are looked up before the built-in backends. Hence it is possible for a backend package to overwrite or extend a built-in backend.</p>"},{"location":"developers/","title":"For developers","text":""},{"location":"developers/#adding-new-backends","title":"Adding new backends","text":"<p>See interface.py, which defines the interface of a backend and may serve as a template for creating new backends.</p>"},{"location":"developers/#setting-up-local-graphdb-and-fuseki-services-for-testing","title":"Setting up local GraphDB and Fuseki services for testing","text":"<p>Tripper comes with an inbuilt backend to the SPARQLWrapper. In order to test this properly a real triplestore is needed. This is not done in the automatic workflows on github. However, local graphDB and Fuseki services can be setup as described below and tested with <code>tests/backends/test_sparqlwrapper_graphdb_fuseki.py</code>.</p> <p>The backend configurations corresponding to the local GraphDB and Fuseki services can be found in <code>[tests/input/session.yaml]</code>.</p>"},{"location":"developers/#setting-up-graphdb-service","title":"Setting up GraphDB service","text":"<p>To create the local instance of graphdb: <pre><code>docker pull ontotext/graphdb:10.8.3 # latest tag 17.02.2025\ndocker run -d -p 7200:7200 --name graphdb ontotext/graphdb:10.8.3\n</code></pre></p> <p>Then go to http://localhost:7200/ in your browser. You can add a new repository by pressing <code>create new reposotory</code> in the bottom right corner. Choose <code>GraphDB Reposotory</code> and write \"test_repo\" as repository ID. Tick off <code>Enable full-text search</code> and leave the rest as predefined. Click <code>Create</code>.</p> <p>Go to <code>Setup</code> and select <code>Repositories</code>. Activating \"test_repo\" by clicking the pin icon (Set as default repository) and then the restart icon (Restart repository test_repo).</p> <p>You can now run the test test_sparqlwrapper_graphdb_fuseki.py with graphdb.</p> <p>Note that if the graphdb instance is not found the test will just be skipped.</p>"},{"location":"developers/#setting-up-fuseki-service","title":"Setting up Fuseki service","text":"<p>Similarly a jena-fuseki instance can be tested locally as follows:</p> <pre><code>docker pull stain/jena-fuseki\ndocker run -d --name fuseki -p 3030:3030 -e ADMIN_PASSWORD=admin0 -e=FUSEKI_DATASET_1=test_repo stain/jena-fuseki\n</code></pre> <p>You can now run the test <code>test_sparqlwrapper_graphdb_fuseki.py</code> with fuseki.</p> <p>Note that if the fuseki instance is not found the test will just be skipped.</p>"},{"location":"developers/#creating-new-release","title":"Creating new release","text":"<p>To create a new release, it is good to have a release summary.</p> <p>To add this, create a milestone that matches the new version and tag, e.g., <code>v1.0.8</code>.</p> <p>Then create a new issue, adding it to the milestone and add the <code>release-summary</code> label.</p> <p>For the issue description, write the actual release summary. This will be included as part of the changelog as well as the release notes on GitHub.</p> <p>Then, go to create a new GitHub releases and select the tag that matches the milestone (creating a new one). Add again the tag as the release title (optionally write something else that defines this release as a title).</p> <p>Finally, press the \"Publish release\" button and ensure the release workflow succeeds (check the release workflow).</p>"},{"location":"developers/#testing-documentation-locally","title":"Testing documentation locally","text":"<p>To test the documentation locally, just install and run mkdocs:</p> <pre><code>pip install .[dev]\nmkdocs build\nmkdocs serve\n</code></pre> <p>Then open http://127.0.0.1:8000/tripper/ in your browser.</p>"},{"location":"developers/#profiling","title":"Profiling","text":"<p>To identify performance bottlenecks, install <code>pytest-profiling</code> and run run</p> <pre><code>pytest --profile\n</code></pre> <p>to generate a <code>prof/</code> subdirectory with profiling information.</p> <p>For a graphical overview, install <code>gprof2dot</code> and <code>dot</code> and run</p> <pre><code>pytest --profile-svg\n</code></pre> <p>This will create the figure <code>prof/combined.svg</code>, which shows what functions that uses most time and how many times they are called.</p> <p>See https://pypi.org/project/pytest-profiling/ for more information.</p>"},{"location":"known-issues/","title":"Known issues","text":"<ul> <li> <p>If you use the rdflib backend and don't have write permissions to     the cache directory (which e.g. can happen if you run Python in     docker as a non-root user), you may get a <code>urllib.error.HTTPError</code>     error when accessing an online rdf resource.</p> <p>Setting the environment variable <code>XDG_CACHE_HOME</code> to a directory that you have write access to will solve the problem.</p> </li> </ul>"},{"location":"planned-backends/","title":"Planned backends","text":"<p>In addition to the currently existing backends, the following additional backends may be supported in upcoming versions:</p> <ul> <li>Redland librdf</li> <li>Apache Jena Fuseki</li> <li>Allegrograph</li> <li>Wikidata</li> </ul>"},{"location":"session/","title":"Session","text":"<p>Tripper provides simple support for sessions.</p> <p>First you should configure some triplestores you want to use. This is done in a YAML where each configured triplestore is identified with a unique name.</p> <p>The default location of this configuration file depends on the system:</p> <ul> <li>Linux: <code>$HOME/.config/tripper/session.yaml</code></li> <li>Windows: <code>$HOME/AppData/Local/tripper/Config/session.yaml</code></li> <li>Darwin: <code>$HOME/Library/Config/tripper/session.yaml</code></li> </ul> <p>The schema of the YAML file is simple. A session should have a name that identifies it and should be followed by keyword arguments accepted by the <code>Triplestore</code> constructor.</p> <p>Here is an example of a possible session file:</p> <pre><code>---\n\nRdflibTest:\n  backend: rdflib\n\nGraphDBTest:\n  backend: sparqlwrapper\n  base_iri: http://localhost:7200/repositories/test_repo\n  update_iri: http://localhost:7200/repositories/test_repo/statements\n  check_iri: http://localhost:7200/repositories\n\nFusekiTest:\n  backend: sparqlwrapper\n  base_iri: http://localhost:3030/test_repo\n  update_iri: http://localhost:3030/test_repo/update\n  check_iri: http://localhost:3030\n  username: admin\n  password: admin0\n\nMyKB:\n  backend: sparqlwrapper\n  base_iri: https://graphdb.myproject.eu/repositories/test_repo\n  update_iri: https://graphdb.myproject.eu/repositories/test_repo/statements\n  check_iri: https://graphdb.myproject.eu/repositories\n  username: myname\n  password: KEYRING\n</code></pre> <p>The first entry is an in-memory rdflib backend.</p> <p>The second and third entries correspond to GraphDB and Fuseki services, respectively. These can be started with docker as described in the developers section.</p> <p>The fourth entry is just a dummy example, showing how to use keyring.</p> <p>Each entry starts with the name identifying the configured triplestore. The keywords following it, correspond to the keyword arguments passed to the Triplestore constructor.</p> <p>If an entry has a <code>password</code> keyword with the special value \"KEYRING\", the value is replaced with the password looked up using the keyring library.</p> <p>Tip</p> <p>To store the password for the \"MyKB\" backend in the keyring, make sure that you have keyring installed and run the following command in a terminal</p> <pre><code>keyring set MyKB myname\n</code></pre> <p>Enter the password in the prompt. That's it, now you can access <code>MyKB</code> as if the password was hardcoded into the session.yaml file.</p> <p>See the keyring documentation for improved security by using one of the recommended keyring backends for your system.</p>"},{"location":"session/#example","title":"Example","text":"<p>If you have started the Fuseki service (as described on developers), you can now do:</p> <pre><code>&gt;&gt;&gt; from tripper import Literal, Session\n\n# Normally you will call Session with no arguments\n&gt;&gt;&gt; session = Session()  # doctest: +SKIP\n\n# ...but it is also possible to specify the config file explicitly\n&gt;&gt;&gt; session = Session(\"tests/input/session.yaml\")\n&gt;&gt;&gt; ts = session.get_triplestore(\"FusekiTest\")\n&gt;&gt;&gt; EX = ts.bind(\"ex\", \"http://example.com#\")\n\n&gt;&gt;&gt; ts.remove()  # clear the triplestore  # doctest: +ELLIPSIS\n&lt;SPARQLWrapper.Wrapper.QueryResult object at 0x...&gt;\n\n&gt;&gt;&gt; ts.add_triples([\n...     (EX.john, EX.hasName, Literal(\"John\")),\n...     (EX.john, EX.hasSon, EX.lars),\n... ])\n&gt;&gt;&gt; list(ts.triples())  # doctest: +NORMALIZE_WHITESPACE\n[('http://example.com#john', 'http://example.com#hasName', Literal('John')),\n ('http://example.com#john', 'http://example.com#hasSon', 'http://example.com#lars')]\n</code></pre>"},{"location":"tutorial/","title":"Basic tutorial","text":""},{"location":"tutorial/#introduction","title":"Introduction","text":"<p>Tripper is a Python library providing a common interface to a range of pre-defined triplestores. This is done via a plugin system for different triplestore <code>backends</code>. See the README file for a list of currently supported backends.</p> <p>The API provided by Tripper is modelled after rdflib, so if you know that library, you will find Tripper rather familiar. But there are some differences to be aware of. Most important are:</p> <ul> <li>All IRIs are represented by Python strings.   Example: <code>\"https://w3id.org/emmo#Metre\"</code></li> <li>Blank nodes are strings starting with \"_:\".   Example: <code>\"_:bnode1\"</code></li> <li>Literals are constructed with <code>tripper.Literal</code>.   Example: <code>tripper.Literal(3.14, datatype=XSD.float)</code></li> <li>Namespace object works similar to namespace objects in rdflib, but its   attribution access expands to plain Python strings.   Example: <code>XSD.float</code></li> </ul> <p>Tripper namespaces has also additional features that make them very   convinient when working with ontologies, like EMMO that uses   numerical IRIs.</p>"},{"location":"tutorial/#getting-started","title":"Getting started","text":"<p>To interface a triplestore, you create an instance of Triplestore providing the name of the triplestore as the <code>backend</code> argument.</p> <p>For example, to create an interface to an in-memory rdflib triplestore, you can use the <code>rdflib</code> backend:</p> <pre><code>&gt;&gt;&gt; from tripper import Triplestore\n&gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n</code></pre>"},{"location":"tutorial/#namespace-objects","title":"Namespace objects","text":"<p>Namespace objects are a very convenient feature that simplifies writing IRIs. Tripper provides a set of standard pre-defined namespaces that can simply be imported. For example:</p> <pre><code>&gt;&gt;&gt; from tripper import OWL, RDFS\n&gt;&gt;&gt; RDFS.subClassOf\n'http://www.w3.org/2000/01/rdf-schema#subClassOf'\n</code></pre> <p>New namespaces can be created using the Namespace class, but are usually added with the [<code>bind()</code>] method:</p> <pre><code>&gt;&gt;&gt; ONTO = ts.bind(\"onto\", \"http://example.com/onto#\")\n&gt;&gt;&gt; ONTO.MyConcept\n'http://example.com/onto#MyConcept'\n</code></pre>"},{"location":"tutorial/#adding-triples-to-the-triplestore","title":"Adding triples to the triplestore","text":"<p>Triples can now be added to the triplestore, using the [<code>add()</code>] and [<code>add_triples()</code>] methods:</p> <pre><code>&gt;&gt;&gt; from tripper.utils import en\n&gt;&gt;&gt; ts.add_triples([\n...     (ONTO.MyConcept, RDFS.subClassOf, OWL.Thing),\n...     (ONTO.MyConcept, RDFS.label, en(\"My briliant ontological concept.\")),\n... ])\n</code></pre> <p>The function [<code>en()</code>] is just a convenient function for adding English literals. It is equivalent to <code>tripper.Literal(msg, lang=\"en\")</code>.</p> <p>Triples can also be added from a source using the [<code>parse()</code>] method. For example will</p> <pre><code>ts.parse(\"onto.ttl\", format=\"turtle\")\n</code></pre> <p>load all triples in turtle file <code>onto.ttl</code> into the triplestore.</p> <p>Similarly, the triplestore can be serialised to a string or a file using the [<code>serialize()</code>] method:</p> <pre><code>ts.serialize(\"onto2.ttl\")  # serialise to file `onto2.ttl`\ns = ts.serialize(format=\"ntriples\")  # serialise to string s in ntriples format\n</code></pre>"},{"location":"tutorial/#retrieving-triples-from-and-querying-a-triplestore","title":"Retrieving triples from and querying a triplestore","text":"<p>A set of convenient functions exist for simple queries, including [<code>triples()</code>], [<code>subjects()</code>], [<code>predicates()</code>], [<code>objects()</code>], [<code>subject_predicates()</code>], [<code>subject_objects()</code>], [<code>predicate_objects()</code>] and [<code>value()</code>]. Except for [<code>value()</code>], they return iterators. For example:</p> <pre><code>&gt;&gt;&gt; ts.objects(subject=ONTO.MyConcept, predicate=RDFS.subClassOf)  # doctest: +ELLIPSIS\n&lt;generator object Triplestore.objects at 0x...&gt;\n\n&gt;&gt;&gt; list(ts.objects(subject=ONTO.MyConcept, predicate=RDFS.subClassOf))\n['http://www.w3.org/2002/07/owl#Thing']\n</code></pre> <p>The [<code>query()</code>] and [<code>update()</code>] methods can be used to query and update the triplestore using SPARQL. See the next section.</p>"},{"location":"tutorial/#slightly-more-advanced-features","title":"Slightly more advanced features","text":""},{"location":"tutorial/#more-advanced-use-of-namespaces","title":"More advanced use of namespaces","text":"<p>Namespace also supports access by label and IRI checking. Both of these features requires loading an ontology. The following example shows how to create an EMMO namespace with IRI checking. The keyword argument <code>label_annotations=True</code> enables access by <code>skos:prefLabel</code>, <code>rdfs:label</code> or <code>skos:altLabel</code>. What labels to use can also be specified explicitly. The <code>check=True</code> enables checking for existing IRIs.</p> <pre><code>&gt;&gt;&gt; EMMO = ts.bind(\n...     prefix=\"emmo\",\n...     namespace=\"https://w3id.org/emmo#\",\n...     label_annotations=True,\n...     check=True,\n... )\n\n# Access by label\n&gt;&gt;&gt; EMMO.Atom\n'https://w3id.org/emmo#EMMO_eb77076b_a104_42ac_a065_798b2d2809ad'\n\n# This fails because we set `check=True`\n&gt;&gt;&gt; EMMO.invalid_name  # doctest: +ELLIPSIS\nTraceback (most recent call last):\n    ...\ntripper.errors.NoSuchIRIError: https://w3id.org/emmo#invalid_name...\n</code></pre> <p>The above example works, since the <code>namespace=\"https://w3id.org/emmo#\"</code> is resolvable. When the IRI in the <code>namespace</code> argument is not resolvable, it is possible to supply a resolvable IRI or a reference to a populated Triplestore instance via the <code>triplestore</code> keyword argument.</p> <p>Access by label makes it much easier to work with ontologies, like EMMO, that uses non-human readable IDs for the IRIs. More about this below.</p> <p>The utility function [<code>extend_namespace()</code>] can be used to add additional known labels to a namespace. For example:</p> <pre><code>&gt;&gt;&gt; from tripper import Namespace\n&gt;&gt;&gt; from tripper.utils import extend_namespace\n&gt;&gt;&gt; FOOD = Namespace(\n...     \"http://onto-ns.com/ontologies/examples/food#\",\n...     label_annotations=True,\n...     check=True,\n...     reload=True,\n...     triplestore=\"https://raw.githubusercontent.com/EMMC-ASBL/tripper/master/tests/ontologies/food.ttl\",\n...     format=\"turtle\",\n... )\n\n# Hamburger is not a known label\n&gt;&gt;&gt; FOOD.Hamburger  # doctest: +ELLIPSIS\nTraceback (most recent call last):\n    ...\ntripper.errors.NoSuchIRIError: http://onto-ns.com/ontologies/examples/food#Hamburger...\n\n# Add Hamburger to known labels\n&gt;&gt;&gt; extend_namespace(FOOD, {\"Hamburger\": FOOD + \"Hamburger\"})\n&gt;&gt;&gt; FOOD.Hamburger == FOOD + \"Hamburger\"\nTrue\n\n# Fish is not a known label\n&gt;&gt;&gt; FOOD.Fish  # doctest: +ELLIPSIS\nTraceback (most recent call last):\n    ...\ntripper.errors.NoSuchIRIError: http://onto-ns.com/ontologies/examples/food#Fish...\n\n# Extend FOOD from an online turtle file\n&gt;&gt;&gt; extend_namespace(\n...    FOOD,\n...    \"https://raw.githubusercontent.com/EMMC-ASBL/tripper/master/tests/ontologies/food-more.ttl\",\n...    format=\"turtle\",\n... )\n\n# Now Fish is in the namespace\n&gt;&gt;&gt; FOOD.Fish\n'http://onto-ns.com/ontologies/examples/food#FOOD_90f5dd54_9e5c_46c9_824f_e10625a90c26'\n</code></pre> <p>Namespace objects also have a few convenient methods, including:</p> <ul> <li> <p>Return the namespace base IRI as a string:</p> <p>+FOOD  # same as str(FOOD)   'http://onto-ns.com/ontologies/examples/food#'</p> </li> <li> <p>Return the namespace base IRI with the final slash or hash stripped off:</p> <p>-FOOD   'http://onto-ns.com/ontologies/examples/food'</p> </li> <li> <p>Reverse mapping of namespace IRIs</p> <p>EMMO('https://w3id.org/emmo#EMMO_eb77076b_a104_42ac_a065_798b2d2809ad')   'Atom'</p> </li> </ul>"},{"location":"tutorial/#writing-sparql-queries-using-tripper","title":"Writing SPARQL queries using Tripper","text":"<p>A challenge with ontologies using numerical IRIs is that SPARQL queries become difficult to read and understand. This challenge is greatly mitigated by using the <code>label_annotations</code> feature of Tripper namespaces. The example below shows how to write and execute a SPARQL query with Tripper that finds the IRI and unit symbol of all length units. Note: 1. EMMO classes and properties are written as <code>{EMMO.LengthUnit}</code>, which would expand to <code>https://w3id.org/emmo#EMMO_b3600e73_3e05_479d_9714_c041c3acf5cc</code>. 2. The curly brackets after the <code>WHERE</code> clause have to be written <code>{{</code>, <code>}}</code> because the query is an f-string.</p> <pre><code># Load pre-inferred EMMO\n&gt;&gt;&gt; ts = Triplestore(\"rdflib\", base_iri=\"https://w3id.org/emmo#\")\n&gt;&gt;&gt; ts.parse(\"https://w3id.org/emmo#inferred\")\n\n# Bind \"emmo\" prefix to base_iri\n&gt;&gt;&gt; EMMO = ts.bind(\"emmo\", label_annotations=True, check=True)\n\n# Get IRI and symbol of all length units\n&gt;&gt;&gt; query = f\"\"\"\n... PREFIX owl:  &lt;http://www.w3.org/2002/07/owl#&gt;\n... PREFIX rdf:  &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\n... PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n...\n... SELECT ?unit ?symbol\n... WHERE {{\n...   ?unit rdfs:subClassOf &lt;{EMMO.LengthUnit}&gt; .\n...   ?unit rdfs:subClassOf ?r .\n...   ?r rdf:type owl:Restriction .\n...   ?r owl:onProperty &lt;{EMMO.unitSymbolValue}&gt; .\n...   ?r owl:hasValue ?symbol .\n... }}\n... \"\"\"\n&gt;&gt;&gt; r = ts.query(query)\n&gt;&gt;&gt; (EMMO.Metre, \"m\") in r\nTrue\n</code></pre>"},{"location":"tutorial/#class-restrictions","title":"Class restrictions","text":"<p>When working with OWL ontologies, it is often required to inspect or add class restrictions. The Triplestore class has two convenient methods for this, that do not require knowledge about how restrictions are represented in RDF. Only support basic restrictions, without any nested logical constructs, are supoprted. For more advanced restrictions, we recommend to use EMMOntoPy or Owlready2.</p> <p>A restriction restricts a class to only those individuals that satisfy the restriction. It is described by the following set of parameters.</p> <ul> <li>cls: IRI of class to which the restriction applies.</li> <li>property: IRI of restriction property.</li> <li> <p>type: The type of the restriction.  Should be one of:</p> <ul> <li>some: existential restriction (target is a class IRI)</li> <li>only: universal restriction (target is a class IRI)</li> <li>exactly: cardinality restriction (target is a class IRI)</li> <li>min: minimum cardinality restriction (target is a class IRI)</li> <li>max: maximum cardinality restriction (target is a class IRI)</li> <li>value: Value restriction (target is an IRI of an individual or a literal)</li> </ul> </li> <li> <p>cardinality: the cardinality value for cardinality restrictions.</p> </li> <li>value: The IRI or literal value of the restriction target.</li> </ul> <p>As an example, the class <code>onto:Bacteria</code> can be logically restricted to be unicellular. In Manchester syntax, this can be stated as <code>onto:Bacteria emmo:hasPart exactly 1 onto:Cell</code>. With Tripper this can be stated as:</p> <p><pre><code>&gt;&gt;&gt; iri = ts.add_restriction(\n...     cls=ONTO.Bacteria,\n...     property=EMMO.hasPart,\n...     type=\"exactly\",\n...     cardinality=1,\n...     value=ONTO.Cell,\n... )\n</code></pre> The returned <code>iri</code> is the blank node IRI of the new restriction.</p> <p>To find the above restriction, the [<code>restrictions()</code>] method can be used. It returns an iterator over all restrictions that matches the provided criteria. For example:</p> <pre><code>&gt;&gt;&gt; g = ts.restrictions(cls=ONTO.Bacteria, property=EMMO.hasPart, asdict=True)\n&gt;&gt;&gt; list(g)  # doctest: +ELLIPSIS\n[{'iri': '_:...', 'cls': 'http://example.com/onto#Bacteria', 'property': 'https://w3id.org/emmo#EMMO_17e27c22_37e1_468c_9dd7_95e137f73e7f', 'type': 'exactly', 'cardinality': 1, 'value': 'http://example.com/onto#Cell'}]\n</code></pre> <p>With the <code>asdict</code> argument set to false, an iterator over the IRIs of all matching restrictions is returned:</p> <pre><code>&gt;&gt;&gt; g = ts.restrictions(cls=ONTO.Bacteria, property=EMMO.hasPart, asdict=False)\n&gt;&gt;&gt; next(g) == iri\nTrue\n</code></pre>"},{"location":"tutorial/#utilities","title":"Utilities","text":"<p>Todo: Describe the <code>tripper.utils</code> module</p>"},{"location":"tutorial/#specialised-features","title":"Specialised features","text":""},{"location":"tutorial/#working-with-mappings","title":"Working with mappings","text":"<p>With a data model, we here mean an abstract model that describes the structure of a dataset. To provide a shared semantic meaning of a data model and its properties (structural elements), one can create mappings between these elements and ontological concepts (typically a class in an OWL ontology).</p> <p>Mappings can also be used to semantically document the arguments and return values of a function.</p> <p>The Triplestore class has two specialised methods for adding mappings, [<code>map()</code>] and [<code>add_function()</code>]. The purpose of the [<code>map()</code>] method, is to map a data models and its properties to ontological concepts, while [<code>add_function()</code>] maps the arguments and return value of a function to ontological concepts.</p> <p>Note, the name of the [<code>map()</code>] and [<code>add_function()</code>] methods are not very intuitive and may be changed in the future.</p>"},{"location":"tutorial/#adding-mappings","title":"Adding mappings","text":"<p>Lets assume that you have a data model identified by the IRI <code>http://onto-ns.com/meta/ex/0.1/MyDataModel</code>, which has a property (structural element) called velocity. A namespace object for this data model can be created with</p> <pre><code>from tripper import Namespace\nDM = Namespace(\"http://onto-ns.com/meta/ex/0.1/MyDataModel#\")\n</code></pre> <p>and use to map the data model property <code>velocity</code> to the concept <code>ONTO.Velocity</code> in the ontology</p> <pre><code>ts.map(DM.velocity, ONTO.Velocity)\n</code></pre> <p>One can also work directly with DLite and SOFT7 data models. Here we repeat the above with DLite:</p> <pre><code>import dlite\nmymodel = dlite.get_instance(\"http://onto-ns.com/meta/ex/0.1/MyDataModel\")\nts.map(mymodel.velocity, ONTO.Velocity)\n</code></pre> <p>The <code>add_function()</code> method documents a Python function semantically and adds mappings for its arguments and return value(s). Currently, it supports both EMMO and the Function Ontology (FnO) for the semantic documentation.</p> <p>For example, to semantically document the general function <code>mean()</code> applied to the special context of arm lengths, one can do</p> <pre><code>def mean(x, y):\n    \"\"\"Returns the mean value of `x` and `y`.\"\"\"\n    return (x + y)/2\n\nts.add_function(\n    mean,\n    expects=(ONTO.RightArmLength, ONTO.LeftArmLength),\n    returns=ONTO.AverageArmLength,\n)\n</code></pre>"},{"location":"tutorial/#using-mappings","title":"Using mappings","text":"<p>Todo: Describe the <code>tripper.mappings</code> subpackage...</p>"},{"location":"api_reference/errors/","title":"errors","text":"<p>Exceptions and warnings for the tripper package.</p>"},{"location":"api_reference/errors/#tripper.errors.ArgumentTypeError","title":"<code> ArgumentTypeError            (TripperError, TypeError)         </code>","text":"<p>Invalid argument type.</p> Source code in <code>tripper/errors.py</code> <pre><code>class ArgumentTypeError(TripperError, TypeError):\n    \"\"\"Invalid argument type.\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.ArgumentValueError","title":"<code> ArgumentValueError            (TripperError, ValueError)         </code>","text":"<p>Invalid argument value (of correct type).</p> Source code in <code>tripper/errors.py</code> <pre><code>class ArgumentValueError(TripperError, ValueError):\n    \"\"\"Invalid argument value (of correct type).\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.CannotGetFunctionError","title":"<code> CannotGetFunctionError            (TripperError)         </code>","text":"<p>Not able to get function documented in the triplestore.</p> Source code in <code>tripper/errors.py</code> <pre><code>class CannotGetFunctionError(TripperError):\n    \"\"\"Not able to get function documented in the triplestore.\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.IRIExistsError","title":"<code> IRIExistsError            (NamespaceError)         </code>","text":"<p>IRI already exists.</p> Source code in <code>tripper/errors.py</code> <pre><code>class IRIExistsError(NamespaceError):\n    \"\"\"IRI already exists.\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.NamespaceError","title":"<code> NamespaceError            (TripperError)         </code>","text":"<p>Namespace error.</p> Source code in <code>tripper/errors.py</code> <pre><code>class NamespaceError(TripperError):\n    \"\"\"Namespace error.\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.NoSuchIRIError","title":"<code> NoSuchIRIError            (NamespaceError)         </code>","text":"<p>Namespace has no such IRI.</p> Source code in <code>tripper/errors.py</code> <pre><code>class NoSuchIRIError(NamespaceError):\n    \"\"\"Namespace has no such IRI.\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.PermissionWarning","title":"<code> PermissionWarning            (TripperWarning, UserWarning)         </code>","text":"<p>Not enough permissions.</p> Source code in <code>tripper/errors.py</code> <pre><code>class PermissionWarning(TripperWarning, UserWarning):\n    \"\"\"Not enough permissions.\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.TripperError","title":"<code> TripperError            (Exception)         </code>","text":"<p>Base exception for tripper errors.</p> Source code in <code>tripper/errors.py</code> <pre><code>class TripperError(Exception):\n    \"\"\"Base exception for tripper errors.\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.TripperWarning","title":"<code> TripperWarning            (Warning)         </code>","text":"<p>Base class for tripper warnings.</p> Source code in <code>tripper/errors.py</code> <pre><code>class TripperWarning(Warning):\n    \"\"\"Base class for tripper warnings.\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.UniquenessError","title":"<code> UniquenessError            (TripperError)         </code>","text":"<p>More than one matching triple.</p> Source code in <code>tripper/errors.py</code> <pre><code>class UniquenessError(TripperError):\n    \"\"\"More than one matching triple.\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.UnknownDatatypeWarning","title":"<code> UnknownDatatypeWarning            (TripperWarning)         </code>","text":"<p>Unknown datatype.</p> Source code in <code>tripper/errors.py</code> <pre><code>class UnknownDatatypeWarning(TripperWarning):\n    \"\"\"Unknown datatype.\"\"\"\n</code></pre>"},{"location":"api_reference/errors/#tripper.errors.UnusedArgumentWarning","title":"<code> UnusedArgumentWarning            (TripperWarning)         </code>","text":"<p>Argument is unused.</p> Source code in <code>tripper/errors.py</code> <pre><code>class UnusedArgumentWarning(TripperWarning):\n    \"\"\"Argument is unused.\"\"\"\n</code></pre>"},{"location":"api_reference/interface/","title":"interface","text":"<p>Provides the ITriplestore protocol class, that documents the interface of the triplestore backends.</p>"},{"location":"api_reference/interface/#tripper.interface.ITriplestore","title":"<code> ITriplestore            (Protocol)         </code>","text":"<p>Interface for triplestore backends.</p> <p>In addition to the methods specified by this interface, a backend may also implement the following optional methods:</p> <pre><code># Whether the backend perfers SPQRQL queries instead of using the\n# triples() method.\nprefer_sparql = True\n\ndef parse(\n        self,\n        source: Union[str, Path, IO] = None,\n        location: str = None,\n        data: str = None,\n        format: str = None,\n        **kwargs\n    ):\n    \"\"\"Parse source and add the resulting triples to triplestore.\n\n    The source is specified using one of `source`, `location` or `data`.\n\n    Arguments:\n        source: File-like object or file name.\n        location: String with relative or absolute URL to source.\n        data: String containing the data to be parsed.\n        format: Needed if format can not be inferred from source.\n        kwargs: Additional backend-specific parameters controlling\n            the parsing.\n    \"\"\"\n\ndef serialize(\n        self,\n        destination: Union[str, Path, IO] = None,\n        format: str ='xml',\n        **kwargs\n    ):\n    \"\"\"Serialise to destination.\n\n    Arguments:\n        destination: File name or object to write to.  If None, the\n            serialisation is returned.\n        format: Format to serialise as.  Supported formats, depends on\n            the backend.\n        kwargs: Additional backend-specific parameters controlling\n            the serialisation.\n\n    Returns:\n        Serialised string if `destination` is None.\n    \"\"\"\n\ndef query(self, query_object: str, **kwargs) -&gt; List[Tuple[str, ...]]:\n    \"\"\"SPARQL query.\n\n    Arguments:\n        query_object: String with the SPARQL query.\n        kwargs: Additional backend-specific keyword arguments.\n\n    Returns:\n        The return type depends on type of query:\n          - SELECT: list of tuples of IRIs for each matching row\n          - ASK: bool\n          - CONSTRUCT, DESCRIBE: generator over triples\n    \"\"\"\n\ndef update(self, update_object: str, **kwargs):\n    \"\"\"Update triplestore with SPARQL.\n\n    Arguments:\n        query_object: String with the SPARQL query.\n        kwargs: Additional backend-specific keyword arguments.\n\n    Note:\n        This method is intended for INSERT and DELETE queries.  Use\n        the query() method for SELECT queries.\n    \"\"\"\n\ndef bind(self, prefix: str, namespace: str) -&gt; Namespace:\n    \"\"\"Bind prefix to namespace.\n\n    Should only be defined if the backend supports namespaces.\n    \"\"\"\n\ndef is_available(self, timeout: float = 5, interval: float = 1) -&gt; bool:\n    \"\"\"Checks if the backend is available.\n\n    Arguments:\n        timeout: Total time in seconds to wait for a response.\n        interval: Internal time interval in seconds between checking if\n            the service has responded.\n\n    Returns:\n        Returns true if the backend is available.\n    \"\"\"\n\ndef namespaces(self) -&gt; dict:\n    \"\"\"Returns a dict mapping prefixes to namespaces.\n\n    Should only be defined if the backend supports namespaces.\n    Used by triplestore.parse() to get prefixes after reading\n    triples from an external source.\n    \"\"\"\n\n@classmethod\ndef create_database(cls, database: str, **kwargs):\n    \"\"\"Create a new database in backend.\n\n    Parameters:\n        database: Name of the new database.\n        kwargs: Keyword arguments passed to the backend\n            create_database() method.\n\n    Note:\n        This is a class method, which operates on the backend\n        triplestore without connecting to it.\n    \"\"\"\n\n@classmethod\ndef remove_database(cls, database: str, **kwargs):\n    \"\"\"Remove a database in backend.\n\n    Parameters:\n        database: Name of the database to be removed.\n        kwargs: Keyword arguments passed to the backend\n            remove_database() method.\n\n    Note:\n        This is a class method, which operates on the backend\n        triplestore without connecting to it.\n    \"\"\"\n\n@classmethod\ndef list_databases(cls, **kwargs):\n    \"\"\"For backends that supports multiple databases, list of all\n    databases.\n\n    Parameters:\n        kwargs: Keyword arguments passed to the backend\n            list_database() method.\n\n    Note:\n        This is a class method, which operates on the backend\n        triplestore without connecting to it.\n    \"\"\"\n</code></pre> Source code in <code>tripper/interface.py</code> <pre><code>class ITriplestore(Protocol):\n    '''Interface for triplestore backends.\n\n    In addition to the methods specified by this interface, a backend\n    may also implement the following optional methods:\n\n    ```python\n\n    # Whether the backend perfers SPQRQL queries instead of using the\n    # triples() method.\n    prefer_sparql = True\n\n    def parse(\n            self,\n            source: Union[str, Path, IO] = None,\n            location: str = None,\n            data: str = None,\n            format: str = None,\n            **kwargs\n        ):\n        \"\"\"Parse source and add the resulting triples to triplestore.\n\n        The source is specified using one of `source`, `location` or `data`.\n\n        Arguments:\n            source: File-like object or file name.\n            location: String with relative or absolute URL to source.\n            data: String containing the data to be parsed.\n            format: Needed if format can not be inferred from source.\n            kwargs: Additional backend-specific parameters controlling\n                the parsing.\n        \"\"\"\n\n    def serialize(\n            self,\n            destination: Union[str, Path, IO] = None,\n            format: str ='xml',\n            **kwargs\n        ):\n        \"\"\"Serialise to destination.\n\n        Arguments:\n            destination: File name or object to write to.  If None, the\n                serialisation is returned.\n            format: Format to serialise as.  Supported formats, depends on\n                the backend.\n            kwargs: Additional backend-specific parameters controlling\n                the serialisation.\n\n        Returns:\n            Serialised string if `destination` is None.\n        \"\"\"\n\n    def query(self, query_object: str, **kwargs) -&gt; List[Tuple[str, ...]]:\n        \"\"\"SPARQL query.\n\n        Arguments:\n            query_object: String with the SPARQL query.\n            kwargs: Additional backend-specific keyword arguments.\n\n        Returns:\n            The return type depends on type of query:\n              - SELECT: list of tuples of IRIs for each matching row\n              - ASK: bool\n              - CONSTRUCT, DESCRIBE: generator over triples\n        \"\"\"\n\n    def update(self, update_object: str, **kwargs):\n        \"\"\"Update triplestore with SPARQL.\n\n        Arguments:\n            query_object: String with the SPARQL query.\n            kwargs: Additional backend-specific keyword arguments.\n\n        Note:\n            This method is intended for INSERT and DELETE queries.  Use\n            the query() method for SELECT queries.\n        \"\"\"\n\n    def bind(self, prefix: str, namespace: str) -&gt; Namespace:\n        \"\"\"Bind prefix to namespace.\n\n        Should only be defined if the backend supports namespaces.\n        \"\"\"\n\n    def is_available(self, timeout: float = 5, interval: float = 1) -&gt; bool:\n        \"\"\"Checks if the backend is available.\n\n        Arguments:\n            timeout: Total time in seconds to wait for a response.\n            interval: Internal time interval in seconds between checking if\n                the service has responded.\n\n        Returns:\n            Returns true if the backend is available.\n        \"\"\"\n\n    def namespaces(self) -&gt; dict:\n        \"\"\"Returns a dict mapping prefixes to namespaces.\n\n        Should only be defined if the backend supports namespaces.\n        Used by triplestore.parse() to get prefixes after reading\n        triples from an external source.\n        \"\"\"\n\n    @classmethod\n    def create_database(cls, database: str, **kwargs):\n        \"\"\"Create a new database in backend.\n\n        Parameters:\n            database: Name of the new database.\n            kwargs: Keyword arguments passed to the backend\n                create_database() method.\n\n        Note:\n            This is a class method, which operates on the backend\n            triplestore without connecting to it.\n        \"\"\"\n\n    @classmethod\n    def remove_database(cls, database: str, **kwargs):\n        \"\"\"Remove a database in backend.\n\n        Parameters:\n            database: Name of the database to be removed.\n            kwargs: Keyword arguments passed to the backend\n                remove_database() method.\n\n        Note:\n            This is a class method, which operates on the backend\n            triplestore without connecting to it.\n        \"\"\"\n\n    @classmethod\n    def list_databases(cls, **kwargs):\n        \"\"\"For backends that supports multiple databases, list of all\n        databases.\n\n        Parameters:\n            kwargs: Keyword arguments passed to the backend\n                list_database() method.\n\n        Note:\n            This is a class method, which operates on the backend\n            triplestore without connecting to it.\n        \"\"\"\n\n    ```\n    '''\n\n    def __init__(self, base_iri: \"Optional[str]\" = None, **kwargs):\n        \"\"\"Initialise triplestore.\n\n        Arguments:\n            base_iri: Optional base IRI to initiate the triplestore from.\n            kwargs: Additional keyword arguments passed to the backend.\n        \"\"\"\n\n    def triples(self, triple: \"Triple\") -&gt; \"Generator\":\n        \"\"\"Returns a generator over matching triples.\n\n        Arguments:\n            triple: A `(s, p, o)` tuple where `s`, `p` and `o` should\n                either be None (matching anything) or an exact IRI to\n                match.\n        \"\"\"\n\n    def add_triples(self, triples: \"Sequence[Triple]\"):\n        \"\"\"Add a sequence of triples.\n\n        Arguments:\n            triples: A sequence of `(s, p, o)` tuples to add to the\n                triplestore.\n        \"\"\"\n\n    def remove(self, triple: \"Triple\"):\n        \"\"\"Remove all matching triples from the backend.\"\"\"\n</code></pre>"},{"location":"api_reference/interface/#tripper.interface.ITriplestore.__init__","title":"<code>__init__(self, base_iri=None, **kwargs)</code>  <code>special</code>","text":"<p>Initialise triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>base_iri</code> <code>Optional[str]</code> <p>Optional base IRI to initiate the triplestore from.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments passed to the backend.</p> <code>{}</code> Source code in <code>tripper/interface.py</code> <pre><code>def __init__(self, base_iri: \"Optional[str]\" = None, **kwargs):\n    \"\"\"Initialise triplestore.\n\n    Arguments:\n        base_iri: Optional base IRI to initiate the triplestore from.\n        kwargs: Additional keyword arguments passed to the backend.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/interface/#tripper.interface.ITriplestore.add_triples","title":"<code>add_triples(self, triples)</code>","text":"<p>Add a sequence of triples.</p> <p>Parameters:</p> Name Type Description Default <code>triples</code> <code>Sequence[Triple]</code> <p>A sequence of <code>(s, p, o)</code> tuples to add to the triplestore.</p> required Source code in <code>tripper/interface.py</code> <pre><code>def add_triples(self, triples: \"Sequence[Triple]\"):\n    \"\"\"Add a sequence of triples.\n\n    Arguments:\n        triples: A sequence of `(s, p, o)` tuples to add to the\n            triplestore.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/interface/#tripper.interface.ITriplestore.remove","title":"<code>remove(self, triple)</code>","text":"<p>Remove all matching triples from the backend.</p> Source code in <code>tripper/interface.py</code> <pre><code>def remove(self, triple: \"Triple\"):\n    \"\"\"Remove all matching triples from the backend.\"\"\"\n</code></pre>"},{"location":"api_reference/interface/#tripper.interface.ITriplestore.triples","title":"<code>triples(self, triple)</code>","text":"<p>Returns a generator over matching triples.</p> <p>Parameters:</p> Name Type Description Default <code>triple</code> <code>Triple</code> <p>A <code>(s, p, o)</code> tuple where <code>s</code>, <code>p</code> and <code>o</code> should either be None (matching anything) or an exact IRI to match.</p> required Source code in <code>tripper/interface.py</code> <pre><code>def triples(self, triple: \"Triple\") -&gt; \"Generator\":\n    \"\"\"Returns a generator over matching triples.\n\n    Arguments:\n        triple: A `(s, p, o)` tuple where `s`, `p` and `o` should\n            either be None (matching anything) or an exact IRI to\n            match.\n    \"\"\"\n</code></pre>"},{"location":"api_reference/literal/","title":"literal","text":"<p>Literal RDF values.</p> <p>Literals may be used as objects in RDF triples to provide a value to a resource.</p> <p>See also https://www.w3.org/TR/rdf11-concepts/#section-Graph-Literal</p>"},{"location":"api_reference/literal/#tripper.literal.Literal","title":"<code> Literal            (str)         </code>","text":"<p>A literal RDF value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[datetime, bytes, bytearray, bool, int, float, str, None, dict, list]</code> <p>The literal value. Can be given as a string or a Python object.</p> required <code>lang</code> <code>Optional[str]</code> <p>A standard language code, like \"en\", \"no\", etc.  Implies that the <code>value</code> is a language-tagged string.</p> required <code>datatype</code> <code>Optional[str, type]</code> <p>The datatype of this literal. Can be given either as a string with the datatype IRI (ex: <code>\"http://www.w3.org/2001/XMLSchema#integer\"</code>) or as a Python type (ex: <code>int</code>).  If not given, the datatype is inferred from <code>value</code>.  Should not be combined with <code>lang</code>.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from tripper import XSD, Literal\n\n# Inferring the data type\n&gt;&gt;&gt; l1 = Literal(42)\n&gt;&gt;&gt; l1\nLiteral('42', datatype='http://www.w3.org/2001/XMLSchema#integer')\n\n&gt;&gt;&gt; l1.value\n42\n\n# String values with no datatype are assumed to be strings\n&gt;&gt;&gt; l2 = Literal(\"42\")\n&gt;&gt;&gt; l2.value\n'42'\n\n# Explicit providing the datatype\n&gt;&gt;&gt; l3 = Literal(\"42\", datatype=XSD.integer)\n&gt;&gt;&gt; l3\nLiteral('42', datatype='http://www.w3.org/2001/XMLSchema#integer')\n\n&gt;&gt;&gt; l3.value\n42\n\n# Localised or language-tagged string literal\n&gt;&gt;&gt; Literal(\"Hello world\", lang=\"en\")\nLiteral('Hello world', lang='en')\n\n# Dicts, lists and None are assumed to be of type rdf:JSON\n&gt;&gt;&gt; l4 = Literal({\"name\": \"Jon Doe\"})\n&gt;&gt;&gt; l4.datatype\n'http://www.w3.org/1999/02/22-rdf-syntax-ns#JSON'\n\n&gt;&gt;&gt; l4.value\n{'name': 'Jon Doe'}\n\n# Literal of custom datatype (`value` must be a string)\n# This will issue an `UnknownDatatypeWarning` which we ignore...\n&gt;&gt;&gt; import warnings\n&gt;&gt;&gt; from tripper.errors import UnknownDatatypeWarning\n&gt;&gt;&gt; with warnings.catch_warnings():\n...     warnings.filterwarnings(\n...         action=\"ignore\", category=UnknownDatatypeWarning,\n...     )\n...     Literal(\"a value\", datatype=\"http://example.com/onto#MyType\")\nLiteral('a value', datatype='http://example.com/onto#MyType')\n</code></pre> Source code in <code>tripper/literal.py</code> <pre><code>class Literal(str):\n    \"\"\"A literal RDF value.\n\n    Arguments:\n        value (Union[datetime, bytes, bytearray, bool, int, float,\n            str, None, dict, list]): The literal value. Can be\n            given as a string or a Python object.\n        lang (Optional[str]): A standard language code, like \"en\",\n            \"no\", etc.  Implies that the `value` is a language-tagged\n            string.\n        datatype (Optional[str, type]): The datatype of this literal.\n            Can be given either as a string with the datatype IRI (ex:\n            `\"http://www.w3.org/2001/XMLSchema#integer\"`) or as a\n            Python type (ex: `int`).  If not given, the datatype is\n            inferred from `value`.  Should not be combined with\n            `lang`.\n\n    Examples:\n\n        ```python\n        &gt;&gt;&gt; from tripper import XSD, Literal\n\n        # Inferring the data type\n        &gt;&gt;&gt; l1 = Literal(42)\n        &gt;&gt;&gt; l1\n        Literal('42', datatype='http://www.w3.org/2001/XMLSchema#integer')\n\n        &gt;&gt;&gt; l1.value\n        42\n\n        # String values with no datatype are assumed to be strings\n        &gt;&gt;&gt; l2 = Literal(\"42\")\n        &gt;&gt;&gt; l2.value\n        '42'\n\n        # Explicit providing the datatype\n        &gt;&gt;&gt; l3 = Literal(\"42\", datatype=XSD.integer)\n        &gt;&gt;&gt; l3\n        Literal('42', datatype='http://www.w3.org/2001/XMLSchema#integer')\n\n        &gt;&gt;&gt; l3.value\n        42\n\n        # Localised or language-tagged string literal\n        &gt;&gt;&gt; Literal(\"Hello world\", lang=\"en\")\n        Literal('Hello world', lang='en')\n\n        # Dicts, lists and None are assumed to be of type rdf:JSON\n        &gt;&gt;&gt; l4 = Literal({\"name\": \"Jon Doe\"})\n        &gt;&gt;&gt; l4.datatype\n        'http://www.w3.org/1999/02/22-rdf-syntax-ns#JSON'\n\n        &gt;&gt;&gt; l4.value\n        {'name': 'Jon Doe'}\n\n        # Literal of custom datatype (`value` must be a string)\n        # This will issue an `UnknownDatatypeWarning` which we ignore...\n        &gt;&gt;&gt; import warnings\n        &gt;&gt;&gt; from tripper.errors import UnknownDatatypeWarning\n        &gt;&gt;&gt; with warnings.catch_warnings():\n        ...     warnings.filterwarnings(\n        ...         action=\"ignore\", category=UnknownDatatypeWarning,\n        ...     )\n        ...     Literal(\"a value\", datatype=\"http://example.com/onto#MyType\")\n        Literal('a value', datatype='http://example.com/onto#MyType')\n\n        ```\n\n    \"\"\"\n\n    # pylint: disable=too-many-nested-blocks\n\n    # Note that the order of datatypes matters - it is used by\n    # utils.parse_literal() when inferring the datatype of a literal.\n    datatypes = {\n        datetime.datetime: (XSD.dateTime,),\n        datetime.date: (XSD.date,),\n        datetime.time: (XSD.time,),\n        datetime.timedelta: (XSD.duration,),\n        bytes: (XSD.hexBinary, XSD.base64Binary),\n        bytearray: (XSD.hexBinary, XSD.base64Binary),\n        bool: (XSD.boolean,),\n        int: (\n            XSD.integer,\n            XSD.int,\n            XSD.short,\n            XSD.long,\n            XSD.nonNegativeInteger,\n            XSD.nonPositiveInteger,\n            XSD.negativeInteger,\n            XSD.unsignedInt,\n            XSD.unsignedShort,\n            XSD.unsignedLong,\n            XSD.byte,\n            XSD.unsignedByte,\n        ),\n        float: (\n            XSD.double,\n            XSD.decimal,\n            XSD.dateTimeStamp,\n            XSD.real,\n            XSD.rational,\n        ),\n        str: (\n            XSD.string,\n            RDFS.Literal,\n            RDF.PlainLiteral,\n            RDF.HTML,\n            RDF.JSON,\n            RDF.XMLLiteral,\n            RDF.langString,\n            XSD.NCName,\n            XSD.NMTOKEN,\n            XSD.Name,\n            XSD.anyURI,\n            XSD.language,\n            XSD.normalizedString,\n            XSD.token,\n        ),\n        list: (RDF.JSON,),\n        dict: (RDF.JSON,),\n        None.__class__: (RDF.JSON,),\n    }\n    if Quantity:\n        datatypes[Quantity] = (SIQuantityDatatype,)\n\n    lang: \"Union[str, None]\"\n    datatype: \"Union[str, None]\"\n\n    def __new__(\n        cls,\n        value: (\n            \"Union[datetime.datetime, datetime.date, datetime.time, \"\n            \"datetime.timedelta, bytes, bytearray, bool, int, float, str, \"\n            \"None, dict, list]\"\n        ),\n        lang: \"Optional[str]\" = None,\n        datatype: \"Optional[Union[str, type]]\" = None,\n    ):\n        # pylint: disable=too-many-branches,too-many-statements\n        string = super().__new__(cls, value)\n        string.lang = None\n        string.datatype = None\n\n        # Get lang\n        if lang:\n            if datatype:\n                raise TypeError(\n                    \"A literal can only have one of `lang` or `datatype`.\"\n                )\n            string.lang = str(lang)\n\n        # Get datatype\n        elif datatype in cls.datatypes:\n            string.datatype = cls.datatypes[datatype][0]  # type: ignore\n        elif datatype == RDF.JSON:\n            if isinstance(value, str):\n                # Raises an exception if `value` is not a valid JSON string\n                json.loads(value)\n            else:\n                value = json.dumps(value)\n            string = super().__new__(cls, value)\n            string.lang = None\n            string.datatype = RDF.JSON\n        elif datatype == SIQuantityDatatype:\n            if Quantity and isinstance(value, Quantity):\n                value = f\"{value:~P}\"\n            string = super().__new__(cls, value)\n            string.lang = None\n            string.datatype = SIQuantityDatatype\n        elif datatype:\n            assert isinstance(datatype, str)  # nosec\n            # Create canonical representation of value for given datatype\n            val = None\n            for typ, names in cls.datatypes.items():\n                for name in names:\n                    if name == datatype:\n                        try:\n                            if hasattr(typ, \"fromisoformat\"):\n                                val = typ.fromisoformat(value).isoformat()\n                            else:\n                                val = typ(value)\n                            break\n                        except:  # pylint: disable=bare-except\n                            pass  # nosec\n                    if val:\n                        break\n            if val is not None:\n                # Re-initialize the value anew, similarly to what is done in\n                # the first line of this method.\n                string = super().__new__(cls, val)\n                string.lang = None\n\n            string.datatype = datatype\n\n        # Infer datatype from value\n        elif isinstance(value, Literal):\n            string.lang = value.lang\n            string.datatype = value.datatype\n        elif isinstance(value, str):\n            string.datatype = None\n        elif isinstance(value, bool):\n            string.datatype = XSD.boolean\n        elif isinstance(value, int):\n            string.datatype = XSD.integer\n        elif isinstance(value, float):\n            string.datatype = XSD.double\n        elif isinstance(value, (bytes, bytearray)):\n            # Re-initialize the value anew, similarly to what is done in\n            # the first line of this method.\n            string = super().__new__(cls, value.hex())\n            string.lang = None\n            string.datatype = XSD.hexBinary\n        elif isinstance(value, datetime.datetime):\n            string = super().__new__(cls, value.isoformat())\n            string.lang = None\n            string.datatype = XSD.dateTime\n        elif isinstance(value, datetime.date):\n            string = super().__new__(cls, value.isoformat())\n            string.lang = None\n            string.datatype = XSD.date\n        elif isinstance(value, datetime.time):\n            string = super().__new__(cls, value.isoformat())\n            string.lang = None\n            string.datatype = XSD.time\n        elif isinstance(value, datetime.timedelta):\n            string = super().__new__(cls, format_duration(value))\n            string.lang = None\n            string.datatype = XSD.duration\n        elif value is None or isinstance(value, (dict, list)):\n            string = super().__new__(cls, json.dumps(value))\n            string.lang = None\n            string.datatype = RDF.JSON\n        elif Quantity and isinstance(value, Quantity):\n            string = super().__new__(cls, f\"{value:~P}\")\n            string.lang = None\n            string.datatype = SIQuantityDatatype\n\n        # Some consistency checking\n        if (\n            string.datatype == XSD.nonPositiveInteger\n            and int(value) &gt; 0  # type: ignore[arg-type]\n        ):\n            raise TypeError(f\"not a xsd:nonPositiveInteger: '{string}'\")\n        if (\n            string.datatype == XSD.nonNegativeInteger\n            and int(value) &lt; 0  # type: ignore[arg-type]\n        ):\n            raise TypeError(f\"not a xsd:nonNegativeInteger: '{string}'\")\n        if (\n            string.datatype\n            in (\n                XSD.unsignedInt,\n                XSD.unsignedShort,\n                XSD.unsignedLong,\n                XSD.unsignedByte,\n            )\n            and int(value) &lt; 0  # type: ignore[arg-type]\n        ):\n            raise TypeError(f\"not an unsigned integer: '{string}'\")\n\n        # Check if datatype is known\n        if string.datatype and not any(\n            string.datatype in types for types in cls.datatypes.values()\n        ):\n            warnings.warn(\n                f\"unknown datatype: {string.datatype} - assuming xsd:string\",\n                category=UnknownDatatypeWarning,\n            )\n\n        return string\n\n    def __hash__(self):\n        return hash((str(self), self.lang, self.datatype))\n\n    def __eq__(self, other):  # pylint: disable=too-many-return-statements\n        if not isinstance(other, Literal):\n            if isinstance(other, str) and (\n                self.lang or self.datatype in self.datatypes[str]\n            ):\n                return str(self) == other\n            other = Literal(other)\n        if str(self) != str(other):\n            return False\n        if self.lang and other.lang and self.lang != other.lang:\n            return False\n        if (\n            self.datatype\n            and other.datatype\n            and self.datatype != other.datatype\n        ):\n            return False\n        strings = set(self.datatypes[str] + (None,))\n        if self.datatype is None and other.datatype not in strings:\n            return False\n        if other.datatype is None and self.datatype not in strings:\n            return False\n        return True\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __repr__(self) -&gt; str:\n        lang = f\", lang='{self.lang}'\" if self.lang else \"\"\n        datatype = f\", datatype='{self.datatype}'\" if self.datatype else \"\"\n        return f\"Literal('{self}'{lang}{datatype})\"\n\n    value = property(\n        fget=lambda self: self.to_python(),\n        doc=\"Appropriate python datatype derived from this RDF literal.\",\n    )\n\n    def to_python(self):\n        \"\"\"Returns an appropriate python datatype derived from this RDF\n        literal.\"\"\"\n        if self.datatype == XSD.boolean:\n            value = False if str(self) == \"False\" else bool(self)\n        elif self.datatype in self.datatypes[int]:\n            value = int(self)\n        elif self.datatype in self.datatypes[float]:\n            value = float(self)\n        elif self.datatype == XSD.dateTime:\n            value = datetime.datetime.fromisoformat(self)\n        elif self.datatype == XSD.date:\n            value = datetime.date.fromisoformat(self)\n        elif self.datatype == XSD.time:\n            value = datetime.time.fromisoformat(self)\n        elif self.datatype == XSD.duration:\n            value = parse_duration(self)\n        elif self.datatype == RDF.JSON:\n            value = json.loads(str(self))\n        elif self.datatype == SIQuantityDatatype:\n            if Quantity:\n                # pylint: disable=import-outside-toplevel,cyclic-import\n                from tripper.units import get_ureg\n\n                ureg = get_ureg()\n                value = ureg.Quantity(self)\n            else:\n                warnings.warn(\n                    \"pint is needed to convert emmo:SIQuantityDatatype to \"\n                    \"a quantity\"\n                )\n                value = str(self)\n        else:\n            value = str(self)\n\n        return value\n\n    def n3(self) -&gt; str:  # pylint: disable=invalid-name\n        \"\"\"Returns a representation in n3 format.\"\"\"\n\n        form = self.replace(\"\\\\\", r\"\\\\\").replace('\"', r\"\\\"\")\n\n        if self.lang:\n            return f'\"{form}\"@{self.lang}'\n        if self.datatype:\n            return f'\"{form}\"^^&lt;{self.datatype}&gt;'\n        return f'\"{form}\"'\n</code></pre>"},{"location":"api_reference/literal/#tripper.literal.Literal.value","title":"<code>value</code>  <code>property</code> <code>readonly</code>","text":"<p>Appropriate python datatype derived from this RDF literal.</p>"},{"location":"api_reference/literal/#tripper.literal.Literal.__new__","title":"<code>__new__(cls, value, lang=None, datatype=None)</code>  <code>special</code> <code>staticmethod</code>","text":"<p>Create and return a new object.  See help(type) for accurate signature.</p> Source code in <code>tripper/literal.py</code> <pre><code>def __new__(\n    cls,\n    value: (\n        \"Union[datetime.datetime, datetime.date, datetime.time, \"\n        \"datetime.timedelta, bytes, bytearray, bool, int, float, str, \"\n        \"None, dict, list]\"\n    ),\n    lang: \"Optional[str]\" = None,\n    datatype: \"Optional[Union[str, type]]\" = None,\n):\n    # pylint: disable=too-many-branches,too-many-statements\n    string = super().__new__(cls, value)\n    string.lang = None\n    string.datatype = None\n\n    # Get lang\n    if lang:\n        if datatype:\n            raise TypeError(\n                \"A literal can only have one of `lang` or `datatype`.\"\n            )\n        string.lang = str(lang)\n\n    # Get datatype\n    elif datatype in cls.datatypes:\n        string.datatype = cls.datatypes[datatype][0]  # type: ignore\n    elif datatype == RDF.JSON:\n        if isinstance(value, str):\n            # Raises an exception if `value` is not a valid JSON string\n            json.loads(value)\n        else:\n            value = json.dumps(value)\n        string = super().__new__(cls, value)\n        string.lang = None\n        string.datatype = RDF.JSON\n    elif datatype == SIQuantityDatatype:\n        if Quantity and isinstance(value, Quantity):\n            value = f\"{value:~P}\"\n        string = super().__new__(cls, value)\n        string.lang = None\n        string.datatype = SIQuantityDatatype\n    elif datatype:\n        assert isinstance(datatype, str)  # nosec\n        # Create canonical representation of value for given datatype\n        val = None\n        for typ, names in cls.datatypes.items():\n            for name in names:\n                if name == datatype:\n                    try:\n                        if hasattr(typ, \"fromisoformat\"):\n                            val = typ.fromisoformat(value).isoformat()\n                        else:\n                            val = typ(value)\n                        break\n                    except:  # pylint: disable=bare-except\n                        pass  # nosec\n                if val:\n                    break\n        if val is not None:\n            # Re-initialize the value anew, similarly to what is done in\n            # the first line of this method.\n            string = super().__new__(cls, val)\n            string.lang = None\n\n        string.datatype = datatype\n\n    # Infer datatype from value\n    elif isinstance(value, Literal):\n        string.lang = value.lang\n        string.datatype = value.datatype\n    elif isinstance(value, str):\n        string.datatype = None\n    elif isinstance(value, bool):\n        string.datatype = XSD.boolean\n    elif isinstance(value, int):\n        string.datatype = XSD.integer\n    elif isinstance(value, float):\n        string.datatype = XSD.double\n    elif isinstance(value, (bytes, bytearray)):\n        # Re-initialize the value anew, similarly to what is done in\n        # the first line of this method.\n        string = super().__new__(cls, value.hex())\n        string.lang = None\n        string.datatype = XSD.hexBinary\n    elif isinstance(value, datetime.datetime):\n        string = super().__new__(cls, value.isoformat())\n        string.lang = None\n        string.datatype = XSD.dateTime\n    elif isinstance(value, datetime.date):\n        string = super().__new__(cls, value.isoformat())\n        string.lang = None\n        string.datatype = XSD.date\n    elif isinstance(value, datetime.time):\n        string = super().__new__(cls, value.isoformat())\n        string.lang = None\n        string.datatype = XSD.time\n    elif isinstance(value, datetime.timedelta):\n        string = super().__new__(cls, format_duration(value))\n        string.lang = None\n        string.datatype = XSD.duration\n    elif value is None or isinstance(value, (dict, list)):\n        string = super().__new__(cls, json.dumps(value))\n        string.lang = None\n        string.datatype = RDF.JSON\n    elif Quantity and isinstance(value, Quantity):\n        string = super().__new__(cls, f\"{value:~P}\")\n        string.lang = None\n        string.datatype = SIQuantityDatatype\n\n    # Some consistency checking\n    if (\n        string.datatype == XSD.nonPositiveInteger\n        and int(value) &gt; 0  # type: ignore[arg-type]\n    ):\n        raise TypeError(f\"not a xsd:nonPositiveInteger: '{string}'\")\n    if (\n        string.datatype == XSD.nonNegativeInteger\n        and int(value) &lt; 0  # type: ignore[arg-type]\n    ):\n        raise TypeError(f\"not a xsd:nonNegativeInteger: '{string}'\")\n    if (\n        string.datatype\n        in (\n            XSD.unsignedInt,\n            XSD.unsignedShort,\n            XSD.unsignedLong,\n            XSD.unsignedByte,\n        )\n        and int(value) &lt; 0  # type: ignore[arg-type]\n    ):\n        raise TypeError(f\"not an unsigned integer: '{string}'\")\n\n    # Check if datatype is known\n    if string.datatype and not any(\n        string.datatype in types for types in cls.datatypes.values()\n    ):\n        warnings.warn(\n            f\"unknown datatype: {string.datatype} - assuming xsd:string\",\n            category=UnknownDatatypeWarning,\n        )\n\n    return string\n</code></pre>"},{"location":"api_reference/literal/#tripper.literal.Literal.n3","title":"<code>n3(self)</code>","text":"<p>Returns a representation in n3 format.</p> Source code in <code>tripper/literal.py</code> <pre><code>def n3(self) -&gt; str:  # pylint: disable=invalid-name\n    \"\"\"Returns a representation in n3 format.\"\"\"\n\n    form = self.replace(\"\\\\\", r\"\\\\\").replace('\"', r\"\\\"\")\n\n    if self.lang:\n        return f'\"{form}\"@{self.lang}'\n    if self.datatype:\n        return f'\"{form}\"^^&lt;{self.datatype}&gt;'\n    return f'\"{form}\"'\n</code></pre>"},{"location":"api_reference/literal/#tripper.literal.Literal.to_python","title":"<code>to_python(self)</code>","text":"<p>Returns an appropriate python datatype derived from this RDF literal.</p> Source code in <code>tripper/literal.py</code> <pre><code>def to_python(self):\n    \"\"\"Returns an appropriate python datatype derived from this RDF\n    literal.\"\"\"\n    if self.datatype == XSD.boolean:\n        value = False if str(self) == \"False\" else bool(self)\n    elif self.datatype in self.datatypes[int]:\n        value = int(self)\n    elif self.datatype in self.datatypes[float]:\n        value = float(self)\n    elif self.datatype == XSD.dateTime:\n        value = datetime.datetime.fromisoformat(self)\n    elif self.datatype == XSD.date:\n        value = datetime.date.fromisoformat(self)\n    elif self.datatype == XSD.time:\n        value = datetime.time.fromisoformat(self)\n    elif self.datatype == XSD.duration:\n        value = parse_duration(self)\n    elif self.datatype == RDF.JSON:\n        value = json.loads(str(self))\n    elif self.datatype == SIQuantityDatatype:\n        if Quantity:\n            # pylint: disable=import-outside-toplevel,cyclic-import\n            from tripper.units import get_ureg\n\n            ureg = get_ureg()\n            value = ureg.Quantity(self)\n        else:\n            warnings.warn(\n                \"pint is needed to convert emmo:SIQuantityDatatype to \"\n                \"a quantity\"\n            )\n            value = str(self)\n    else:\n        value = str(self)\n\n    return value\n</code></pre>"},{"location":"api_reference/literal/#tripper.literal.format_duration","title":"<code>format_duration(td)</code>","text":"<p>Format a timedelta object as a ISO 8601 string.</p> Source code in <code>tripper/literal.py</code> <pre><code>def format_duration(td: \"datetime.timedelta\") -&gt; str:\n    \"\"\"Format a timedelta object as a ISO 8601 string.\"\"\"\n    dm = 60\n    dh = dm * 60\n    dD = dh * 24\n    dM = DAYS_PER_YEAR / 12 * dD\n    dY = DAYS_PER_YEAR * dD\n    seconds = td.total_seconds()\n    sign = \"-\" if seconds &lt; 0 else \"\"\n    t = abs(seconds)\n    Y = f\"{t // dY:g}Y\" if t &gt; dY else \"\"\n    t %= dY\n    M = f\"{t // dM:g}M\" if t &gt; dM else \"\"\n    t %= dM\n    D = f\"{t // dD:g}D\" if t &gt; dD else \"\"\n    t %= dD\n    h = f\"{t // dh:g}H\" if t &gt; dh else \"\"\n    t %= dh\n    m = f\"{t // dm:g}M\" if t &gt; dm else \"\"\n    t %= dm\n    s = f\"{t:g}S\" if t else \"\"\n    T = \"T\" if h or m or s else \"\"\n    return f\"{sign}P{Y}{M}{D}{T}{h}{m}{s}\"\n</code></pre>"},{"location":"api_reference/literal/#tripper.literal.parse_duration","title":"<code>parse_duration(duration)</code>","text":"<p>Parse an ISO 8601 duration string to a timedelta object.</p> <p>The duration should be a string of the form \"PnYnMnDTnHnMnS\", where <code>n</code> is a number. A negative duration can be prefixed with \"-\".</p> Source code in <code>tripper/literal.py</code> <pre><code>def parse_duration(duration: str) -&gt; \"datetime.timedelta\":\n    \"\"\"Parse an ISO 8601 duration string to a timedelta object.\n\n    The duration should be a string of the form \"PnYnMnDTnHnMnS\",\n    where `n` is a number. A negative duration can be prefixed\n    with \"-\".\n    \"\"\"\n    m = re.match(\n        \"(-)?P([0-9.]+Y)?([0-9.]+M)?([0-9.]+D)?\"\n        \"(T([0-9.]+H)?([0-9.]+M)?([0-9.eE+-]+S)?)?\",\n        duration,\n    )\n    if not m:\n        raise ValueError(\n            f\"Invalid duration literal '{duration}'. \"\n            \"Should be of the form 'PnYnMnDTnHnMnS'\"\n        )\n    sign, Y, M, D, _, h, m, s = m.groups()\n    sn = -1 if sign == \"-\" else 1\n    days = seconds = 0.0\n    if Y:\n        days += DAYS_PER_YEAR * float(Y[:-1])\n    if M:\n        days += DAYS_PER_YEAR / 12 * float(M[:-1])\n    if D:\n        days += float(D[:-1])\n    if h:\n        seconds += 3600 * float(h[:-1])\n    if m:\n        seconds += 60 * float(m[:-1])\n    if s:\n        seconds += float(s[:-1])\n    return datetime.timedelta(days=sn * days, seconds=sn * seconds)\n</code></pre>"},{"location":"api_reference/namespace/","title":"namespace","text":"<p>Provides a simple representation of namespaces.</p>"},{"location":"api_reference/namespace/#tripper.namespace.Namespace","title":"<code> Namespace        </code>","text":"<p>Represent a namespace.</p> <p>Parameters:</p> Name Type Description Default <code>iri</code> <code>str</code> <p>IRI of namespace to represent.</p> required <code>label_annotations</code> <code>Union[Sequence, bool]</code> <p>Sequence of label annotations. If given, check the underlying ontology during attribute access if the name correspond to a label. The label annotations should be ordered from highest to lowest precedense. If True is provided, <code>label_annotations</code> is set to <code>(SKOS.prefLabel, RDF.label, SKOS.altLabel)</code>.</p> <code>()</code> <code>check</code> <code>bool</code> <p>Whether to check underlying ontology if the IRI exists during attribute access.  If true, NoSuchIRIError will be raised if the IRI does not exist in this namespace.</p> <code>False</code> <code>reload</code> <code>Optional[bool]</code> <p>Whether to reload the ontology (which is needed when <code>label_annotations</code> or <code>check</code> are given) disregardless whether it has been cached locally.</p> <code>None</code> <code>triplestore</code> <code>Optional[Union[Triplestore, str]]</code> <p>Use this triplestore for label lookup and checking. Can be either a Triplestore object or an URL to load from. Defaults to <code>iri</code>.</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Format to use when loading from a triplestore.</p> <code>None</code> <code>cachemode</code> <code>int</code> <p>Deprecated. Use <code>reload</code> instead (with <code>cachemode=NO_CACHE</code> corresponding to <code>reload=True</code>).</p> <code>-1</code> <code>triplestore_url</code> <code>Optional[str]</code> <p>Deprecated. Use the <code>triplestore</code> argument instead.</p> <code>None</code> Source code in <code>tripper/namespace.py</code> <pre><code>class Namespace:\n    \"\"\"Represent a namespace.\n\n    Arguments:\n        iri: IRI of namespace to represent.\n        label_annotations: Sequence of label annotations. If given, check\n            the underlying ontology during attribute access if the name\n            correspond to a label. The label annotations should be ordered\n            from highest to lowest precedense.\n            If True is provided, `label_annotations` is set to\n            ``(SKOS.prefLabel, RDF.label, SKOS.altLabel)``.\n        check: Whether to check underlying ontology if the IRI exists during\n            attribute access.  If true, NoSuchIRIError will be raised if the\n            IRI does not exist in this namespace.\n        reload: Whether to reload the ontology (which is needed when\n            `label_annotations` or `check` are given) disregardless whether it\n            has been cached locally.\n        triplestore: Use this triplestore for label lookup and checking.\n            Can be either a Triplestore object or an URL to load from.\n            Defaults to `iri`.\n        format: Format to use when loading from a triplestore.\n        cachemode: Deprecated. Use `reload` instead (with `cachemode=NO_CACHE`\n            corresponding to `reload=True`).\n        triplestore_url: Deprecated. Use the `triplestore` argument instead.\n    \"\"\"\n\n    # pylint: disable=too-many-instance-attributes\n\n    __slots__ = (\n        \"_iri\",  # Ontology IRI\n        \"_label_annotations\",  # Recognised annotations for labels\n        \"_check\",  # Whether to check that IRIs exists\n        \"_iris\",  # Dict mapping labels to IRIs\n        \"_reviris\",  # Dict mapping IRIs to labels. The reverse of `_iris`\n        \"_reload\",  # Whether to reload\n        \"_triplestore\",  # Triplestore for label lookup and checking\n        \"_format\",  # Format to use when loading from a triplestore\n    )\n\n    def __init__(\n        self,\n        iri: str,\n        label_annotations: \"Union[Sequence, bool]\" = (),\n        check: bool = False,\n        reload: \"Optional[bool]\" = None,\n        triplestore: \"Optional[Union[Triplestore, str]]\" = None,\n        format: \"Optional[str]\" = None,\n        cachemode: int = -1,\n        triplestore_url: \"Optional[str]\" = None,\n    ):\n        # pylint: disable=redefined-builtin\n        if cachemode != -1:\n            warnings.warn(\n                \"The `cachemode` argument of Triplestore.__init__() is \"\n                \"deprecated.  Use `reload` instead (with `cachemode=NO_CACHE` \"\n                \"corresponding to `reload=True`).\\n\\n\"\n                \"Will be removed in v0.3.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            if reload is None and cachemode == 0:\n                reload = True\n\n        if triplestore_url:\n            warnings.warn(\n                \"The `triplestore_url` argument of Triplestore.__init__() is \"\n                \"deprecated.  Use the `triplestore` argument instead (which \"\n                \"now accepts a string argument with the URL).\\n\\n\"\n                \"Will be removed in v0.3.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            if triplestore is None:\n                triplestore = triplestore_url\n\n        if label_annotations is True:\n            label_annotations = (SKOS.prefLabel, RDF.label, SKOS.altLabel)\n\n        need_triplestore = bool(check or label_annotations)\n\n        self._iri = str(iri)\n        self._label_annotations = (\n            tuple(label_annotations) if label_annotations else ()\n        )\n        self._check = bool(check)\n        self._iris: \"Optional[dict]\" = {} if need_triplestore else None\n        self._reviris: \"dict\" = {}\n        self._reload = reload\n        self._triplestore = triplestore\n        self._format = format\n\n    def _update_iris(self, triplestore=None, reload=False, format=None):\n        \"\"\"Update the internal cache from `triplestore`.\n\n        Arguments:\n            triplestore: Triplestore to update the cache from.\n            reload: If true, reload regardless we have a local cache.\n            format: If `triplestore` is a tring or Path, parse it using this\n                format.\n        \"\"\"\n        # pylint: disable=redefined-builtin\n\n        # Import Triplestore here to avoid cyclic import\n        from .triplestore import (  # pylint: disable=import-outside-toplevel,cyclic-import\n            Triplestore,\n        )\n\n        if not reload and self._load_cache():\n            return\n\n        if triplestore is None:\n            triplestore = self._iri\n\n        if isinstance(triplestore, (str, Path)):\n            # Ignore UnusedArgumentWarning when creating triplestore\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", category=UnusedArgumentWarning)\n                ts = Triplestore(\"rdflib\")\n            ts.parse(triplestore, format=format)\n        elif isinstance(triplestore, Triplestore):\n            ts = triplestore\n        elif not isinstance(triplestore, Triplestore):\n            raise NamespaceError(\n                \"If given, `triplestore` argument must be either a URL \"\n                \"(string), Path or a Triplestore object.\"\n            )\n\n        # Add (label, full_iri) pairs\n        iri = self._iri.rstrip(\"/#\")\n        for label in reversed(self._label_annotations):\n            self._iris.update(\n                (getattr(o, \"value\", o), s)\n                for s, o in ts.subject_objects(label)\n                if s.startswith(iri)\n            )\n\n        # Add (name, full_iri) pairs\n        self._iris.update(\n            (s[len(self._iri) :], s)\n            for s in ts.subjects()\n            if s.startswith(iri)\n        )\n        # Clear _reviris, such that the dict will be regenerated next time\n        # it is needed\n        self._reviris.clear()\n\n    def _get_labels(self, iri):\n        \"\"\"Return annotation labels corresponding to the given IRI.\"\"\"\n        self._update_iris()\n        if not \":\" in iri:\n            iri = self._iri + iri\n        labels = [\n            k\n            for k, v in self._iris.items()\n            if v == iri and iri != self._iri + k\n        ]\n        return labels\n\n    def _get_cachefile(self) -&gt; Path:\n        \"\"\"Return path to cache file for this namespace.\"\"\"\n        # pylint: disable=too-many-function-args\n        name = self._iri.rstrip(\"#/\").rsplit(\"/\", 1)[-1]\n        hashno = hashlib.shake_128(self._iri.encode()).hexdigest(5)\n        return get_cachedir() / f\"{name}-{hashno}.cache\"\n\n    def _save_cache(self):\n        \"\"\"Save current cache.\"\"\"\n        # pylint: disable=invalid-name\n        try:\n            cachefile = self._get_cachefile()\n            if self._iris and not sys.is_finalizing():\n                with open(cachefile, \"wb\") as f:\n                    pickle.dump(self._iris, f)\n        except OSError as exc:\n            warnings.warn(\n                f\"Cannot access cache file: {exc}\\n\\n\"\n                \"You can select cache directory with the XDG_CACHE_HOME \"\n                \"environment variable.\"\n            )\n\n    def _load_cache(self) -&gt; bool:\n        \"\"\"Update cache with cache file.\n\n        Returns true if there exists a cache file to load from.\n        \"\"\"\n        # pylint: disable=invalid-name\n        try:\n            cachefile = self._get_cachefile()\n            if self._iris is None:\n                self._iris = {}\n            if cachefile.exists():\n                with open(cachefile, \"rb\") as f:\n                    self._iris.update(pickle.load(f))  # nosec\n                return True\n            return False\n        except OSError as exc:\n            warnings.warn(\n                f\"Cannot create cache directory: {exc}\\n\\n\"\n                \"You can select cache directory with the XDG_CACHE_HOME \"\n                \"environment variable.\"\n            )\n            return False\n\n    def __getattr__(self, name):\n        if self._iris and name in self._iris:\n            return self._iris[name]\n        if self._iris == {}:\n            self._update_iris(\n                triplestore=self._triplestore,\n                reload=self._reload,\n                format=self._format,\n            )\n            if name in self._iris:\n                return self._iris[name]\n        if self._check:\n\n            # Hack to work around a pytest bug.  During its collection\n            # phase pytest tries to mock namespace objects with an\n            # attribute `__wrapped__`.\n            if name == \"__wrapped__\":\n                return super().__getattr__(self, name)\n\n            msg = \"\"\n            try:\n                cachefile = self._get_cachefile()\n                if cachefile.exists():\n                    msg = (\n                        \"\\nMaybe you have to remove the cache file: \"\n                        f\"{cachefile}\"\n                    )\n            except OSError as exc:\n                warnings.warn(\n                    f\"Cannot access cache file: {exc}\\n\\n\"\n                    \"You can select cache directory with the XDG_CACHE_HOME \"\n                    \"environment variable.\"\n                )\n            raise NoSuchIRIError(self._iri + name + msg)\n        return self._iri + name\n\n    def __getitem__(self, key):\n        return self.__getattr__(key)\n\n    def __repr__(self):\n        return f\"Namespace('{self._iri}')\"\n\n    def __str__(self):\n        return self._iri\n\n    def __add__(self, other):\n        return self._iri + str(other)\n\n    def __contains__(self, name):\n        return name in self._iri\n\n    def __hash__(self):\n        return hash(self._iri)\n\n    def __eq__(self, other):\n        return self._iri == str(other)\n\n    def __del__(self):\n        if self._iris:\n            self._save_cache()\n\n    def __dir__(self):\n        names = dir(self.__class__)\n        if self._iris == {}:\n            self._update_iris(\n                triplestore=self._triplestore,\n                reload=self._reload,\n                format=self._format,\n            )\n        if self._iris:\n            names += list(self._iris.keys())\n        return names\n\n    def __neg__(self):\n        \"\"\"The negation operator (-) is a short-hand for returning the base\n        iri with any trailing slash or hash stripped off.\n\n        Examples:\n\n        &gt;&gt;&gt; from tripper import OWL\n        &gt;&gt;&gt; -OWL\n        'http://www.w3.org/2002/07/owl'\n\n        \"\"\"\n        return self._iri.rstrip(\"/#\")\n\n    def __pos__(self):\n        \"\"\"The positive operator (+) is a short-hand for returning the base\n        iri.\n\n        Examples:\n\n        &gt;&gt;&gt; from tripper import OWL\n        &gt;&gt;&gt; +OWL\n        'http://www.w3.org/2002/07/owl#'\n\n        \"\"\"\n        return self._iri\n\n    def __call__(self, iri):\n        \"\"\"Returns the name or label associated with the given iri.\"\"\"\n        if not iri.startswith(self._iri):\n            raise NamespaceError(\n                f\"IRI '{iri}' is not in namespace '{self._iri}'\"\n            )\n        if self._iris is None:\n            return iri[len(self._iri) :]\n        if not self._reviris:\n            if not self._iris:\n                self._update_iris(\n                    triplestore=self._triplestore,\n                    reload=self._reload,\n                    format=self._format,\n                )\n            self._reviris = {\n                iri: label for label, iri in reversed(self._iris.items())\n            }\n        if iri not in self._reviris:\n            cachefile = self._get_cachefile()\n            raise NoSuchIRIError(\n                f\"{iri}\\n\"\n                f\"Maybe you have to remove the cache file: {cachefile}\"\n            )\n        return self._reviris[iri]\n</code></pre>"},{"location":"api_reference/namespace/#tripper.namespace.Namespace.__call__","title":"<code>__call__(self, iri)</code>  <code>special</code>","text":"<p>Returns the name or label associated with the given iri.</p> Source code in <code>tripper/namespace.py</code> <pre><code>def __call__(self, iri):\n    \"\"\"Returns the name or label associated with the given iri.\"\"\"\n    if not iri.startswith(self._iri):\n        raise NamespaceError(\n            f\"IRI '{iri}' is not in namespace '{self._iri}'\"\n        )\n    if self._iris is None:\n        return iri[len(self._iri) :]\n    if not self._reviris:\n        if not self._iris:\n            self._update_iris(\n                triplestore=self._triplestore,\n                reload=self._reload,\n                format=self._format,\n            )\n        self._reviris = {\n            iri: label for label, iri in reversed(self._iris.items())\n        }\n    if iri not in self._reviris:\n        cachefile = self._get_cachefile()\n        raise NoSuchIRIError(\n            f\"{iri}\\n\"\n            f\"Maybe you have to remove the cache file: {cachefile}\"\n        )\n    return self._reviris[iri]\n</code></pre>"},{"location":"api_reference/namespace/#tripper.namespace.Namespace.__neg__","title":"<code>__neg__(self)</code>  <code>special</code>","text":"<p>The negation operator (-) is a short-hand for returning the base iri with any trailing slash or hash stripped off.</p> <p>Examples:</p> <p>from tripper import OWL -OWL 'http://www.w3.org/2002/07/owl'</p> Source code in <code>tripper/namespace.py</code> <pre><code>def __neg__(self):\n    \"\"\"The negation operator (-) is a short-hand for returning the base\n    iri with any trailing slash or hash stripped off.\n\n    Examples:\n\n    &gt;&gt;&gt; from tripper import OWL\n    &gt;&gt;&gt; -OWL\n    'http://www.w3.org/2002/07/owl'\n\n    \"\"\"\n    return self._iri.rstrip(\"/#\")\n</code></pre>"},{"location":"api_reference/namespace/#tripper.namespace.Namespace.__pos__","title":"<code>__pos__(self)</code>  <code>special</code>","text":"<p>The positive operator (+) is a short-hand for returning the base iri.</p> <p>Examples:</p> <p>from tripper import OWL +OWL 'http://www.w3.org/2002/07/owl#'</p> Source code in <code>tripper/namespace.py</code> <pre><code>def __pos__(self):\n    \"\"\"The positive operator (+) is a short-hand for returning the base\n    iri.\n\n    Examples:\n\n    &gt;&gt;&gt; from tripper import OWL\n    &gt;&gt;&gt; +OWL\n    'http://www.w3.org/2002/07/owl#'\n\n    \"\"\"\n    return self._iri\n</code></pre>"},{"location":"api_reference/namespace/#tripper.namespace.get_cachedir","title":"<code>get_cachedir(create=True)</code>","text":"<p>Returns cross-platform path to tripper cache directory.</p> <p>If <code>create</code> is true, create the cache directory if it doesn't exists.</p> <p>The XDG_CACHE_HOME environment variable is used if it exists.</p> Source code in <code>tripper/namespace.py</code> <pre><code>def get_cachedir(create=True) -&gt; Path:\n    \"\"\"Returns cross-platform path to tripper cache directory.\n\n    If `create` is true, create the cache directory if it doesn't exists.\n\n    The XDG_CACHE_HOME environment variable is used if it exists.\n    \"\"\"\n    site_cachedir = os.getenv(\"XDG_CACHE_HOME\")\n    finaldir = None\n    if not site_cachedir:\n        if sys.platform.startswith(\"win32\"):\n            site_cachedir = Path.home() / \"AppData\" / \"Local\"\n            finaldir = \"Cache\"\n        elif sys.platform.startswith(\"darwin\"):\n            site_cachedir = Path.home() / \"Library\" / \"Caches\"\n        else:  # Default to UNIX\n            site_cachedir = Path.home() / \".cache\"  # type: ignore\n    cachedir = Path(site_cachedir) / \"tripper\"  # type: ignore\n    if finaldir:\n        cachedir /= finaldir\n\n    if create:\n        try:\n            cachedir.mkdir(parents=True, exist_ok=True)\n        except PermissionError as exc:\n            warnings.warn(\n                f\"{exc}: {cachedir}\",\n                category=PermissionWarning,\n            )\n\n    return cachedir\n</code></pre>"},{"location":"api_reference/session/","title":"session","text":"<p>A session that makes it easy to manage triplestore connections.</p>"},{"location":"api_reference/session/#tripper.session.Session","title":"<code> Session        </code>","text":"<p>A class making it easy to access pre-configured triplestores.</p> <p>Each triplestore is identified by a name and configured in a user-defined YAML file.</p> <p>See https://emmc-asbl.github.io/tripper/latest/session/ for more info.</p> <p>This class can also be used to configure other resources. The easies is to create a subclass that implements a method similar to get_triplestore() for the other resource.</p> Source code in <code>tripper/session.py</code> <pre><code>class Session:\n    \"\"\"A class making it easy to access pre-configured triplestores.\n\n    Each triplestore is identified by a name and configured in a\n    user-defined YAML file.\n\n    See https://emmc-asbl.github.io/tripper/latest/session/ for more info.\n\n    This class can also be used to configure other resources. The\n    easies is to create a subclass that implements a method similar to\n    get_triplestore() for the other resource.\n\n    \"\"\"\n\n    # pylint: disable=too-few-public-methods\n\n    def __init__(self, config: \"Optional[Union[Path, str]]\" = None):\n        \"\"\"Initialises a session.\n\n        Arguments:\n            config: Configuration file.\n\n\n        The default location of the configuration file depends on the system:\n\n        - Linux: $HOME/.config/tripper/session.yaml\n        - Windows: $HOME/AppData/Local/tripper/Config/session.yaml\n        - Darwin: $HOME/Library/Config/tripper/session.yaml\n\n        \"\"\"\n        import yaml  # pylint: disable=import-outside-toplevel,import-error\n\n        if config is None:\n            config = get_configdir() / \"session.yaml\"\n\n        with open(config, \"rt\", encoding=\"utf-8\") as f:\n            self.sessions = yaml.safe_load(f)\n\n    def _get_config(self, name: str, password: \"Optional[str]\" = None) -&gt; dict:\n        \"\"\"Returns configrations for the named service.\"\"\"\n        if name not in self.sessions:\n            raise ValueError(f\"no session configured for: '{name}'\")\n\n        conf = self.sessions[name]\n\n        if \"password\" in conf:\n            if password:\n                conf[\"password\"] = password\n            elif \"username\" in conf and conf[\"password\"] == \"KEYRING\":\n                import keyring  # pylint: disable=import-outside-toplevel,import-error\n\n                conf[\"password\"] = keyring.get_password(name, conf[\"username\"])\n\n        return conf\n\n    def get_names(self) -&gt; list:\n        \"\"\"Return a list with all configured session names.\"\"\"\n        return list(self.sessions.keys())\n\n    def get_triplestore(\n        self, name: str, password: \"Optional[str]\" = None\n    ) -&gt; \"Triplestore\":\n        \"\"\"Return a new triplestore instance with the given name.\"\"\"\n        conf = self._get_config(name, password=password)\n\n        return Triplestore(**conf)\n</code></pre>"},{"location":"api_reference/session/#tripper.session.Session.__init__","title":"<code>__init__(self, config=None)</code>  <code>special</code>","text":"<p>Initialises a session.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[Path, str]]</code> <p>Configuration file.</p> <code>None</code> <p>The default location of the configuration file depends on the system:</p> <ul> <li>Linux: $HOME/.config/tripper/session.yaml</li> <li>Windows: $HOME/AppData/Local/tripper/Config/session.yaml</li> <li>Darwin: $HOME/Library/Config/tripper/session.yaml</li> </ul> Source code in <code>tripper/session.py</code> <pre><code>def __init__(self, config: \"Optional[Union[Path, str]]\" = None):\n    \"\"\"Initialises a session.\n\n    Arguments:\n        config: Configuration file.\n\n\n    The default location of the configuration file depends on the system:\n\n    - Linux: $HOME/.config/tripper/session.yaml\n    - Windows: $HOME/AppData/Local/tripper/Config/session.yaml\n    - Darwin: $HOME/Library/Config/tripper/session.yaml\n\n    \"\"\"\n    import yaml  # pylint: disable=import-outside-toplevel,import-error\n\n    if config is None:\n        config = get_configdir() / \"session.yaml\"\n\n    with open(config, \"rt\", encoding=\"utf-8\") as f:\n        self.sessions = yaml.safe_load(f)\n</code></pre>"},{"location":"api_reference/session/#tripper.session.Session.get_names","title":"<code>get_names(self)</code>","text":"<p>Return a list with all configured session names.</p> Source code in <code>tripper/session.py</code> <pre><code>def get_names(self) -&gt; list:\n    \"\"\"Return a list with all configured session names.\"\"\"\n    return list(self.sessions.keys())\n</code></pre>"},{"location":"api_reference/session/#tripper.session.Session.get_triplestore","title":"<code>get_triplestore(self, name, password=None)</code>","text":"<p>Return a new triplestore instance with the given name.</p> Source code in <code>tripper/session.py</code> <pre><code>def get_triplestore(\n    self, name: str, password: \"Optional[str]\" = None\n) -&gt; \"Triplestore\":\n    \"\"\"Return a new triplestore instance with the given name.\"\"\"\n    conf = self._get_config(name, password=password)\n\n    return Triplestore(**conf)\n</code></pre>"},{"location":"api_reference/session/#tripper.session.get_configdir","title":"<code>get_configdir(create=True)</code>","text":"<p>Returns cross-platform path to tripper config directory.</p> <p>If <code>create</code> is true, create the config directory if it doesn't exists.</p> <p>The XDG_CONFIG_HOME environment variable is used if it exists.</p> Source code in <code>tripper/session.py</code> <pre><code>def get_configdir(create: bool = True) -&gt; Path:\n    \"\"\"Returns cross-platform path to tripper config directory.\n\n    If `create` is true, create the config directory if it doesn't exists.\n\n    The XDG_CONFIG_HOME environment variable is used if it exists.\n    \"\"\"\n    site_configdir = os.getenv(\"XDG_CONFIG_HOME\")\n    if not site_configdir:\n        site_configdir = os.getenv(\"XDG_CONFIG_DIRS\")\n        if site_configdir:\n            site_configdir = site_configdir.split(\":\")[0]\n\n    finaldir = None\n    if not site_configdir:\n        if sys.platform.startswith(\"win32\"):\n            site_configdir = Path.home() / \"AppData\" / \"Local\"\n            finaldir = \"Config\"\n        elif sys.platform.startswith(\"darwin\"):\n            site_configdir = Path.home() / \"Library\" / \"Config\"\n        else:  # Default to UNIX\n            site_configdir = Path.home() / \".config\"  # type: ignore\n    configdir = Path(site_configdir) / \"tripper\"  # type: ignore\n    if finaldir:\n        configdir /= finaldir\n\n    if create:\n        path = Path(configdir.root)\n        for part in configdir.parts[1:]:\n            path /= part\n            if not path.exists():\n                path.mkdir()\n\n    return configdir\n</code></pre>"},{"location":"api_reference/triplestore/","title":"triplestore","text":"<p>A module encapsulating different triplestores using the strategy design pattern.</p> <p>For a list over available backends, see https://emmc-asbl.github.io/tripper/latest/#available-backends</p> <p>This module has no dependencies outside the standard library, but the triplestore backends may have.</p> <p>For developers: The usage of <code>s</code>, <code>p</code>, and <code>o</code> represent the different parts of an RDF Triple: subject, predicate, and object.</p>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore","title":"<code> Triplestore        </code>","text":"<p>Provides a common frontend to a range of triplestore backends.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>class Triplestore:\n    \"\"\"Provides a common frontend to a range of triplestore backends.\"\"\"\n\n    # pylint: disable=too-many-instance-attributes\n\n    default_namespaces = {\n        \"xml\": XML,\n        \"rdf\": RDF,\n        \"rdfs\": RDFS,\n        \"xsd\": XSD,\n        \"owl\": OWL,\n        # \"skos\": SKOS,\n        # \"dcat\": DCAT,\n        # \"dc\": DC,\n        # \"dcterms\": DCTERMS,\n        # \"foaf\": FOAF,\n        # \"doap\": DOAP,\n        # \"map\": MAP,\n        # \"dm\": DM,\n    }\n\n    def __init__(\n        self,\n        backend: str,\n        base_iri: \"Optional[str]\" = None,\n        database: \"Optional[str]\" = None,\n        package: \"Optional[str]\" = None,\n        check_url: \"Optional[str]\" = None,  # Deprecated\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialise triplestore using the backend with the given name.\n\n        Arguments:\n            backend: Name of the backend module.\n\n                For built-in backends or backends provided via a\n                backend package (using entrypoints), this should just\n                be the name of the backend with no dots (ex: \"rdflib\").\n\n                For a custom backend, you can provide the full module name,\n                including the dots (ex:\"mypackage.mybackend\").  If `package`\n                is given, `backend` is interpreted relative to `package`\n                (ex: ..mybackend).\n\n                For a list over available backends, see\n                https://github.com/EMMC-ASBL/tripper#available-backends\n\n            base_iri: Base IRI used by the add_function() method when adding\n                new triples. May also be used by the backend.\n            database: Name of database to connect to (for backends that\n                supports it).\n            package: Required when `backend` is a relative module.  In that\n                case, it is relative to `package`.\n            check_url: Deprecated. A URL for checking whether the backend\n                is available. Use the optional keyword argument `check_iri`\n                instead. Defaults to `base_iri`.\n            kwargs: Keyword arguments passed to the backend's __init__()\n                method.\n\n        Attributes:\n            backend_name: Name of backend.\n            base_iri: Assigned to the `base_iri` argument.\n            closed: Whether the triplestore is closed.\n            kwargs: Dict with additional keyword arguments.\n            namespaces: Dict mapping namespace prefixes to IRIs.\n            package: Name of Python package if the backend is implemented as\n                a relative module. Assigned to the `package` argument.\n\n        Notes:\n            If the backend establishes a connection that should be closed\n            it is useful to instantiate the Triplestore as a context manager:\n\n                &gt;&gt;&gt; import tripper\n                &gt;&gt;&gt; with tripper.Triplestore(\"rdflib\") as ts:\n                ...     print(ts.backend_name)\n                rdflib\n\n            This ensures that the connection is automatically closed when the\n            context manager exits.\n        \"\"\"\n        if check_url:\n            warnings.warn(\n                \"Argument `check_url` is deprecated. It defaults to \"\n                \"`base_iri` and is rarely needed.  If needed, `check_iri` \"\n                \"instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        backend_name = backend.rsplit(\".\", 1)[-1]\n        module = self._load_backend(backend, package)\n        cls = getattr(module, f\"{backend_name.title()}Strategy\")\n        self.base_iri = base_iri\n        self.namespaces: \"Dict[str, Namespace]\" = {}\n        self.closed = False\n        self.backend_name = backend_name\n        self.database = database\n        self.package = package\n        self.check_url = check_url if check_url else base_iri  # Deprecated\n        self.kwargs = kwargs.copy()\n        self.backend = cls(base_iri=base_iri, database=database, **kwargs)\n\n        # Cache functions in the triplestore for fast access\n        self.function_repo: \"Dict[str, Union[float, Callable, None]]\" = {}\n\n        for prefix, namespace in self.default_namespaces.items():\n            self.bind(prefix, namespace)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc):\n        self.close()\n\n    @classmethod\n    def _load_backend(cls, backend: str, package: \"Optional[str]\" = None):\n        \"\"\"Load and return backend module.  The arguments has the same meaning\n        as corresponding arguments to __init__().\n\n        If `backend` contains a dot or `package` is given, import `backend`\n        using `package` for relative imports.\n\n        Otherwise, if there in the \"tripper.backends\" entry point group exists\n        an entry point who's name matches `backend`, then the corresponding\n        module is loaded.\n\n        Otherwise, look for the `backend` in any of the (sub)packages listed\n        `backend_packages` module variable.\n        \"\"\"\n        # Explicitly specified backend\n        if \".\" in backend or package:\n            return importlib.import_module(backend, package)\n\n        # Installed backend package\n        for entry_point in get_entry_points(\"tripper.backends\"):\n            if entry_point.name == backend:\n                return importlib.import_module(entry_point.module)\n\n        # Backend module\n        for pack in backend_packages:\n            try:\n                return importlib.import_module(f\"{pack}.{backend}\")\n            except ModuleNotFoundError:\n                pass\n\n        raise ModuleNotFoundError(\n            f\"No tripper backend named '{backend}'\",\n            name=backend,\n        )\n\n    # Methods implemented by backend\n    # ------------------------------\n    def triples(  # pylint: disable=redefined-builtin\n        self,\n        subject: \"Optional[Union[str, Triple]]\" = None,\n        predicate: \"Optional[str]\" = None,\n        object: \"Optional[Union[str, Literal]]\" = None,\n        triple: \"Optional[Triple]\" = None,\n    ) -&gt; \"Generator[Triple, None, None]\":\n        \"\"\"Returns a generator over matching triples.\n\n        Arguments:\n            subject: If given, match triples with this subject.\n            predicate: If given, match triples with this predicate.\n            object: If given, match triples with this object.\n            triple: Deprecated. A `(s, p, o)` tuple where `s`, `p` and `o`\n                should either be None (matching anything) or an exact IRI\n                to match.\n\n        Returns:\n            Generator over all matching triples.\n        \"\"\"\n        # __TODO__: Remove these lines when deprecated\n        if triple or (subject and not isinstance(subject, str)):\n            warnings.warn(\n                \"The `triple` argument is deprecated.  Use `subject`, \"\n                \"`predicate` and `object` arguments instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        if subject and not isinstance(subject, str):\n            subject, predicate, object = subject\n        elif triple:\n            subject, predicate, object = triple\n\n        return self.backend.triples((subject, predicate, object))\n\n    def add_triples(self, triples: \"Iterable[Triple]\") -&gt; None:\n        \"\"\"Add a sequence of triples.\n\n        Arguments:\n            triples: A sequence of `(s, p, o)` tuples to add to the\n                triplestore.\n        \"\"\"\n        self.backend.add_triples(triples)\n\n    def remove(  # pylint: disable=redefined-builtin\n        self,\n        subject: \"Optional[Union[str, Triple]]\" = None,\n        predicate: \"Optional[str]\" = None,\n        object: \"Optional[Union[str, Literal]]\" = None,\n        triple: \"Optional[Triple]\" = None,\n    ) -&gt; None:\n        \"\"\"Remove all matching triples from the backend.\n\n        Arguments:\n            subject: If given, match triples with this subject.\n                For backward compatibility `subject` may also be an\n                `(s, p, o)` triple.\n            predicate: If given, match triples with this predicate.\n            object: If given, match triples with this object.\n            triple: Deprecated. A `(s, p, o)` tuple where `s`, `p` and `o`\n                should either be None (matching anything) or an exact IRI\n                to match.\n        \"\"\"\n        # __TODO__: Remove these lines when deprecated\n        if triple or (subject and not isinstance(subject, str)):\n            warnings.warn(\n                \"The `triple` argument is deprecated.  Use `subject`, \"\n                \"`predicate` and `object` arguments instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        if subject and not isinstance(subject, str):\n            subject, predicate, object = subject\n        elif triple:\n            subject, predicate, object = triple\n\n        return self.backend.remove((subject, predicate, object))\n\n    # Methods optionally implemented by backend\n    # -----------------------------------------\n    def close(self) -&gt; None:\n        \"\"\"Calls the backend close() method if it is implemented.\n        Otherwise, this method has no effect.\n        \"\"\"\n        # It should be ok to call close() regardless of whether the backend\n        # implements this method or not.  Hence, don't call _check_method().\n        if not self.closed and hasattr(self.backend, \"close\"):\n            self.backend.close()\n        self.closed = True\n\n    def parse(\n        self,\n        source=None,\n        format=None,\n        fallback_backend=\"rdflib\",\n        fallback_backend_kwargs=None,\n        **kwargs,  # pylint: disable=redefined-builtin\n    ) -&gt; None:\n        \"\"\"Parse source and add the resulting triples to triplestore.\n\n        Arguments:\n            source: File-like object. File name or URL.\n            format: Needed if format can not be inferred from source.\n            fallback_backend: If the current backend doesn't implement\n                parse, use the `fallback_backend` instead.\n            fallback_backend_kwargs: Dict with additional keyword arguments\n                for initialising `fallback_backend`.\n            kwargs: Keyword arguments passed to the backend.\n                The rdflib backend supports e.g. `location` (absolute\n                or relative URL) and `data` (string containing the\n                data to be parsed) arguments.\n        \"\"\"\n        if hasattr(self.backend, \"parse\"):\n            self._check_method(\"parse\")\n            self.backend.parse(source=source, format=format, **kwargs)\n        else:\n            if fallback_backend_kwargs is None:\n                fallback_backend_kwargs = {}\n            ts = Triplestore(\n                backend=fallback_backend, **fallback_backend_kwargs\n            )\n            ts.parse(source=source, format=format, **kwargs)\n            self.add_triples(ts.triples())\n\n        if hasattr(self.backend, \"namespaces\"):\n            for prefix, namespace in self.backend.namespaces().items():\n                if prefix and prefix not in self.namespaces:\n                    self.namespaces[prefix] = Namespace(namespace)\n\n    def serialize(\n        self,\n        destination=None,\n        format=\"turtle\",  # pylint: disable=redefined-builtin\n        fallback_backend=\"rdflib\",\n        fallback_backend_kwargs=None,\n        **kwargs,\n    ) -&gt; \"Union[None, str]\":\n        \"\"\"Serialise triplestore.\n\n        Arguments:\n            destination: File name or object to write to.  If None, the\n                serialisation is returned.\n            format: Format to serialise as.  Supported formats, depends on\n                the backend.\n            fallback_backend: If the current backend doesn't implement\n                serialisation, use the `fallback_backend` instead.\n            fallback_backend_kwargs: Dict with additional keyword arguments\n                for initialising `fallback_backend`.\n            kwargs: Passed to the backend serialize() method.\n\n        Returns:\n            Serialized string if `destination` is None.\n        \"\"\"\n        if hasattr(self.backend, \"parse\"):\n            self._check_method(\"serialize\")\n            return self.backend.serialize(\n                destination=destination, format=format, **kwargs\n            )\n\n        if fallback_backend_kwargs is None:\n            fallback_backend_kwargs = {}\n        ts = Triplestore(backend=fallback_backend, **fallback_backend_kwargs)\n        ts.add_triples(self.triples())\n        for prefix, iri in self.namespaces.items():\n            ts.bind(prefix, iri)\n        return ts.serialize(destination=destination, format=format, **kwargs)\n\n    def query(\n        self,\n        query: str,\n        iris: \"Optional[dict]\" = None,\n        literals: \"Optional[dict]\" = None,\n        **kwargs,\n    ) -&gt; \"Any\":\n        \"\"\"SPARQL query.\n\n        The `query` argument may contain variables for IRIs and literals,\n        to be substituted using the `iris` and `literals` arguments. These\n        variables are prefixed `$`. This makes them easy to distinguish from\n        query variables, that are typically prefixed with `?`.\n\n        The query substitutions may be useful when the query is constructed\n        from user input, since they are properly escaped and will be inserted\n        in the query as a single token.  This may prevent sparql injection\n        attacks.\n\n        Arguments:\n            query: String with the SPARQL query.\n            iris: Dict used for query substitutions that maps IRI variables\n                to IRIs. The IRIs may be provided as fully expanded or\n                prefixed with a prefix registered in the triplestore namespace.\n            literals: Dict used for query substitutions that maps literal\n                variables to literals.\n            kwargs: Keyword arguments passed to the backend query() method.\n\n        Returns:\n            The return type depends on type of query:\n              - SELECT: list of tuples of IRIs for each matching row\n              - ASK: bool\n              - CONSTRUCT, DESCRIBE: generator over triples\n\n        Note:\n            This method is intended for SELECT, ASK, CONSTRUCT and\n            DESCRIBE queries.  Use the update() method for INSERT and\n            DELETE queries.\n\n            Not all backends may support all types of queries.\n\n        Examples:\n            Query for everyone with the name \"John Dow\":\n\n            &gt;&gt;&gt; from tripper import FOAF, Literal, Triplestore\n            &gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n            &gt;&gt;&gt; ts.bind(\"foaf\", FOAF)\n            Namespace('http://xmlns.com/foaf/0.1/')\n\n            &gt;&gt;&gt; ts.add_triples([\n            ...     (\":john\", FOAF.name, Literal(\"John Dow\")),\n            ...     (\":jack\", FOAF.name, Literal(\"Jack Hudson\")),\n            ... ])\n            &gt;&gt;&gt; ts.query(\n            ...     \"SELECT ?s WHERE { ?s $name $obj .}\",\n            ...     iris={\"name\": \"foaf:name\"},\n            ...     literals={\"obj\": \"John Dow\"},\n            ... )\n            [(':john',)]\n\n        \"\"\"\n        self._check_method(\"query\")\n        new_query = substitute_query(\n            query, iris=iris, literals=literals, prefixes=self.namespaces\n        )\n        return self.backend.query(new_query, **kwargs)\n\n    def update(\n        self,\n        query: str,\n        iris: \"Optional[dict]\" = None,\n        literals: \"Optional[dict]\" = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Update triplestore with SPARQL.\n\n        Arguments:\n            query: String with the SPARQL query.\n            iris: Dict used for query substitutions that maps IRI variables\n                to IRIs. The IRIs may be provided as fully expanded or\n                prefixed with a prefix registered in the triplestore namespace.\n            literals: Dict used for query substitutions that maps literal\n                variables to literals.\n            kwargs: Keyword arguments passed to the backend update() method.\n\n        Note:\n            See `query()` for how to the query substitution arguments `iris`\n            and `literals`.\n\n            This method is intended for INSERT and DELETE queries. Use\n            the query() method for SELECT, ASK, CONSTRUCT and DESCRIBE queries.\n\n        \"\"\"\n        self._check_method(\"update\")\n        new_query = substitute_query(\n            query, iris=iris, literals=literals, prefixes=self.namespaces\n        )\n        return self.backend.update(new_query, **kwargs)\n\n    @overload\n    def bind(\n        self,\n        prefix: str,\n        namespace: \"Union[str, Namespace, Triplestore]\",\n        **kwargs,\n    ) -&gt; \"Namespace\": ...\n\n    @overload\n    def bind(\n        self,\n        prefix: str,\n        namespace: None,\n        **kwargs,\n    ) -&gt; None: ...\n\n    def bind(\n        self,\n        prefix: str,\n        namespace: \"Union[str, Namespace, Triplestore, None]\" = \"\",\n        **kwargs,\n    ) -&gt; \"Union[Namespace, None]\":\n        \"\"\"Bind prefix to namespace and return the new Namespace object.\n\n        Arguments:\n            prefix: Prefix to bind the the namespace.\n            namespace: Namespace to bind to.  The default is to bind to the\n                `base_iri` of the current triplestore.\n                If `namespace` is None, the corresponding prefix is removed.\n            kwargs: Keyword arguments are passed to the Namespace()\n                constructor.\n\n        Returns:\n            New Namespace object or None if namespace is removed.\n        \"\"\"\n        if namespace == \"\":\n            namespace = self\n\n        if isinstance(namespace, str):\n            ns = Namespace(namespace, **kwargs)\n        elif isinstance(namespace, Triplestore):\n            if not namespace.base_iri:\n                raise ValueError(\n                    f\"triplestore object {namespace} has no `base_iri`\"\n                )\n            ns = Namespace(namespace.base_iri, **kwargs)\n        elif isinstance(namespace, Namespace):\n            # pylint: disable=protected-access\n            ns = Namespace(namespace._iri, **kwargs)\n        elif namespace is None:\n            del self.namespaces[prefix]\n            if hasattr(self.backend, \"bind\"):\n                self.backend.bind(prefix, None)\n            return None\n        else:\n            raise TypeError(f\"invalid `namespace` type: {type(namespace)}\")\n\n        if hasattr(self.backend, \"bind\"):\n            self.backend.bind(\n                prefix, ns._iri  # pylint: disable=protected-access\n            )\n\n        self.namespaces[prefix] = ns\n        return ns\n\n    def is_available(self, timeout: float = 5, interval: float = 1) -&gt; bool:\n        \"\"\"Checks if the backend is available.\n\n        Arguments:\n            timeout: Total time in seconds to wait for a response.\n            interval: Internal time interval in seconds between checking if\n                the service has responded.\n\n        Returns:\n            Returns true if the backend is available.\n        \"\"\"\n        if hasattr(self.backend, \"is_available\"):\n            return self.backend.is_available(\n                timeout=timeout, interval=interval\n            )\n        return True\n\n    @classmethod\n    def create_database(cls, backend: str, database: str, **kwargs):\n        \"\"\"Create a new database in backend.\n\n        Arguments:\n            backend: Name of backend.\n            database: Name of the new database.\n            kwargs: Keyword arguments passed to the backend\n                create_database() method.\n\n        Note:\n            This is a class method, which operates on the backend\n            triplestore without connecting to it.\n        \"\"\"\n        cls._check_backend_method(backend, \"create_database\")\n        backend_class = cls._get_backend(backend)\n        return backend_class.create_database(database=database, **kwargs)\n\n    @classmethod\n    def remove_database(cls, backend: str, database: str, **kwargs):\n        \"\"\"Remove a database in backend.\n\n        Arguments:\n            backend: Name of backend.\n            database: Name of the database to be removed.\n            kwargs: Keyword arguments passed to the backend\n                remove_database() method.\n\n        Note:\n            This is a class method, which operates on the backend\n            triplestore without connecting to it.\n        \"\"\"\n        cls._check_backend_method(backend, \"remove_database\")\n        backend_class = cls._get_backend(backend)\n        return backend_class.remove_database(database=database, **kwargs)\n\n    @classmethod\n    def list_databases(cls, backend: str, **kwargs):\n        \"\"\"For backends that supports multiple databases, list of all\n        databases.\n\n        Arguments:\n            backend: Name of backend.\n            kwargs: Keyword arguments passed to the backend\n                list_databases() method.\n\n        Note:\n            This is a class method, which operates on the backend\n            triplestore without connecting to it.\n        \"\"\"\n        cls._check_backend_method(backend, \"list_databases\")\n        backend_class = cls._get_backend(backend)\n        return backend_class.list_databases(**kwargs)\n\n    # Convenient methods\n    # ------------------\n    # These methods are modelled after rdflib and provide some convinient\n    # interfaces to the triples(), add_triples() and remove() methods\n    # implemented by all backends.\n\n    prefer_sparql = property(\n        fget=lambda self: getattr(self.backend, \"prefer_sparql\", None),\n        doc=(\n            \"Whether the backend prefer SPARQL over the triples() interface. \"\n            \"Is None if not specified by the backend.\"\n            \"\\n\\n\"\n            \"Even though Tripper requires that the Triplestore.triples() is \"\n            \"implemented, Triplestore.query() must be used for some \"\n            \"backends in specific cases (like fuseki when working on RDF \"\n            \"lists) because of how blank nodes are treated. \"\n            \"\\n\\n\"\n            \"The purpose of this property is to let tripper \"\n            \"automatically select the most appropriate interface depending \"\n            \"on the current backend settings.\"\n        ),\n    )\n\n    @classmethod\n    def _check_backend_method(cls, backend: str, name: str):\n        \"\"\"Checks that `backend` has a method called `name`.\n\n        Raises NotImplementedError if it hasn't.\n        \"\"\"\n        backend_class = cls._get_backend(backend)\n        if not hasattr(backend_class, name):\n            raise NotImplementedError(\n                f\"Triplestore backend {backend!r} do not implement a \"\n                f'\"{name}()\" method.'\n            )\n\n    @classmethod\n    def _get_backend(cls, backend: str, package: \"Optional[str]\" = None):\n        \"\"\"Returns the class implementing the given backend.\"\"\"\n        module = cls._load_backend(backend, package=package)\n        return getattr(module, f\"{backend.title()}Strategy\")\n\n    def _check_method(self, name):\n        \"\"\"Check that backend implements the given method.\"\"\"\n        self._check_backend_method(self.backend_name, name)\n\n    def add(self, triple: \"Triple\"):\n        \"\"\"Add `triple` to triplestore.\"\"\"\n        self.add_triples([triple])\n\n    def value(  # pylint: disable=redefined-builtin\n        self,\n        subject=None,\n        predicate=None,\n        object=None,\n        default=None,\n        any=False,\n        lang=None,\n    ) -&gt; \"Union[str, Literal]\":\n        \"\"\"Return the value for a pair of two criteria.\n\n        Useful if one knows that there may only be one value.\n        Two of `subject`, `predicate` or `object` must be provided.\n\n        Arguments:\n            subject: Possible criteria to match.\n            predicate: Possible criteria to match.\n            object: Possible criteria to match.\n            default: Value to return if no matches are found.\n            any: Used to define how many values to return. Can be set to:\n                `False` (default): return the value or raise UniquenessError\n                if there is more than one matching value.\n                `True`: return any matching value if there is more than one.\n                `None`: return a generator over all matching values.\n            lang: If provided, require that the value must be a localised\n                literal with the given language code.\n\n        Returns:\n            The value of the `subject`, `predicate` or `object` that is\n            None.\n        \"\"\"\n        spo = (subject, predicate, object)\n        if sum(iri is None for iri in spo) != 1:\n            raise ValueError(\n                \"Exactly one of `subject`, `predicate` or `object` must be \"\n                \"None.\"\n            )\n\n        # Index of subject-predicate-object argument that is None\n        (idx,) = [i for i, v in enumerate(spo) if v is None]\n\n        triples = self.triples(subject, predicate, object)\n\n        if lang:\n            triples = (\n                t\n                for t in triples\n                if isinstance(t[idx], Literal)\n                and t[idx].lang == lang  # type: ignore\n            )\n\n        if any is None:\n            return (t[idx] for t in triples)  # type: ignore\n\n        try:\n            value = next(triples)[idx]\n        except StopIteration:\n            return default\n\n        try:\n            next(triples)\n        except StopIteration:\n            return value\n\n        if any is True:\n            return value\n        raise UniquenessError(\n            f\"More than one match: {(subject, predicate, object)}\"\n        )\n\n    def objects(self, subject=None, predicate=None):\n        \"\"\"Returns a generator of objects for given subject and predicate.\"\"\"\n        for _, _, o in self.triples(subject=subject, predicate=predicate):\n            yield o\n\n    def predicates(\n        self, subject=None, object=None  # pylint: disable=redefined-builtin\n    ):\n        \"\"\"Returns a generator of predicates for given subject and object.\"\"\"\n        for _, p, _ in self.triples(subject=subject, object=object):\n            yield p\n\n    def subjects(\n        self, predicate=None, object=None  # pylint: disable=redefined-builtin\n    ):\n        \"\"\"Returns a generator of subjects for given predicate and object.\"\"\"\n        for s, _, _ in self.triples(predicate=predicate, object=object):\n            yield s\n\n    def predicate_objects(self, subject=None):\n        \"\"\"Returns a generator of (predicate, object) tuples for given\n        subject.\"\"\"\n        for _, p, o in self.triples(subject=subject):\n            yield p, o\n\n    def subject_objects(self, predicate=None):\n        \"\"\"Returns a generator of (subject, object) tuples for given\n        predicate.\"\"\"\n        for s, _, o in self.triples(predicate=predicate):\n            yield s, o\n\n    def subject_predicates(\n        self, object=None\n    ):  # pylint: disable=redefined-builtin\n        \"\"\"Returns a generator of (subject, predicate) tuples for given\n        object.\"\"\"\n        for s, p, _ in self.triples(object=object):\n            yield s, p\n\n    def has(\n        self, subject=None, predicate=None, object=None\n    ):  # pylint: disable=redefined-builtin\n        \"\"\"Returns true if the triplestore has any triple matching\n        the give subject, predicate and/or object.\"\"\"\n        triple = self.triples(\n            subject=subject, predicate=predicate, object=object\n        )\n        try:\n            next(triple)\n        except StopIteration:\n            return False\n        return True\n\n    def set(self, triple):\n        \"\"\"Convenience method to update the value of object.\n\n        Removes any existing triples for subject and predicate before adding\n        the given `triple`.\n        \"\"\"\n        s, p, _ = triple\n        self.remove(s, p)\n        self.add(triple)\n\n    # Methods providing additional functionality\n    # ------------------------------------------\n    def expand_iri(self, iri: str, strict: bool = False) -&gt; str:\n        \"\"\"\n        Return the full IRI if `iri` is prefixed.\n        Otherwise `iri` isreturned.\n\n        Examples:\n\n        ```python\n        &gt;&gt;&gt; from tripper import Triplestore\n        &gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n\n        # Unknown prefix raises an exception\n        &gt;&gt;&gt; ts.expand_iri(\"ex:Concept\", strict=True)  # doctest: +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        tripper.errors.NamespaceError: Undefined prefix \"ex\" in IRI: ex:Concept\n\n        &gt;&gt;&gt; EX = ts.bind(\"ex\", \"http://example.com#\")\n        &gt;&gt;&gt; ts.expand_iri(\"ex:Concept\")\n        'http://example.com#Concept'\n\n        # Returns iri if it has no prefix\n        &gt;&gt;&gt; ts.expand_iri(\"http://example.com#Concept\")\n        'http://example.com#Concept'\n\n        ```\n\n        \"\"\"\n        return expand_iri(iri, self.namespaces, strict=strict)\n\n    def prefix_iri(self, iri: str, require_prefixed: bool = False):\n        # pylint: disable=line-too-long\n        \"\"\"Return prefixed IRI.\n\n        This is the reverse of expand_iri().\n\n        If `require_prefixed` is true, a NamespaceError exception is raised\n        if no prefix can be found.\n\n        Examples:\n\n        ```python\n        &gt;&gt;&gt; from tripper import Triplestore\n        &gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n        &gt;&gt;&gt; ts.prefix_iri(\"http://example.com#Concept\")\n        'http://example.com#Concept'\n\n        &gt;&gt;&gt; ts.prefix_iri(\n        ...     \"http://example.com#Concept\", require_prefixed=True\n        ... )  # doctest: +ELLIPSIS\n        Traceback (most recent call last):\n        ...\n        tripper.errors.NamespaceError: No prefix defined for IRI: http://example.com#Concept\n\n        &gt;&gt;&gt; EX = ts.bind(\"ex\", \"http://example.com#\")\n        &gt;&gt;&gt; ts.prefix_iri(\"http://example.com#Concept\")\n        'ex:Concept'\n\n        ```\n\n        \"\"\"\n        return prefix_iri(iri, self.namespaces, require_prefixed)\n\n    # Types of restrictions defined in OWL\n    _restriction_types = {\n        \"some\": (OWL.someValueFrom, None),\n        \"only\": (OWL.allValueFrom, None),\n        \"exactly\": (OWL.onClass, OWL.qualifiedCardinality),\n        \"min\": (OWL.onClass, OWL.minQualifiedCardinality),\n        \"max\": (OWL.onClass, OWL.maxQualifiedCardinality),\n        \"value\": (OWL.hasValue, None),\n    }\n\n    def add_restriction(  # pylint: disable=redefined-builtin\n        self,\n        cls: str,\n        property: str,\n        value: \"Union[str, Literal]\",\n        type: \"RestrictionType\",\n        cardinality: \"Optional[int]\" = None,\n        hashlength: int = 16,\n    ) -&gt; str:\n        \"\"\"Add a restriction to a class in the triplestore.\n\n        Arguments:\n            cls: IRI of class to which the restriction applies.\n            property: IRI of restriction property.\n            value: The IRI or literal value of the restriction target.\n            type: The type of the restriction.  Should be one of:\n                - some: existential restriction (value is a class IRI)\n                - only: universal restriction (value is a class IRI)\n                - exactly: cardinality restriction (value is a class IRI)\n                - min: minimum cardinality restriction (value is a class IRI)\n                - max: maximum cardinality restriction (value is a class IRI)\n                - value: Value restriction (value is an IRI of an individual\n                  or a literal)\n\n            cardinality: the cardinality value for cardinality restrictions.\n            hashlength: Number of bytes in the hash part of the bnode IRI.\n\n        Returns:\n            The IRI of the created blank node representing the restriction.\n        \"\"\"\n        iri = bnode_iri(\n            prefix=\"restriction\",\n            source=f\"{cls} {property} {value} {type} {cardinality}\",\n            length=hashlength,\n        )\n        triples = [\n            (cls, RDFS.subClassOf, iri),\n            (iri, RDF.type, OWL.Restriction),\n            (iri, OWL.onProperty, property),\n        ]\n        if type not in self._restriction_types:\n            raise ArgumentValueError(\n                '`type` must be one of: \"some\", \"only\", \"exactly\", \"min\", '\n                '\"max\" or \"value\"'\n            )\n        pred, card = self._restriction_types[type]\n        triples.append((iri, pred, value))\n        if card:\n            if not cardinality:\n                raise ArgumentTypeError(\n                    f\"`cardinality` must be provided for type='{type}'\"\n                )\n            triples.append(\n                (\n                    iri,\n                    card,\n                    Literal(cardinality, datatype=XSD.nonNegativeInteger),\n                ),\n            )\n        self.add_triples(triples)\n        return iri\n\n    def restrictions(  # pylint: disable=redefined-builtin\n        self,\n        cls: \"Optional[str]\" = None,\n        property: \"Optional[str]\" = None,\n        value: \"Optional[Union[str, Literal]]\" = None,\n        type: \"Optional[RestrictionType]\" = None,\n        cardinality: \"Optional[int]\" = None,\n        asdict: bool = True,\n    ) -&gt; \"Generator[dict, None, None]\":\n        # pylint: disable=too-many-boolean-expressions\n        \"\"\"Returns a generator over matching restrictions.\n\n        Arguments:\n            cls: IRI of class to which the restriction applies.\n            property: IRI of restriction property.\n            value: The IRI or literal value of the restriction target.\n            type: The type of the restriction.  Should be one of:\n                - some: existential restriction (value is a class IRI)\n                - only: universal restriction (value is a class IRI)\n                - exactly: cardinality restriction (value is a class IRI)\n                - min: minimum cardinality restriction (value is a class IRI)\n                - max: maximum cardinality restriction (value is a class IRI)\n                - value: Value restriction (value is an IRI of an individual\n                  or a literal)\n\n            cardinality: the cardinality value for cardinality restrictions.\n            asdict: Whether to returned generator is over dicts (see\n                _get_restriction_dict()). Default is to return a generator\n                over blank node IRIs.\n\n        Returns:\n            A generator over matching restrictions.  See `asdict` argument\n            for types iterated over.\n        \"\"\"\n        if type is None:\n            types = set(self._restriction_types.keys())\n        elif type not in self._restriction_types:\n            raise ArgumentValueError(\n                f\"Invalid `type='{type}'`, it must be one of: \"\n                f\"{', '.join(self._restriction_types.keys())}.\"\n            )\n        else:\n            types = {type} if isinstance(type, str) else set(type)\n\n        if isinstance(value, Literal):\n            types.intersection_update({\"value\"})\n        elif isinstance(value, str):\n            types.difference_update({\"value\"})\n\n        if cardinality:\n            types.intersection_update({\"exactly\", \"min\", \"max\"})\n        if not types:\n            raise ArgumentValueError(\n                f\"Inconsistent type='{type}', value='{value}' and \"\n                f\"cardinality='{cardinality}' arguments\"\n            )\n        pred = {self._restriction_types[t][0] for t in types}\n        card = {\n            self._restriction_types[t][1]\n            for t in types\n            if self._restriction_types[t][1]\n        }\n\n        if cardinality:\n            lcard = Literal(cardinality, datatype=XSD.nonNegativeInteger)\n\n        for iri in self.subjects(predicate=OWL.onProperty, object=property):\n            if (\n                self.has(iri, RDF.type, OWL.Restriction)\n                and (not cls or self.has(cls, RDFS.subClassOf, iri))\n                and any(self.has(iri, p, value) for p in pred)\n                and (\n                    not card\n                    or not cardinality\n                    or any(self.has(iri, c, lcard) for c in card)\n                )\n            ):\n                yield self._get_restriction_dict(iri) if asdict else iri\n\n    def _get_restriction_dict(self, iri):\n        \"\"\"Return a dict describing restriction with `iri`.\n\n        The returned dict has the following keys:\n        - iri: (str) IRI of the restriction itself (blank node).\n        - cls: (str) IRI of class to which the restriction applies.\n        - property: (str) IRI of restriction property\n        - type: (str) One of: \"some\", \"only\", \"exactly\", \"min\", \"max\", \"value\".\n        - cardinality: (int) Restriction cardinality (optional).\n        - value: (str|Literal) IRI or literal value of the restriction target.\n        \"\"\"\n        dct = dict(self.predicate_objects(iri))\n        if OWL.onClass in dct:\n            ((t, p, c),) = [\n                (t, p, c)\n                for t, (p, c) in self._restriction_types.items()\n                if c in dct\n            ]\n        else:\n            ((t, p, c),) = [\n                (t, p, c)\n                for t, (p, c) in self._restriction_types.items()\n                if p in dct\n            ]\n        return {\n            \"iri\": iri,\n            \"cls\": self.value(predicate=RDFS.subClassOf, object=iri),\n            \"property\": dct[OWL.onProperty],\n            \"type\": t,\n            \"cardinality\": int(dct[c]) if c else None,\n            \"value\": dct[p],\n        }\n\n    def available(self, timeout: float = 5, interval: float = 1) -&gt; bool:\n        \"\"\"Checks if the backend is available.\n\n        This is done by sending a request to the URL specified\n        in the `check_url` attribute and checking for the response.\n\n        Arguments:\n            timeout: Total time in seconds to wait for a response.\n        interval: Internal time interval in seconds between checking if the\n            service has responded.\n\n        Returns:\n            Returns true if the service responds with code 200,\n            otherwise false is returned.\n\n        \"\"\"\n        warnings.warn(\n            \"Method `Triplestore.available()` is deprecated. \"\n            \"Use `Triplestore.is_available()` instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if self.check_url is None:\n            raise ValueError(\n                \"`check_url` must be assigned before calling available()\"\n            )\n        return check_service_availability(\n            self.check_url, timeout=timeout, interval=interval\n        )\n\n    def map(\n        self,\n        source: str,\n        target: str,\n        cost: \"Optional[Union[float, Callable]]\" = None,\n        target_cost: bool = True,\n    ):\n        \"\"\"Add 'mapsTo' relation to the triplestore.\n\n        Arguments:\n            source: Source IRI.\n            target: IRI of target ontological concept.\n            cost: User-defined cost of following this mapping relation\n                represented as a float.  It may be given either as a\n                float or as a callable taking three arguments\n\n                    cost(triplestore, input_iris, output_iri)\n\n                and returning the cost as a float.\n            target_cost: Whether the cost is assigned to mapping steps\n                that have `target` as output.\n        \"\"\"\n        return self.add_mapsTo(\n            target=target,\n            source=source,\n            cost=cost,\n            target_cost=target_cost,\n        )\n\n    def add_mapsTo(\n        self,\n        target: str,\n        source: str,\n        property_name: \"Optional[str]\" = None,\n        cost: \"Optional[Union[float, Callable]]\" = None,\n        target_cost: bool = True,\n    ):\n        \"\"\"Add 'mapsTo' relation to triplestore.\n\n        Arguments:\n            target: IRI of target ontological concept.\n            source: Source IRI (or entity object).\n            property_name: Name of property if `source` is an entity or\n                an entity IRI.\n            cost: User-defined cost of following this mapping relation\n                represented as a float.  It may be given either as a\n                float or as a callable taking three arguments\n\n                    cost(triplestore, input_iris, output_iri)\n\n                and returning the cost as a float.\n            target_cost: Whether the cost is assigned to mapping steps\n                that have `target` as output.\n\n        Note:\n            This is equivalent to the `map()` method, but reverts the\n            two first arguments and adds the `property_name` argument.\n        \"\"\"\n        self.bind(\"map\", MAP)\n\n        if not property_name and not isinstance(source, str):\n            raise TripperError(\n                \"`property_name` is required when `target` is not a string.\"\n            )\n\n        target = self.expand_iri(target)\n        source = self.expand_iri(infer_iri(source))\n        if property_name:\n            self.add((f\"{source}#{property_name}\", MAP.mapsTo, target))\n        else:\n            self.add((source, MAP.mapsTo, target))\n        if cost is not None:\n            dest = target if target_cost else source\n            self._add_cost(cost, dest)\n\n    def _get_function(self, func_iri):\n        \"\"\"Returns Python function object corresponding to `func_iri`.\n\n        Raises CannotGetFunctionError on failure.\n\n        If the function is cached in the the `function_repo` attribute,\n        it is returned directly.\n\n        Otherwise an attempt is made to import the module implementing the\n        function.  If that fails, the corresponding PyPI package is first\n        installed before importing the module again.\n\n        Finally the function is cached and returned.\n\n        Note: Don't use call this method directly.  Use instead the\n        `eval_function()` method, which will at some point will provide\n        sandboxing for security.\n        \"\"\"\n        if func_iri in self.function_repo and self.function_repo[func_iri]:\n            return self.function_repo[func_iri]\n\n        func_name = self.value(func_iri, OTEIO.hasPythonFunctionName)\n        module_name = self.value(func_iri, OTEIO.hasPythonModuleName)\n        package_name = self.value(func_iri, OTEIO.hasPythonPackageName)\n\n        if not func_name or not module_name:\n            raise CannotGetFunctionError(\n                f\"no documentation of how to access function: {func_iri}\"\n            )\n\n        # Import module implementing the function\n        try:\n            module = importlib.import_module(module_name, package_name)\n        except ModuleNotFoundError:\n            # If we cannot find the module, try to install the pypi\n            # package and try to import the module again\n            pypi_package = self.value(func_iri, OTEIO.hasPypiPackageName)\n            if not pypi_package:\n                raise CannotGetFunctionError(  # pylint: disable=raise-missing-from\n                    f\"PyPI package not documented for function: {func_iri}\"\n                )\n\n            try:\n                subprocess.run(  # nosec\n                    args=[\n                        sys.executable,\n                        \"-m\",\n                        \"pip\",\n                        \"install\",\n                        pypi_package,\n                    ],\n                    check=True,\n                )\n            except subprocess.CalledProcessError as exc:\n                raise CannotGetFunctionError(\n                    f\"failed installing PyPI package '{pypi_package}'\"\n                ) from exc\n\n            try:\n                module = importlib.import_module(module_name, package_name)\n            except ModuleNotFoundError as exc:\n                raise CannotGetFunctionError(\n                    f\"failed importing module '{module_name}'\"\n                    + f\" from '{package_name}'\"\n                    if package_name\n                    else \"\"\n                ) from exc\n\n        func = getattr(module, str(func_name))\n        self.function_repo[func_iri] = func\n\n        return func\n\n    def eval_function(self, func_iri, args=(), kwargs=None) -&gt; \"Any\":\n        \"\"\"Evaluate mapping function and return the result.\n\n        Arguments:\n            func_iri: IRI of the function to be evaluated.\n            args: Sequence of positional arguments passed to the function.\n            kwargs: Mapping of keyword arguments passed to the function.\n\n        Returns:\n            The return value of the function.\n\n        Note:\n            The current implementation does not protect against side\n            effect or malicious code.  Be warned!\n            This may be improved in the future.\n        \"\"\"\n        func = self._get_function(func_iri)\n        if not kwargs:\n            kwargs = {}\n\n        # FIXME: Add sandboxing\n        result = func(*args, **kwargs)\n\n        return result\n\n    def add_function(\n        self,\n        func: \"Union[Callable, str]\",\n        expects: \"Union[str, Sequence, Mapping]\" = (),\n        returns: \"Union[str, Sequence]\" = (),\n        base_iri: \"Optional[str]\" = None,\n        standard: str = \"emmo\",\n        cost: \"Optional[Union[float, Callable]]\" = None,\n        func_name: \"Optional[str]\" = None,\n        module_name: \"Optional[str]\" = None,\n        package_name: \"Optional[str]\" = None,\n        pypi_package_name: \"Optional[str]\" = None,\n    ):\n        # pylint: disable=too-many-branches,too-many-arguments\n        \"\"\"Inspect function and add triples describing it to the triplestore.\n\n        Arguments:\n            func: Function to describe.  Should either be a callable or a\n                string with a unique function IRI.\n            expects: Sequence of IRIs to ontological concepts corresponding\n                to positional arguments of `func`.  May also be given as a\n                dict mapping argument names to corresponding ontological IRIs.\n            returns: IRI of return value.  May also be given as a sequence\n                of IRIs, if multiple values are returned.\n            base_iri: Base of the IRI representing the function in the\n                knowledge base.  Defaults to the base IRI of the triplestore.\n            standard: Name of ontology to use when describing the function.\n                Valid values are:\n                - \"emmo\": Elementary Multiperspective Material Ontology (EMMO)\n                - \"fno\": Function Ontology (FnO)\n            cost: User-defined cost of following this mapping relation\n                represented as a float.  It may be given either as a\n                float or as a callable taking three arguments\n\n                    cost(triplestore, input_iris, output_iri)\n\n                and returning the cost as a float.\n            func_name: Function name.  Needed if `func` is given as an IRI.\n            module_name: Fully qualified name of Python module implementing\n                this function.  Default is to infer from `func`.\n                implementing the function.\n            package_name: Name of Python package implementing this function.\n                Default is inferred from either the module or first part of\n                `module_name`.\n            pypi_package_name: Name and version of PyPI package implementing\n                this mapping function (specified as in requirements.txt).\n                Defaults to `package_name`.\n\n        Returns:\n            func_iri: IRI of the added function.\n        \"\"\"\n        if isinstance(expects, str):\n            expects = [expects]\n        if isinstance(returns, str):\n            returns = [returns]\n\n        method = getattr(self, f\"_add_function_{standard}\")\n        func_iri = method(func, expects, returns, base_iri)\n        self.function_repo[func_iri] = func if callable(func) else None\n        if cost is not None:\n            self._add_cost(cost, func_iri)\n\n        # Add standard-independent documentation of how to access the\n        # mapping function\n        self._add_function_doc(\n            func=func if callable(func) else None,\n            func_iri=func_iri,\n            func_name=func_name,\n            module_name=module_name,\n            package_name=package_name,\n            pypi_package_name=pypi_package_name,\n        )\n\n        return func_iri\n\n    def _add_function_doc(\n        self,\n        func_iri: \"str\",\n        func: \"Optional[Callable]\" = None,\n        func_name: \"Optional[str]\" = None,\n        module_name: \"Optional[str]\" = None,\n        package_name: \"Optional[str]\" = None,\n        pypi_package_name: \"Optional[str]\" = None,\n    ):\n        \"\"\"Add standard-independent documentation of how to access the\n        function.\n\n        Arguments:\n            func_iri: IRI of individual in the triplestore that stands for\n                the function.\n            func: Optional reference to the function itself.\n            func_name: Function name.  Needed if `func` is given as an IRI.\n            module_name: Fully qualified name of Python module implementing\n                this function.  Default is to infer from `func`.\n                implementing the function.\n            package_name: Name of Python package implementing this function.\n                Default is inferred from either the module or first part of\n                `module_name`.\n            pypi_package_name: Name and version of PyPI package implementing\n                this mapping function (specified as in requirements.txt).\n                Defaults to `package_name`.\n        \"\"\"\n        if callable(func):\n            func_name = func.__name__\n            module = inspect.getmodule(func)\n            if not module:\n                raise TypeError(\n                    f\"inspect is not able to infer module from function \"\n                    f\"'{func.__name__}'\"\n                )\n            if not module_name:\n                module_name = module.__name__\n            if not package_name:\n                package_name = module.__package__  # type: ignore\n            if not pypi_package_name:\n                pypi_package_name = package_name\n\n        if func_name and module_name:\n            self.bind(\"oteio\", OTEIO)\n            self.add(\n                (\n                    func_iri,\n                    OTEIO.hasPythonFunctionName,\n                    Literal(func_name, datatype=XSD.string),\n                )\n            )\n            self.add(\n                (\n                    func_iri,\n                    OTEIO.hasPythonModuleName,\n                    Literal(module_name, datatype=XSD.string),\n                )\n            )\n            if package_name:\n                self.add(\n                    (\n                        func_iri,\n                        OTEIO.hasPythonPackageName,\n                        Literal(package_name, datatype=XSD.string),\n                    )\n                )\n            if pypi_package_name:\n                self.add(\n                    (\n                        func_iri,\n                        OTEIO.hasPypiPackageName,\n                        Literal(pypi_package_name, datatype=XSD.string),\n                    )\n                )\n        else:\n            warnings.warn(\n                f\"Function and module name for function '{func_name}' \"\n                \"is not provided and cannot be inferred.  How to access \"\n                \"the function will not be documented.\",\n                stacklevel=3,\n            )\n\n    def _add_cost(\n        self,\n        cost: \"Union[float, Callable]\",\n        dest_iri,\n        base_iri=None,\n        pypi_package_name=None,\n    ):\n        \"\"\"Help function that adds `cost` to destination IRI `dest_iri`.\n\n        Arguments:\n            cost: User-defined cost of following this mapping relation\n                represented as a float.  It may be given either as a\n                float or as a callable taking three arguments\n\n                    cost(triplestore, input_iris, output_iri)\n\n                and returning the cost as a float.\n            dest_iri: destination iri that the cost should be associated with.\n            base_iri: Base of the IRI representing the function in the\n                knowledge base.  Defaults to the base IRI of the triplestore.\n            pypi_package_name: Name and version of PyPI package implementing\n                this cost function (specified as in requirements.txt).\n        \"\"\"\n        if base_iri is None:\n            base_iri = self.base_iri if self.base_iri else \":\"\n\n        if self.has(dest_iri, DM.hasCost):\n            warnings.warn(f\"A cost is already assigned to IRI: {dest_iri}\")\n        elif callable(cost):\n            cost_iri = f\"{base_iri}cost_function{function_id(cost)}\"\n            self.add(\n                (dest_iri, DM.hasCost, Literal(cost_iri, datatype=XSD.anyURI))\n            )\n            self.function_repo[cost_iri] = cost\n            self._add_function_doc(\n                func=cost,\n                func_iri=cost_iri,\n                pypi_package_name=pypi_package_name,\n            )\n        else:\n            self.add((dest_iri, DM.hasCost, Literal(cost)))\n\n    def _get_cost(self, dest_iri, input_iris=(), output_iri=None):\n        \"\"\"Return evaluated cost for given destination iri.\"\"\"\n        v = self.value(dest_iri, DM.hasCost)\n\n        if v.datatype and v.datatype != XSD.anyURI:\n            return v.value\n        cost = self._get_function(v.value)\n        return cost(self, input_iris, output_iri)\n\n    def _add_function_fno(self, func, expects, returns, base_iri):\n        \"\"\"Implementing add_function() for FnO.\"\"\"\n        # pylint: disable=too-many-locals,too-many-statements\n        self.bind(\"fno\", FNO)\n        self.bind(\"dcterms\", DCTERMS)\n        self.bind(\"map\", MAP)\n\n        if base_iri is None:\n            base_iri = self.base_iri if self.base_iri else \":\"\n\n        if callable(func):\n            fid = function_id(func)  # Function id\n            func_iri = f\"{base_iri}{func.__name__}_{fid}\"\n            name = func.__name__\n            doc_string = inspect.getdoc(func)\n            parlist = f\"_:{func.__name__}{fid}_parlist\"\n            outlist = f\"_:{func.__name__}{fid}_outlist\"\n            if isinstance(expects, Sequence):\n                pars = list(zip(expects, inspect.signature(func).parameters))\n            else:\n                pars = [\n                    (expects[par], par)\n                    for par in inspect.signature(func).parameters\n                ]\n        elif isinstance(func, str):\n            func_iri = func\n            name = split_iri(func)[1]\n            doc_string = \"\"\n            parlist = f\"_:{func_iri}_parlist\"\n            outlist = f\"_:{func_iri}_outlist\"\n            pariris = (\n                expects if isinstance(expects, Sequence) else expects.values()\n            )\n            parnames = [split_iri(pariri)[1] for pariri in pariris]\n            pars = list(zip(pariris, parnames))\n        else:\n            raise TypeError(\"`func` should be either a callable or an IRI\")\n\n        self.add((func_iri, RDF.type, FNO.Function))\n        self.add((func_iri, RDFS.label, en(name)))\n        self.add((func_iri, FNO.expects, parlist))\n        self.add((func_iri, FNO.returns, outlist))\n        if doc_string:\n            self.add((func_iri, DCTERMS.description, en(doc_string)))\n\n        lst = parlist\n        for i, (iri, parname) in enumerate(pars):\n            lst_next = f\"{parlist}{i+2}\" if i &lt; len(pars) - 1 else RDF.nil\n            par = f\"{func_iri}_parameter{i+1}_{parname}\"\n            self.add((par, RDF.type, FNO.Parameter))\n            self.add((par, RDFS.label, en(parname)))\n            self.add((par, MAP.mapsTo, iri))\n            self.add((lst, RDF.first, par))\n            self.add((lst, RDF.rest, lst_next))\n            lst = lst_next\n\n        lst = outlist\n        for i, iri in enumerate(returns):\n            lst_next = f\"{outlist}{i+2}\" if i &lt; len(returns) - 1 else RDF.nil\n            val = f\"{func_iri}_output{i+1}\"\n            self.add((val, RDF.type, FNO.Output))\n            self.add((val, MAP.mapsTo, iri))\n            self.add((lst, RDF.first, val))\n            self.add((lst, RDF.rest, lst_next))\n            lst = lst_next\n\n        return func_iri\n\n    def _add_function_emmo(self, func, expects, returns, base_iri):\n        \"\"\"Implementing add_function() method for the \"emmo\" standard.\"\"\"\n        # pylint: disable=too-many-locals\n        self.bind(\"emmo\", EMMO)\n        self.bind(\"dcterms\", DCTERMS)\n        self.bind(\"map\", MAP)\n\n        # Hardcode EMMO IRIs to avoid slow lookup\n        Task = EMMO.EMMO_4299e344_a321_4ef2_a744_bacfcce80afc\n        Datum = EMMO.EMMO_50d6236a_7667_4883_8ae1_9bb5d190423a\n        hasInput = EMMO.EMMO_36e69413_8c59_4799_946c_10b05d266e22\n        hasOutput = EMMO.EMMO_c4bace1d_4db0_4cd3_87e9_18122bae2840\n        # Software = EMMO.EMMO_8681074a_e225_4e38_b586_e85b0f43ce38\n        # hasSoftware = EMMO.Software  # TODO: fix when EMMO has hasSoftware\n\n        if base_iri is None:\n            base_iri = self.base_iri if self.base_iri else \":\"\n\n        if callable(func):\n            fid = function_id(func)  # Function id\n            func_iri = f\"{base_iri}{func.__name__}_{fid}\"\n            name = func.__name__\n            doc_string = inspect.getdoc(func)\n            if isinstance(expects, Sequence):\n                pars = list(zip(inspect.signature(func).parameters, expects))\n            else:\n                pars = expects.items()\n        elif isinstance(func, str):\n            func_iri = func\n            name = split_iri(func)[1]\n            doc_string = \"\"\n            pariris = (\n                expects if isinstance(expects, Sequence) else expects.values()\n            )\n            parnames = [split_iri(pariri)[1] for pariri in pariris]\n            pars = list(zip(parnames, pariris))\n        else:\n            raise TypeError(\"`func` should be either a callable or an IRI\")\n\n        self.add((func_iri, RDF.type, Task))\n        self.add((func_iri, RDFS.label, en(name)))\n        for parname, iri in pars:\n            self.add((iri, RDF.type, Datum))\n            self.add((iri, RDFS.label, en(parname)))\n            self.add((func_iri, hasInput, iri))\n        for iri in returns:\n            self.add((iri, RDF.type, Datum))\n            self.add((func_iri, hasOutput, iri))\n        if doc_string:\n            self.add((func_iri, DCTERMS.description, en(doc_string)))\n\n        return func_iri\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.prefer_sparql","title":"<code>prefer_sparql</code>  <code>property</code> <code>readonly</code>","text":"<p>Whether the backend prefer SPARQL over the triples() interface. Is None if not specified by the backend.</p> <p>Even though Tripper requires that the Triplestore.triples() is implemented, Triplestore.query() must be used for some backends in specific cases (like fuseki when working on RDF lists) because of how blank nodes are treated. </p> <p>The purpose of this property is to let tripper automatically select the most appropriate interface depending on the current backend settings.</p>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.__init__","title":"<code>__init__(self, backend, base_iri=None, database=None, package=None, check_url=None, **kwargs)</code>  <code>special</code>","text":"<p>Initialise triplestore using the backend with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Name of the backend module.</p> <p>For built-in backends or backends provided via a backend package (using entrypoints), this should just be the name of the backend with no dots (ex: \"rdflib\").</p> <p>For a custom backend, you can provide the full module name, including the dots (ex:\"mypackage.mybackend\").  If <code>package</code> is given, <code>backend</code> is interpreted relative to <code>package</code> (ex: ..mybackend).</p> <p>For a list over available backends, see https://github.com/EMMC-ASBL/tripper#available-backends</p> required <code>base_iri</code> <code>'Optional[str]'</code> <p>Base IRI used by the add_function() method when adding new triples. May also be used by the backend.</p> <code>None</code> <code>database</code> <code>'Optional[str]'</code> <p>Name of database to connect to (for backends that supports it).</p> <code>None</code> <code>package</code> <code>'Optional[str]'</code> <p>Required when <code>backend</code> is a relative module.  In that case, it is relative to <code>package</code>.</p> <code>None</code> <code>check_url</code> <code>'Optional[str]'</code> <p>Deprecated. A URL for checking whether the backend is available. Use the optional keyword argument <code>check_iri</code> instead. Defaults to <code>base_iri</code>.</p> <code>None</code> <code>kwargs</code> <p>Keyword arguments passed to the backend's init() method.</p> <code>{}</code> <p>Attributes:</p> Name Type Description <code>backend_name</code> <p>Name of backend.</p> <code>base_iri</code> <p>Assigned to the <code>base_iri</code> argument.</p> <code>closed</code> <p>Whether the triplestore is closed.</p> <code>kwargs</code> <p>Dict with additional keyword arguments.</p> <code>namespaces</code> <p>Dict mapping namespace prefixes to IRIs.</p> <code>package</code> <p>Name of Python package if the backend is implemented as a relative module. Assigned to the <code>package</code> argument.</p> <p>Notes</p> <p>If the backend establishes a connection that should be closed it is useful to instantiate the Triplestore as a context manager:</p> <pre><code>&gt;&gt;&gt; import tripper\n&gt;&gt;&gt; with tripper.Triplestore(\"rdflib\") as ts:\n...     print(ts.backend_name)\nrdflib\n</code></pre> <p>This ensures that the connection is automatically closed when the context manager exits.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def __init__(\n    self,\n    backend: str,\n    base_iri: \"Optional[str]\" = None,\n    database: \"Optional[str]\" = None,\n    package: \"Optional[str]\" = None,\n    check_url: \"Optional[str]\" = None,  # Deprecated\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialise triplestore using the backend with the given name.\n\n    Arguments:\n        backend: Name of the backend module.\n\n            For built-in backends or backends provided via a\n            backend package (using entrypoints), this should just\n            be the name of the backend with no dots (ex: \"rdflib\").\n\n            For a custom backend, you can provide the full module name,\n            including the dots (ex:\"mypackage.mybackend\").  If `package`\n            is given, `backend` is interpreted relative to `package`\n            (ex: ..mybackend).\n\n            For a list over available backends, see\n            https://github.com/EMMC-ASBL/tripper#available-backends\n\n        base_iri: Base IRI used by the add_function() method when adding\n            new triples. May also be used by the backend.\n        database: Name of database to connect to (for backends that\n            supports it).\n        package: Required when `backend` is a relative module.  In that\n            case, it is relative to `package`.\n        check_url: Deprecated. A URL for checking whether the backend\n            is available. Use the optional keyword argument `check_iri`\n            instead. Defaults to `base_iri`.\n        kwargs: Keyword arguments passed to the backend's __init__()\n            method.\n\n    Attributes:\n        backend_name: Name of backend.\n        base_iri: Assigned to the `base_iri` argument.\n        closed: Whether the triplestore is closed.\n        kwargs: Dict with additional keyword arguments.\n        namespaces: Dict mapping namespace prefixes to IRIs.\n        package: Name of Python package if the backend is implemented as\n            a relative module. Assigned to the `package` argument.\n\n    Notes:\n        If the backend establishes a connection that should be closed\n        it is useful to instantiate the Triplestore as a context manager:\n\n            &gt;&gt;&gt; import tripper\n            &gt;&gt;&gt; with tripper.Triplestore(\"rdflib\") as ts:\n            ...     print(ts.backend_name)\n            rdflib\n\n        This ensures that the connection is automatically closed when the\n        context manager exits.\n    \"\"\"\n    if check_url:\n        warnings.warn(\n            \"Argument `check_url` is deprecated. It defaults to \"\n            \"`base_iri` and is rarely needed.  If needed, `check_iri` \"\n            \"instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n\n    backend_name = backend.rsplit(\".\", 1)[-1]\n    module = self._load_backend(backend, package)\n    cls = getattr(module, f\"{backend_name.title()}Strategy\")\n    self.base_iri = base_iri\n    self.namespaces: \"Dict[str, Namespace]\" = {}\n    self.closed = False\n    self.backend_name = backend_name\n    self.database = database\n    self.package = package\n    self.check_url = check_url if check_url else base_iri  # Deprecated\n    self.kwargs = kwargs.copy()\n    self.backend = cls(base_iri=base_iri, database=database, **kwargs)\n\n    # Cache functions in the triplestore for fast access\n    self.function_repo: \"Dict[str, Union[float, Callable, None]]\" = {}\n\n    for prefix, namespace in self.default_namespaces.items():\n        self.bind(prefix, namespace)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.add","title":"<code>add(self, triple)</code>","text":"<p>Add <code>triple</code> to triplestore.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def add(self, triple: \"Triple\"):\n    \"\"\"Add `triple` to triplestore.\"\"\"\n    self.add_triples([triple])\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.add_function","title":"<code>add_function(self, func, expects=(), returns=(), base_iri=None, standard='emmo', cost=None, func_name=None, module_name=None, package_name=None, pypi_package_name=None)</code>","text":"<p>Inspect function and add triples describing it to the triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>'Union[Callable, str]'</code> <p>Function to describe.  Should either be a callable or a string with a unique function IRI.</p> required <code>expects</code> <code>'Union[str, Sequence, Mapping]'</code> <p>Sequence of IRIs to ontological concepts corresponding to positional arguments of <code>func</code>.  May also be given as a dict mapping argument names to corresponding ontological IRIs.</p> <code>()</code> <code>returns</code> <code>'Union[str, Sequence]'</code> <p>IRI of return value.  May also be given as a sequence of IRIs, if multiple values are returned.</p> <code>()</code> <code>base_iri</code> <code>'Optional[str]'</code> <p>Base of the IRI representing the function in the knowledge base.  Defaults to the base IRI of the triplestore.</p> <code>None</code> <code>standard</code> <code>str</code> <p>Name of ontology to use when describing the function. Valid values are: - \"emmo\": Elementary Multiperspective Material Ontology (EMMO) - \"fno\": Function Ontology (FnO)</p> <code>'emmo'</code> <code>cost</code> <code>'Optional[Union[float, Callable]]'</code> <p>User-defined cost of following this mapping relation represented as a float.  It may be given either as a float or as a callable taking three arguments</p> <pre><code>cost(triplestore, input_iris, output_iri)\n</code></pre> <p>and returning the cost as a float.</p> <code>None</code> <code>func_name</code> <code>'Optional[str]'</code> <p>Function name.  Needed if <code>func</code> is given as an IRI.</p> <code>None</code> <code>module_name</code> <code>'Optional[str]'</code> <p>Fully qualified name of Python module implementing this function.  Default is to infer from <code>func</code>. implementing the function.</p> <code>None</code> <code>package_name</code> <code>'Optional[str]'</code> <p>Name of Python package implementing this function. Default is inferred from either the module or first part of <code>module_name</code>.</p> <code>None</code> <code>pypi_package_name</code> <code>'Optional[str]'</code> <p>Name and version of PyPI package implementing this mapping function (specified as in requirements.txt). Defaults to <code>package_name</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>func_iri</code> <p>IRI of the added function.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def add_function(\n    self,\n    func: \"Union[Callable, str]\",\n    expects: \"Union[str, Sequence, Mapping]\" = (),\n    returns: \"Union[str, Sequence]\" = (),\n    base_iri: \"Optional[str]\" = None,\n    standard: str = \"emmo\",\n    cost: \"Optional[Union[float, Callable]]\" = None,\n    func_name: \"Optional[str]\" = None,\n    module_name: \"Optional[str]\" = None,\n    package_name: \"Optional[str]\" = None,\n    pypi_package_name: \"Optional[str]\" = None,\n):\n    # pylint: disable=too-many-branches,too-many-arguments\n    \"\"\"Inspect function and add triples describing it to the triplestore.\n\n    Arguments:\n        func: Function to describe.  Should either be a callable or a\n            string with a unique function IRI.\n        expects: Sequence of IRIs to ontological concepts corresponding\n            to positional arguments of `func`.  May also be given as a\n            dict mapping argument names to corresponding ontological IRIs.\n        returns: IRI of return value.  May also be given as a sequence\n            of IRIs, if multiple values are returned.\n        base_iri: Base of the IRI representing the function in the\n            knowledge base.  Defaults to the base IRI of the triplestore.\n        standard: Name of ontology to use when describing the function.\n            Valid values are:\n            - \"emmo\": Elementary Multiperspective Material Ontology (EMMO)\n            - \"fno\": Function Ontology (FnO)\n        cost: User-defined cost of following this mapping relation\n            represented as a float.  It may be given either as a\n            float or as a callable taking three arguments\n\n                cost(triplestore, input_iris, output_iri)\n\n            and returning the cost as a float.\n        func_name: Function name.  Needed if `func` is given as an IRI.\n        module_name: Fully qualified name of Python module implementing\n            this function.  Default is to infer from `func`.\n            implementing the function.\n        package_name: Name of Python package implementing this function.\n            Default is inferred from either the module or first part of\n            `module_name`.\n        pypi_package_name: Name and version of PyPI package implementing\n            this mapping function (specified as in requirements.txt).\n            Defaults to `package_name`.\n\n    Returns:\n        func_iri: IRI of the added function.\n    \"\"\"\n    if isinstance(expects, str):\n        expects = [expects]\n    if isinstance(returns, str):\n        returns = [returns]\n\n    method = getattr(self, f\"_add_function_{standard}\")\n    func_iri = method(func, expects, returns, base_iri)\n    self.function_repo[func_iri] = func if callable(func) else None\n    if cost is not None:\n        self._add_cost(cost, func_iri)\n\n    # Add standard-independent documentation of how to access the\n    # mapping function\n    self._add_function_doc(\n        func=func if callable(func) else None,\n        func_iri=func_iri,\n        func_name=func_name,\n        module_name=module_name,\n        package_name=package_name,\n        pypi_package_name=pypi_package_name,\n    )\n\n    return func_iri\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.add_mapsTo","title":"<code>add_mapsTo(self, target, source, property_name=None, cost=None, target_cost=True)</code>","text":"<p>Add 'mapsTo' relation to triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>IRI of target ontological concept.</p> required <code>source</code> <code>str</code> <p>Source IRI (or entity object).</p> required <code>property_name</code> <code>'Optional[str]'</code> <p>Name of property if <code>source</code> is an entity or an entity IRI.</p> <code>None</code> <code>cost</code> <code>'Optional[Union[float, Callable]]'</code> <p>User-defined cost of following this mapping relation represented as a float.  It may be given either as a float or as a callable taking three arguments</p> <pre><code>cost(triplestore, input_iris, output_iri)\n</code></pre> <p>and returning the cost as a float.</p> <code>None</code> <code>target_cost</code> <code>bool</code> <p>Whether the cost is assigned to mapping steps that have <code>target</code> as output.</p> <code>True</code> <p>Note</p> <p>This is equivalent to the <code>map()</code> method, but reverts the two first arguments and adds the <code>property_name</code> argument.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def add_mapsTo(\n    self,\n    target: str,\n    source: str,\n    property_name: \"Optional[str]\" = None,\n    cost: \"Optional[Union[float, Callable]]\" = None,\n    target_cost: bool = True,\n):\n    \"\"\"Add 'mapsTo' relation to triplestore.\n\n    Arguments:\n        target: IRI of target ontological concept.\n        source: Source IRI (or entity object).\n        property_name: Name of property if `source` is an entity or\n            an entity IRI.\n        cost: User-defined cost of following this mapping relation\n            represented as a float.  It may be given either as a\n            float or as a callable taking three arguments\n\n                cost(triplestore, input_iris, output_iri)\n\n            and returning the cost as a float.\n        target_cost: Whether the cost is assigned to mapping steps\n            that have `target` as output.\n\n    Note:\n        This is equivalent to the `map()` method, but reverts the\n        two first arguments and adds the `property_name` argument.\n    \"\"\"\n    self.bind(\"map\", MAP)\n\n    if not property_name and not isinstance(source, str):\n        raise TripperError(\n            \"`property_name` is required when `target` is not a string.\"\n        )\n\n    target = self.expand_iri(target)\n    source = self.expand_iri(infer_iri(source))\n    if property_name:\n        self.add((f\"{source}#{property_name}\", MAP.mapsTo, target))\n    else:\n        self.add((source, MAP.mapsTo, target))\n    if cost is not None:\n        dest = target if target_cost else source\n        self._add_cost(cost, dest)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.add_restriction","title":"<code>add_restriction(self, cls, property, value, type, cardinality=None, hashlength=16)</code>","text":"<p>Add a restriction to a class in the triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>str</code> <p>IRI of class to which the restriction applies.</p> required <code>property</code> <code>str</code> <p>IRI of restriction property.</p> required <code>value</code> <code>'Union[str, Literal]'</code> <p>The IRI or literal value of the restriction target.</p> required <code>type</code> <code>'RestrictionType'</code> <p>The type of the restriction.  Should be one of: - some: existential restriction (value is a class IRI) - only: universal restriction (value is a class IRI) - exactly: cardinality restriction (value is a class IRI) - min: minimum cardinality restriction (value is a class IRI) - max: maximum cardinality restriction (value is a class IRI) - value: Value restriction (value is an IRI of an individual   or a literal)</p> required <code>cardinality</code> <code>'Optional[int]'</code> <p>the cardinality value for cardinality restrictions.</p> <code>None</code> <code>hashlength</code> <code>int</code> <p>Number of bytes in the hash part of the bnode IRI.</p> <code>16</code> <p>Returns:</p> Type Description <code>str</code> <p>The IRI of the created blank node representing the restriction.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def add_restriction(  # pylint: disable=redefined-builtin\n    self,\n    cls: str,\n    property: str,\n    value: \"Union[str, Literal]\",\n    type: \"RestrictionType\",\n    cardinality: \"Optional[int]\" = None,\n    hashlength: int = 16,\n) -&gt; str:\n    \"\"\"Add a restriction to a class in the triplestore.\n\n    Arguments:\n        cls: IRI of class to which the restriction applies.\n        property: IRI of restriction property.\n        value: The IRI or literal value of the restriction target.\n        type: The type of the restriction.  Should be one of:\n            - some: existential restriction (value is a class IRI)\n            - only: universal restriction (value is a class IRI)\n            - exactly: cardinality restriction (value is a class IRI)\n            - min: minimum cardinality restriction (value is a class IRI)\n            - max: maximum cardinality restriction (value is a class IRI)\n            - value: Value restriction (value is an IRI of an individual\n              or a literal)\n\n        cardinality: the cardinality value for cardinality restrictions.\n        hashlength: Number of bytes in the hash part of the bnode IRI.\n\n    Returns:\n        The IRI of the created blank node representing the restriction.\n    \"\"\"\n    iri = bnode_iri(\n        prefix=\"restriction\",\n        source=f\"{cls} {property} {value} {type} {cardinality}\",\n        length=hashlength,\n    )\n    triples = [\n        (cls, RDFS.subClassOf, iri),\n        (iri, RDF.type, OWL.Restriction),\n        (iri, OWL.onProperty, property),\n    ]\n    if type not in self._restriction_types:\n        raise ArgumentValueError(\n            '`type` must be one of: \"some\", \"only\", \"exactly\", \"min\", '\n            '\"max\" or \"value\"'\n        )\n    pred, card = self._restriction_types[type]\n    triples.append((iri, pred, value))\n    if card:\n        if not cardinality:\n            raise ArgumentTypeError(\n                f\"`cardinality` must be provided for type='{type}'\"\n            )\n        triples.append(\n            (\n                iri,\n                card,\n                Literal(cardinality, datatype=XSD.nonNegativeInteger),\n            ),\n        )\n    self.add_triples(triples)\n    return iri\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.add_triples","title":"<code>add_triples(self, triples)</code>","text":"<p>Add a sequence of triples.</p> <p>Parameters:</p> Name Type Description Default <code>triples</code> <code>'Iterable[Triple]'</code> <p>A sequence of <code>(s, p, o)</code> tuples to add to the triplestore.</p> required Source code in <code>tripper/triplestore.py</code> <pre><code>def add_triples(self, triples: \"Iterable[Triple]\") -&gt; None:\n    \"\"\"Add a sequence of triples.\n\n    Arguments:\n        triples: A sequence of `(s, p, o)` tuples to add to the\n            triplestore.\n    \"\"\"\n    self.backend.add_triples(triples)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.available","title":"<code>available(self, timeout=5, interval=1)</code>","text":"<p>Checks if the backend is available.</p> <p>This is done by sending a request to the URL specified in the <code>check_url</code> attribute and checking for the response.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Total time in seconds to wait for a response.</p> <code>5</code> <p>Internal time interval in seconds between checking if the</p> <p>service has responded.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Returns true if the service responds with code 200, otherwise false is returned.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def available(self, timeout: float = 5, interval: float = 1) -&gt; bool:\n    \"\"\"Checks if the backend is available.\n\n    This is done by sending a request to the URL specified\n    in the `check_url` attribute and checking for the response.\n\n    Arguments:\n        timeout: Total time in seconds to wait for a response.\n    interval: Internal time interval in seconds between checking if the\n        service has responded.\n\n    Returns:\n        Returns true if the service responds with code 200,\n        otherwise false is returned.\n\n    \"\"\"\n    warnings.warn(\n        \"Method `Triplestore.available()` is deprecated. \"\n        \"Use `Triplestore.is_available()` instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if self.check_url is None:\n        raise ValueError(\n            \"`check_url` must be assigned before calling available()\"\n        )\n    return check_service_availability(\n        self.check_url, timeout=timeout, interval=interval\n    )\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.bind","title":"<code>bind(self, prefix, namespace='', **kwargs)</code>","text":"<p>Bind prefix to namespace and return the new Namespace object.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>Prefix to bind the the namespace.</p> required <code>namespace</code> <code>'Union[str, Namespace, Triplestore, None]'</code> <p>Namespace to bind to.  The default is to bind to the <code>base_iri</code> of the current triplestore. If <code>namespace</code> is None, the corresponding prefix is removed.</p> <code>''</code> <code>kwargs</code> <p>Keyword arguments are passed to the Namespace() constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>'Union[Namespace, None]'</code> <p>New Namespace object or None if namespace is removed.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def bind(\n    self,\n    prefix: str,\n    namespace: \"Union[str, Namespace, Triplestore, None]\" = \"\",\n    **kwargs,\n) -&gt; \"Union[Namespace, None]\":\n    \"\"\"Bind prefix to namespace and return the new Namespace object.\n\n    Arguments:\n        prefix: Prefix to bind the the namespace.\n        namespace: Namespace to bind to.  The default is to bind to the\n            `base_iri` of the current triplestore.\n            If `namespace` is None, the corresponding prefix is removed.\n        kwargs: Keyword arguments are passed to the Namespace()\n            constructor.\n\n    Returns:\n        New Namespace object or None if namespace is removed.\n    \"\"\"\n    if namespace == \"\":\n        namespace = self\n\n    if isinstance(namespace, str):\n        ns = Namespace(namespace, **kwargs)\n    elif isinstance(namespace, Triplestore):\n        if not namespace.base_iri:\n            raise ValueError(\n                f\"triplestore object {namespace} has no `base_iri`\"\n            )\n        ns = Namespace(namespace.base_iri, **kwargs)\n    elif isinstance(namespace, Namespace):\n        # pylint: disable=protected-access\n        ns = Namespace(namespace._iri, **kwargs)\n    elif namespace is None:\n        del self.namespaces[prefix]\n        if hasattr(self.backend, \"bind\"):\n            self.backend.bind(prefix, None)\n        return None\n    else:\n        raise TypeError(f\"invalid `namespace` type: {type(namespace)}\")\n\n    if hasattr(self.backend, \"bind\"):\n        self.backend.bind(\n            prefix, ns._iri  # pylint: disable=protected-access\n        )\n\n    self.namespaces[prefix] = ns\n    return ns\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.close","title":"<code>close(self)</code>","text":"<p>Calls the backend close() method if it is implemented. Otherwise, this method has no effect.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Calls the backend close() method if it is implemented.\n    Otherwise, this method has no effect.\n    \"\"\"\n    # It should be ok to call close() regardless of whether the backend\n    # implements this method or not.  Hence, don't call _check_method().\n    if not self.closed and hasattr(self.backend, \"close\"):\n        self.backend.close()\n    self.closed = True\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.create_database","title":"<code>create_database(backend, database, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a new database in backend.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Name of backend.</p> required <code>database</code> <code>str</code> <p>Name of the new database.</p> required <code>kwargs</code> <p>Keyword arguments passed to the backend create_database() method.</p> <code>{}</code> <p>Note</p> <p>This is a class method, which operates on the backend triplestore without connecting to it.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>@classmethod\ndef create_database(cls, backend: str, database: str, **kwargs):\n    \"\"\"Create a new database in backend.\n\n    Arguments:\n        backend: Name of backend.\n        database: Name of the new database.\n        kwargs: Keyword arguments passed to the backend\n            create_database() method.\n\n    Note:\n        This is a class method, which operates on the backend\n        triplestore without connecting to it.\n    \"\"\"\n    cls._check_backend_method(backend, \"create_database\")\n    backend_class = cls._get_backend(backend)\n    return backend_class.create_database(database=database, **kwargs)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.eval_function","title":"<code>eval_function(self, func_iri, args=(), kwargs=None)</code>","text":"<p>Evaluate mapping function and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>func_iri</code> <p>IRI of the function to be evaluated.</p> required <code>args</code> <p>Sequence of positional arguments passed to the function.</p> <code>()</code> <code>kwargs</code> <p>Mapping of keyword arguments passed to the function.</p> <code>None</code> <p>Returns:</p> Type Description <code>'Any'</code> <p>The return value of the function.</p> <p>Note</p> <p>The current implementation does not protect against side effect or malicious code.  Be warned! This may be improved in the future.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def eval_function(self, func_iri, args=(), kwargs=None) -&gt; \"Any\":\n    \"\"\"Evaluate mapping function and return the result.\n\n    Arguments:\n        func_iri: IRI of the function to be evaluated.\n        args: Sequence of positional arguments passed to the function.\n        kwargs: Mapping of keyword arguments passed to the function.\n\n    Returns:\n        The return value of the function.\n\n    Note:\n        The current implementation does not protect against side\n        effect or malicious code.  Be warned!\n        This may be improved in the future.\n    \"\"\"\n    func = self._get_function(func_iri)\n    if not kwargs:\n        kwargs = {}\n\n    # FIXME: Add sandboxing\n    result = func(*args, **kwargs)\n\n    return result\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.expand_iri","title":"<code>expand_iri(self, iri, strict=False)</code>","text":"<p>Return the full IRI if <code>iri</code> is prefixed. Otherwise <code>iri</code> isreturned.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from tripper import Triplestore\n&gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n\n# Unknown prefix raises an exception\n&gt;&gt;&gt; ts.expand_iri(\"ex:Concept\", strict=True)  # doctest: +ELLIPSIS\nTraceback (most recent call last):\n...\ntripper.errors.NamespaceError: Undefined prefix \"ex\" in IRI: ex:Concept\n\n&gt;&gt;&gt; EX = ts.bind(\"ex\", \"http://example.com#\")\n&gt;&gt;&gt; ts.expand_iri(\"ex:Concept\")\n'http://example.com#Concept'\n\n# Returns iri if it has no prefix\n&gt;&gt;&gt; ts.expand_iri(\"http://example.com#Concept\")\n'http://example.com#Concept'\n</code></pre> Source code in <code>tripper/triplestore.py</code> <pre><code>def expand_iri(self, iri: str, strict: bool = False) -&gt; str:\n    \"\"\"\n    Return the full IRI if `iri` is prefixed.\n    Otherwise `iri` isreturned.\n\n    Examples:\n\n    ```python\n    &gt;&gt;&gt; from tripper import Triplestore\n    &gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n\n    # Unknown prefix raises an exception\n    &gt;&gt;&gt; ts.expand_iri(\"ex:Concept\", strict=True)  # doctest: +ELLIPSIS\n    Traceback (most recent call last):\n    ...\n    tripper.errors.NamespaceError: Undefined prefix \"ex\" in IRI: ex:Concept\n\n    &gt;&gt;&gt; EX = ts.bind(\"ex\", \"http://example.com#\")\n    &gt;&gt;&gt; ts.expand_iri(\"ex:Concept\")\n    'http://example.com#Concept'\n\n    # Returns iri if it has no prefix\n    &gt;&gt;&gt; ts.expand_iri(\"http://example.com#Concept\")\n    'http://example.com#Concept'\n\n    ```\n\n    \"\"\"\n    return expand_iri(iri, self.namespaces, strict=strict)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.has","title":"<code>has(self, subject=None, predicate=None, object=None)</code>","text":"<p>Returns true if the triplestore has any triple matching the give subject, predicate and/or object.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def has(\n    self, subject=None, predicate=None, object=None\n):  # pylint: disable=redefined-builtin\n    \"\"\"Returns true if the triplestore has any triple matching\n    the give subject, predicate and/or object.\"\"\"\n    triple = self.triples(\n        subject=subject, predicate=predicate, object=object\n    )\n    try:\n        next(triple)\n    except StopIteration:\n        return False\n    return True\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.is_available","title":"<code>is_available(self, timeout=5, interval=1)</code>","text":"<p>Checks if the backend is available.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Total time in seconds to wait for a response.</p> <code>5</code> <code>interval</code> <code>float</code> <p>Internal time interval in seconds between checking if the service has responded.</p> <code>1</code> <p>Returns:</p> Type Description <code>bool</code> <p>Returns true if the backend is available.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def is_available(self, timeout: float = 5, interval: float = 1) -&gt; bool:\n    \"\"\"Checks if the backend is available.\n\n    Arguments:\n        timeout: Total time in seconds to wait for a response.\n        interval: Internal time interval in seconds between checking if\n            the service has responded.\n\n    Returns:\n        Returns true if the backend is available.\n    \"\"\"\n    if hasattr(self.backend, \"is_available\"):\n        return self.backend.is_available(\n            timeout=timeout, interval=interval\n        )\n    return True\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.list_databases","title":"<code>list_databases(backend, **kwargs)</code>  <code>classmethod</code>","text":"<p>For backends that supports multiple databases, list of all databases.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Name of backend.</p> required <code>kwargs</code> <p>Keyword arguments passed to the backend list_databases() method.</p> <code>{}</code> <p>Note</p> <p>This is a class method, which operates on the backend triplestore without connecting to it.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>@classmethod\ndef list_databases(cls, backend: str, **kwargs):\n    \"\"\"For backends that supports multiple databases, list of all\n    databases.\n\n    Arguments:\n        backend: Name of backend.\n        kwargs: Keyword arguments passed to the backend\n            list_databases() method.\n\n    Note:\n        This is a class method, which operates on the backend\n        triplestore without connecting to it.\n    \"\"\"\n    cls._check_backend_method(backend, \"list_databases\")\n    backend_class = cls._get_backend(backend)\n    return backend_class.list_databases(**kwargs)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.map","title":"<code>map(self, source, target, cost=None, target_cost=True)</code>","text":"<p>Add 'mapsTo' relation to the triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Source IRI.</p> required <code>target</code> <code>str</code> <p>IRI of target ontological concept.</p> required <code>cost</code> <code>'Optional[Union[float, Callable]]'</code> <p>User-defined cost of following this mapping relation represented as a float.  It may be given either as a float or as a callable taking three arguments</p> <pre><code>cost(triplestore, input_iris, output_iri)\n</code></pre> <p>and returning the cost as a float.</p> <code>None</code> <code>target_cost</code> <code>bool</code> <p>Whether the cost is assigned to mapping steps that have <code>target</code> as output.</p> <code>True</code> Source code in <code>tripper/triplestore.py</code> <pre><code>def map(\n    self,\n    source: str,\n    target: str,\n    cost: \"Optional[Union[float, Callable]]\" = None,\n    target_cost: bool = True,\n):\n    \"\"\"Add 'mapsTo' relation to the triplestore.\n\n    Arguments:\n        source: Source IRI.\n        target: IRI of target ontological concept.\n        cost: User-defined cost of following this mapping relation\n            represented as a float.  It may be given either as a\n            float or as a callable taking three arguments\n\n                cost(triplestore, input_iris, output_iri)\n\n            and returning the cost as a float.\n        target_cost: Whether the cost is assigned to mapping steps\n            that have `target` as output.\n    \"\"\"\n    return self.add_mapsTo(\n        target=target,\n        source=source,\n        cost=cost,\n        target_cost=target_cost,\n    )\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.objects","title":"<code>objects(self, subject=None, predicate=None)</code>","text":"<p>Returns a generator of objects for given subject and predicate.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def objects(self, subject=None, predicate=None):\n    \"\"\"Returns a generator of objects for given subject and predicate.\"\"\"\n    for _, _, o in self.triples(subject=subject, predicate=predicate):\n        yield o\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.parse","title":"<code>parse(self, source=None, format=None, fallback_backend='rdflib', fallback_backend_kwargs=None, **kwargs)</code>","text":"<p>Parse source and add the resulting triples to triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>File-like object. File name or URL.</p> <code>None</code> <code>format</code> <p>Needed if format can not be inferred from source.</p> <code>None</code> <code>fallback_backend</code> <p>If the current backend doesn't implement parse, use the <code>fallback_backend</code> instead.</p> <code>'rdflib'</code> <code>fallback_backend_kwargs</code> <p>Dict with additional keyword arguments for initialising <code>fallback_backend</code>.</p> <code>None</code> <code>kwargs</code> <p>Keyword arguments passed to the backend. The rdflib backend supports e.g. <code>location</code> (absolute or relative URL) and <code>data</code> (string containing the data to be parsed) arguments.</p> <code>{}</code> Source code in <code>tripper/triplestore.py</code> <pre><code>def parse(\n    self,\n    source=None,\n    format=None,\n    fallback_backend=\"rdflib\",\n    fallback_backend_kwargs=None,\n    **kwargs,  # pylint: disable=redefined-builtin\n) -&gt; None:\n    \"\"\"Parse source and add the resulting triples to triplestore.\n\n    Arguments:\n        source: File-like object. File name or URL.\n        format: Needed if format can not be inferred from source.\n        fallback_backend: If the current backend doesn't implement\n            parse, use the `fallback_backend` instead.\n        fallback_backend_kwargs: Dict with additional keyword arguments\n            for initialising `fallback_backend`.\n        kwargs: Keyword arguments passed to the backend.\n            The rdflib backend supports e.g. `location` (absolute\n            or relative URL) and `data` (string containing the\n            data to be parsed) arguments.\n    \"\"\"\n    if hasattr(self.backend, \"parse\"):\n        self._check_method(\"parse\")\n        self.backend.parse(source=source, format=format, **kwargs)\n    else:\n        if fallback_backend_kwargs is None:\n            fallback_backend_kwargs = {}\n        ts = Triplestore(\n            backend=fallback_backend, **fallback_backend_kwargs\n        )\n        ts.parse(source=source, format=format, **kwargs)\n        self.add_triples(ts.triples())\n\n    if hasattr(self.backend, \"namespaces\"):\n        for prefix, namespace in self.backend.namespaces().items():\n            if prefix and prefix not in self.namespaces:\n                self.namespaces[prefix] = Namespace(namespace)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.predicate_objects","title":"<code>predicate_objects(self, subject=None)</code>","text":"<p>Returns a generator of (predicate, object) tuples for given subject.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def predicate_objects(self, subject=None):\n    \"\"\"Returns a generator of (predicate, object) tuples for given\n    subject.\"\"\"\n    for _, p, o in self.triples(subject=subject):\n        yield p, o\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.predicates","title":"<code>predicates(self, subject=None, object=None)</code>","text":"<p>Returns a generator of predicates for given subject and object.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def predicates(\n    self, subject=None, object=None  # pylint: disable=redefined-builtin\n):\n    \"\"\"Returns a generator of predicates for given subject and object.\"\"\"\n    for _, p, _ in self.triples(subject=subject, object=object):\n        yield p\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.prefix_iri","title":"<code>prefix_iri(self, iri, require_prefixed=False)</code>","text":"<p>Return prefixed IRI.</p> <p>This is the reverse of expand_iri().</p> <p>If <code>require_prefixed</code> is true, a NamespaceError exception is raised if no prefix can be found.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from tripper import Triplestore\n&gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n&gt;&gt;&gt; ts.prefix_iri(\"http://example.com#Concept\")\n'http://example.com#Concept'\n\n&gt;&gt;&gt; ts.prefix_iri(\n...     \"http://example.com#Concept\", require_prefixed=True\n... )  # doctest: +ELLIPSIS\nTraceback (most recent call last):\n...\ntripper.errors.NamespaceError: No prefix defined for IRI: http://example.com#Concept\n\n&gt;&gt;&gt; EX = ts.bind(\"ex\", \"http://example.com#\")\n&gt;&gt;&gt; ts.prefix_iri(\"http://example.com#Concept\")\n'ex:Concept'\n</code></pre> Source code in <code>tripper/triplestore.py</code> <pre><code>def prefix_iri(self, iri: str, require_prefixed: bool = False):\n    # pylint: disable=line-too-long\n    \"\"\"Return prefixed IRI.\n\n    This is the reverse of expand_iri().\n\n    If `require_prefixed` is true, a NamespaceError exception is raised\n    if no prefix can be found.\n\n    Examples:\n\n    ```python\n    &gt;&gt;&gt; from tripper import Triplestore\n    &gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n    &gt;&gt;&gt; ts.prefix_iri(\"http://example.com#Concept\")\n    'http://example.com#Concept'\n\n    &gt;&gt;&gt; ts.prefix_iri(\n    ...     \"http://example.com#Concept\", require_prefixed=True\n    ... )  # doctest: +ELLIPSIS\n    Traceback (most recent call last):\n    ...\n    tripper.errors.NamespaceError: No prefix defined for IRI: http://example.com#Concept\n\n    &gt;&gt;&gt; EX = ts.bind(\"ex\", \"http://example.com#\")\n    &gt;&gt;&gt; ts.prefix_iri(\"http://example.com#Concept\")\n    'ex:Concept'\n\n    ```\n\n    \"\"\"\n    return prefix_iri(iri, self.namespaces, require_prefixed)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.query","title":"<code>query(self, query, iris=None, literals=None, **kwargs)</code>","text":"<p>SPARQL query.</p> <p>The <code>query</code> argument may contain variables for IRIs and literals, to be substituted using the <code>iris</code> and <code>literals</code> arguments. These variables are prefixed <code>$</code>. This makes them easy to distinguish from query variables, that are typically prefixed with <code>?</code>.</p> <p>The query substitutions may be useful when the query is constructed from user input, since they are properly escaped and will be inserted in the query as a single token.  This may prevent sparql injection attacks.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>String with the SPARQL query.</p> required <code>iris</code> <code>'Optional[dict]'</code> <p>Dict used for query substitutions that maps IRI variables to IRIs. The IRIs may be provided as fully expanded or prefixed with a prefix registered in the triplestore namespace.</p> <code>None</code> <code>literals</code> <code>'Optional[dict]'</code> <p>Dict used for query substitutions that maps literal variables to literals.</p> <code>None</code> <code>kwargs</code> <p>Keyword arguments passed to the backend query() method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>The return type depends on type of query</code> <ul> <li>SELECT: list of tuples of IRIs for each matching row</li> <li>ASK: bool</li> <li>CONSTRUCT, DESCRIBE: generator over triples</li> </ul> <p>Note</p> <p>This method is intended for SELECT, ASK, CONSTRUCT and DESCRIBE queries.  Use the update() method for INSERT and DELETE queries.</p> <p>Not all backends may support all types of queries.</p> <p>Examples:</p> <p>Query for everyone with the name \"John Dow\":</p> <pre><code>&gt;&gt;&gt; from tripper import FOAF, Literal, Triplestore\n&gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n&gt;&gt;&gt; ts.bind(\"foaf\", FOAF)\nNamespace('http://xmlns.com/foaf/0.1/')\n</code></pre> <pre><code>&gt;&gt;&gt; ts.add_triples([\n...     (\":john\", FOAF.name, Literal(\"John Dow\")),\n...     (\":jack\", FOAF.name, Literal(\"Jack Hudson\")),\n... ])\n&gt;&gt;&gt; ts.query(\n...     \"SELECT ?s WHERE { ?s $name $obj .}\",\n...     iris={\"name\": \"foaf:name\"},\n...     literals={\"obj\": \"John Dow\"},\n... )\n[(':john',)]\n</code></pre> Source code in <code>tripper/triplestore.py</code> <pre><code>def query(\n    self,\n    query: str,\n    iris: \"Optional[dict]\" = None,\n    literals: \"Optional[dict]\" = None,\n    **kwargs,\n) -&gt; \"Any\":\n    \"\"\"SPARQL query.\n\n    The `query` argument may contain variables for IRIs and literals,\n    to be substituted using the `iris` and `literals` arguments. These\n    variables are prefixed `$`. This makes them easy to distinguish from\n    query variables, that are typically prefixed with `?`.\n\n    The query substitutions may be useful when the query is constructed\n    from user input, since they are properly escaped and will be inserted\n    in the query as a single token.  This may prevent sparql injection\n    attacks.\n\n    Arguments:\n        query: String with the SPARQL query.\n        iris: Dict used for query substitutions that maps IRI variables\n            to IRIs. The IRIs may be provided as fully expanded or\n            prefixed with a prefix registered in the triplestore namespace.\n        literals: Dict used for query substitutions that maps literal\n            variables to literals.\n        kwargs: Keyword arguments passed to the backend query() method.\n\n    Returns:\n        The return type depends on type of query:\n          - SELECT: list of tuples of IRIs for each matching row\n          - ASK: bool\n          - CONSTRUCT, DESCRIBE: generator over triples\n\n    Note:\n        This method is intended for SELECT, ASK, CONSTRUCT and\n        DESCRIBE queries.  Use the update() method for INSERT and\n        DELETE queries.\n\n        Not all backends may support all types of queries.\n\n    Examples:\n        Query for everyone with the name \"John Dow\":\n\n        &gt;&gt;&gt; from tripper import FOAF, Literal, Triplestore\n        &gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n        &gt;&gt;&gt; ts.bind(\"foaf\", FOAF)\n        Namespace('http://xmlns.com/foaf/0.1/')\n\n        &gt;&gt;&gt; ts.add_triples([\n        ...     (\":john\", FOAF.name, Literal(\"John Dow\")),\n        ...     (\":jack\", FOAF.name, Literal(\"Jack Hudson\")),\n        ... ])\n        &gt;&gt;&gt; ts.query(\n        ...     \"SELECT ?s WHERE { ?s $name $obj .}\",\n        ...     iris={\"name\": \"foaf:name\"},\n        ...     literals={\"obj\": \"John Dow\"},\n        ... )\n        [(':john',)]\n\n    \"\"\"\n    self._check_method(\"query\")\n    new_query = substitute_query(\n        query, iris=iris, literals=literals, prefixes=self.namespaces\n    )\n    return self.backend.query(new_query, **kwargs)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.remove","title":"<code>remove(self, subject=None, predicate=None, object=None, triple=None)</code>","text":"<p>Remove all matching triples from the backend.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>'Optional[Union[str, Triple]]'</code> <p>If given, match triples with this subject. For backward compatibility <code>subject</code> may also be an <code>(s, p, o)</code> triple.</p> <code>None</code> <code>predicate</code> <code>'Optional[str]'</code> <p>If given, match triples with this predicate.</p> <code>None</code> <code>object</code> <code>'Optional[Union[str, Literal]]'</code> <p>If given, match triples with this object.</p> <code>None</code> <code>triple</code> <code>'Optional[Triple]'</code> <p>Deprecated. A <code>(s, p, o)</code> tuple where <code>s</code>, <code>p</code> and <code>o</code> should either be None (matching anything) or an exact IRI to match.</p> <code>None</code> Source code in <code>tripper/triplestore.py</code> <pre><code>def remove(  # pylint: disable=redefined-builtin\n    self,\n    subject: \"Optional[Union[str, Triple]]\" = None,\n    predicate: \"Optional[str]\" = None,\n    object: \"Optional[Union[str, Literal]]\" = None,\n    triple: \"Optional[Triple]\" = None,\n) -&gt; None:\n    \"\"\"Remove all matching triples from the backend.\n\n    Arguments:\n        subject: If given, match triples with this subject.\n            For backward compatibility `subject` may also be an\n            `(s, p, o)` triple.\n        predicate: If given, match triples with this predicate.\n        object: If given, match triples with this object.\n        triple: Deprecated. A `(s, p, o)` tuple where `s`, `p` and `o`\n            should either be None (matching anything) or an exact IRI\n            to match.\n    \"\"\"\n    # __TODO__: Remove these lines when deprecated\n    if triple or (subject and not isinstance(subject, str)):\n        warnings.warn(\n            \"The `triple` argument is deprecated.  Use `subject`, \"\n            \"`predicate` and `object` arguments instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    if subject and not isinstance(subject, str):\n        subject, predicate, object = subject\n    elif triple:\n        subject, predicate, object = triple\n\n    return self.backend.remove((subject, predicate, object))\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.remove_database","title":"<code>remove_database(backend, database, **kwargs)</code>  <code>classmethod</code>","text":"<p>Remove a database in backend.</p> <p>Parameters:</p> Name Type Description Default <code>backend</code> <code>str</code> <p>Name of backend.</p> required <code>database</code> <code>str</code> <p>Name of the database to be removed.</p> required <code>kwargs</code> <p>Keyword arguments passed to the backend remove_database() method.</p> <code>{}</code> <p>Note</p> <p>This is a class method, which operates on the backend triplestore without connecting to it.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>@classmethod\ndef remove_database(cls, backend: str, database: str, **kwargs):\n    \"\"\"Remove a database in backend.\n\n    Arguments:\n        backend: Name of backend.\n        database: Name of the database to be removed.\n        kwargs: Keyword arguments passed to the backend\n            remove_database() method.\n\n    Note:\n        This is a class method, which operates on the backend\n        triplestore without connecting to it.\n    \"\"\"\n    cls._check_backend_method(backend, \"remove_database\")\n    backend_class = cls._get_backend(backend)\n    return backend_class.remove_database(database=database, **kwargs)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.restrictions","title":"<code>restrictions(self, cls=None, property=None, value=None, type=None, cardinality=None, asdict=True)</code>","text":"<p>Returns a generator over matching restrictions.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>'Optional[str]'</code> <p>IRI of class to which the restriction applies.</p> <code>None</code> <code>property</code> <code>'Optional[str]'</code> <p>IRI of restriction property.</p> <code>None</code> <code>value</code> <code>'Optional[Union[str, Literal]]'</code> <p>The IRI or literal value of the restriction target.</p> <code>None</code> <code>type</code> <code>'Optional[RestrictionType]'</code> <p>The type of the restriction.  Should be one of: - some: existential restriction (value is a class IRI) - only: universal restriction (value is a class IRI) - exactly: cardinality restriction (value is a class IRI) - min: minimum cardinality restriction (value is a class IRI) - max: maximum cardinality restriction (value is a class IRI) - value: Value restriction (value is an IRI of an individual   or a literal)</p> <code>None</code> <code>cardinality</code> <code>'Optional[int]'</code> <p>the cardinality value for cardinality restrictions.</p> <code>None</code> <code>asdict</code> <code>bool</code> <p>Whether to returned generator is over dicts (see _get_restriction_dict()). Default is to return a generator over blank node IRIs.</p> <code>True</code> <p>Returns:</p> Type Description <code>'Generator[dict, None, None]'</code> <p>A generator over matching restrictions.  See <code>asdict</code> argument for types iterated over.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def restrictions(  # pylint: disable=redefined-builtin\n    self,\n    cls: \"Optional[str]\" = None,\n    property: \"Optional[str]\" = None,\n    value: \"Optional[Union[str, Literal]]\" = None,\n    type: \"Optional[RestrictionType]\" = None,\n    cardinality: \"Optional[int]\" = None,\n    asdict: bool = True,\n) -&gt; \"Generator[dict, None, None]\":\n    # pylint: disable=too-many-boolean-expressions\n    \"\"\"Returns a generator over matching restrictions.\n\n    Arguments:\n        cls: IRI of class to which the restriction applies.\n        property: IRI of restriction property.\n        value: The IRI or literal value of the restriction target.\n        type: The type of the restriction.  Should be one of:\n            - some: existential restriction (value is a class IRI)\n            - only: universal restriction (value is a class IRI)\n            - exactly: cardinality restriction (value is a class IRI)\n            - min: minimum cardinality restriction (value is a class IRI)\n            - max: maximum cardinality restriction (value is a class IRI)\n            - value: Value restriction (value is an IRI of an individual\n              or a literal)\n\n        cardinality: the cardinality value for cardinality restrictions.\n        asdict: Whether to returned generator is over dicts (see\n            _get_restriction_dict()). Default is to return a generator\n            over blank node IRIs.\n\n    Returns:\n        A generator over matching restrictions.  See `asdict` argument\n        for types iterated over.\n    \"\"\"\n    if type is None:\n        types = set(self._restriction_types.keys())\n    elif type not in self._restriction_types:\n        raise ArgumentValueError(\n            f\"Invalid `type='{type}'`, it must be one of: \"\n            f\"{', '.join(self._restriction_types.keys())}.\"\n        )\n    else:\n        types = {type} if isinstance(type, str) else set(type)\n\n    if isinstance(value, Literal):\n        types.intersection_update({\"value\"})\n    elif isinstance(value, str):\n        types.difference_update({\"value\"})\n\n    if cardinality:\n        types.intersection_update({\"exactly\", \"min\", \"max\"})\n    if not types:\n        raise ArgumentValueError(\n            f\"Inconsistent type='{type}', value='{value}' and \"\n            f\"cardinality='{cardinality}' arguments\"\n        )\n    pred = {self._restriction_types[t][0] for t in types}\n    card = {\n        self._restriction_types[t][1]\n        for t in types\n        if self._restriction_types[t][1]\n    }\n\n    if cardinality:\n        lcard = Literal(cardinality, datatype=XSD.nonNegativeInteger)\n\n    for iri in self.subjects(predicate=OWL.onProperty, object=property):\n        if (\n            self.has(iri, RDF.type, OWL.Restriction)\n            and (not cls or self.has(cls, RDFS.subClassOf, iri))\n            and any(self.has(iri, p, value) for p in pred)\n            and (\n                not card\n                or not cardinality\n                or any(self.has(iri, c, lcard) for c in card)\n            )\n        ):\n            yield self._get_restriction_dict(iri) if asdict else iri\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.serialize","title":"<code>serialize(self, destination=None, format='turtle', fallback_backend='rdflib', fallback_backend_kwargs=None, **kwargs)</code>","text":"<p>Serialise triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <p>File name or object to write to.  If None, the serialisation is returned.</p> <code>None</code> <code>format</code> <p>Format to serialise as.  Supported formats, depends on the backend.</p> <code>'turtle'</code> <code>fallback_backend</code> <p>If the current backend doesn't implement serialisation, use the <code>fallback_backend</code> instead.</p> <code>'rdflib'</code> <code>fallback_backend_kwargs</code> <p>Dict with additional keyword arguments for initialising <code>fallback_backend</code>.</p> <code>None</code> <code>kwargs</code> <p>Passed to the backend serialize() method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>'Union[None, str]'</code> <p>Serialized string if <code>destination</code> is None.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def serialize(\n    self,\n    destination=None,\n    format=\"turtle\",  # pylint: disable=redefined-builtin\n    fallback_backend=\"rdflib\",\n    fallback_backend_kwargs=None,\n    **kwargs,\n) -&gt; \"Union[None, str]\":\n    \"\"\"Serialise triplestore.\n\n    Arguments:\n        destination: File name or object to write to.  If None, the\n            serialisation is returned.\n        format: Format to serialise as.  Supported formats, depends on\n            the backend.\n        fallback_backend: If the current backend doesn't implement\n            serialisation, use the `fallback_backend` instead.\n        fallback_backend_kwargs: Dict with additional keyword arguments\n            for initialising `fallback_backend`.\n        kwargs: Passed to the backend serialize() method.\n\n    Returns:\n        Serialized string if `destination` is None.\n    \"\"\"\n    if hasattr(self.backend, \"parse\"):\n        self._check_method(\"serialize\")\n        return self.backend.serialize(\n            destination=destination, format=format, **kwargs\n        )\n\n    if fallback_backend_kwargs is None:\n        fallback_backend_kwargs = {}\n    ts = Triplestore(backend=fallback_backend, **fallback_backend_kwargs)\n    ts.add_triples(self.triples())\n    for prefix, iri in self.namespaces.items():\n        ts.bind(prefix, iri)\n    return ts.serialize(destination=destination, format=format, **kwargs)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.set","title":"<code>set(self, triple)</code>","text":"<p>Convenience method to update the value of object.</p> <p>Removes any existing triples for subject and predicate before adding the given <code>triple</code>.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def set(self, triple):\n    \"\"\"Convenience method to update the value of object.\n\n    Removes any existing triples for subject and predicate before adding\n    the given `triple`.\n    \"\"\"\n    s, p, _ = triple\n    self.remove(s, p)\n    self.add(triple)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.subject_objects","title":"<code>subject_objects(self, predicate=None)</code>","text":"<p>Returns a generator of (subject, object) tuples for given predicate.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def subject_objects(self, predicate=None):\n    \"\"\"Returns a generator of (subject, object) tuples for given\n    predicate.\"\"\"\n    for s, _, o in self.triples(predicate=predicate):\n        yield s, o\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.subject_predicates","title":"<code>subject_predicates(self, object=None)</code>","text":"<p>Returns a generator of (subject, predicate) tuples for given object.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def subject_predicates(\n    self, object=None\n):  # pylint: disable=redefined-builtin\n    \"\"\"Returns a generator of (subject, predicate) tuples for given\n    object.\"\"\"\n    for s, p, _ in self.triples(object=object):\n        yield s, p\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.subjects","title":"<code>subjects(self, predicate=None, object=None)</code>","text":"<p>Returns a generator of subjects for given predicate and object.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def subjects(\n    self, predicate=None, object=None  # pylint: disable=redefined-builtin\n):\n    \"\"\"Returns a generator of subjects for given predicate and object.\"\"\"\n    for s, _, _ in self.triples(predicate=predicate, object=object):\n        yield s\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.triples","title":"<code>triples(self, subject=None, predicate=None, object=None, triple=None)</code>","text":"<p>Returns a generator over matching triples.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <code>'Optional[Union[str, Triple]]'</code> <p>If given, match triples with this subject.</p> <code>None</code> <code>predicate</code> <code>'Optional[str]'</code> <p>If given, match triples with this predicate.</p> <code>None</code> <code>object</code> <code>'Optional[Union[str, Literal]]'</code> <p>If given, match triples with this object.</p> <code>None</code> <code>triple</code> <code>'Optional[Triple]'</code> <p>Deprecated. A <code>(s, p, o)</code> tuple where <code>s</code>, <code>p</code> and <code>o</code> should either be None (matching anything) or an exact IRI to match.</p> <code>None</code> <p>Returns:</p> Type Description <code>'Generator[Triple, None, None]'</code> <p>Generator over all matching triples.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def triples(  # pylint: disable=redefined-builtin\n    self,\n    subject: \"Optional[Union[str, Triple]]\" = None,\n    predicate: \"Optional[str]\" = None,\n    object: \"Optional[Union[str, Literal]]\" = None,\n    triple: \"Optional[Triple]\" = None,\n) -&gt; \"Generator[Triple, None, None]\":\n    \"\"\"Returns a generator over matching triples.\n\n    Arguments:\n        subject: If given, match triples with this subject.\n        predicate: If given, match triples with this predicate.\n        object: If given, match triples with this object.\n        triple: Deprecated. A `(s, p, o)` tuple where `s`, `p` and `o`\n            should either be None (matching anything) or an exact IRI\n            to match.\n\n    Returns:\n        Generator over all matching triples.\n    \"\"\"\n    # __TODO__: Remove these lines when deprecated\n    if triple or (subject and not isinstance(subject, str)):\n        warnings.warn(\n            \"The `triple` argument is deprecated.  Use `subject`, \"\n            \"`predicate` and `object` arguments instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    if subject and not isinstance(subject, str):\n        subject, predicate, object = subject\n    elif triple:\n        subject, predicate, object = triple\n\n    return self.backend.triples((subject, predicate, object))\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.update","title":"<code>update(self, query, iris=None, literals=None, **kwargs)</code>","text":"<p>Update triplestore with SPARQL.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>String with the SPARQL query.</p> required <code>iris</code> <code>'Optional[dict]'</code> <p>Dict used for query substitutions that maps IRI variables to IRIs. The IRIs may be provided as fully expanded or prefixed with a prefix registered in the triplestore namespace.</p> <code>None</code> <code>literals</code> <code>'Optional[dict]'</code> <p>Dict used for query substitutions that maps literal variables to literals.</p> <code>None</code> <code>kwargs</code> <p>Keyword arguments passed to the backend update() method.</p> <code>{}</code> <p>Note</p> <p>See <code>query()</code> for how to the query substitution arguments <code>iris</code> and <code>literals</code>.</p> <p>This method is intended for INSERT and DELETE queries. Use the query() method for SELECT, ASK, CONSTRUCT and DESCRIBE queries.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def update(\n    self,\n    query: str,\n    iris: \"Optional[dict]\" = None,\n    literals: \"Optional[dict]\" = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Update triplestore with SPARQL.\n\n    Arguments:\n        query: String with the SPARQL query.\n        iris: Dict used for query substitutions that maps IRI variables\n            to IRIs. The IRIs may be provided as fully expanded or\n            prefixed with a prefix registered in the triplestore namespace.\n        literals: Dict used for query substitutions that maps literal\n            variables to literals.\n        kwargs: Keyword arguments passed to the backend update() method.\n\n    Note:\n        See `query()` for how to the query substitution arguments `iris`\n        and `literals`.\n\n        This method is intended for INSERT and DELETE queries. Use\n        the query() method for SELECT, ASK, CONSTRUCT and DESCRIBE queries.\n\n    \"\"\"\n    self._check_method(\"update\")\n    new_query = substitute_query(\n        query, iris=iris, literals=literals, prefixes=self.namespaces\n    )\n    return self.backend.update(new_query, **kwargs)\n</code></pre>"},{"location":"api_reference/triplestore/#tripper.triplestore.Triplestore.value","title":"<code>value(self, subject=None, predicate=None, object=None, default=None, any=False, lang=None)</code>","text":"<p>Return the value for a pair of two criteria.</p> <p>Useful if one knows that there may only be one value. Two of <code>subject</code>, <code>predicate</code> or <code>object</code> must be provided.</p> <p>Parameters:</p> Name Type Description Default <code>subject</code> <p>Possible criteria to match.</p> <code>None</code> <code>predicate</code> <p>Possible criteria to match.</p> <code>None</code> <code>object</code> <p>Possible criteria to match.</p> <code>None</code> <code>default</code> <p>Value to return if no matches are found.</p> <code>None</code> <code>any</code> <p>Used to define how many values to return. Can be set to: <code>False</code> (default): return the value or raise UniquenessError if there is more than one matching value. <code>True</code>: return any matching value if there is more than one. <code>None</code>: return a generator over all matching values.</p> <code>False</code> <code>lang</code> <p>If provided, require that the value must be a localised literal with the given language code.</p> <code>None</code> <p>Returns:</p> Type Description <code>'Union[str, Literal]'</code> <p>The value of the <code>subject</code>, <code>predicate</code> or <code>object</code> that is None.</p> Source code in <code>tripper/triplestore.py</code> <pre><code>def value(  # pylint: disable=redefined-builtin\n    self,\n    subject=None,\n    predicate=None,\n    object=None,\n    default=None,\n    any=False,\n    lang=None,\n) -&gt; \"Union[str, Literal]\":\n    \"\"\"Return the value for a pair of two criteria.\n\n    Useful if one knows that there may only be one value.\n    Two of `subject`, `predicate` or `object` must be provided.\n\n    Arguments:\n        subject: Possible criteria to match.\n        predicate: Possible criteria to match.\n        object: Possible criteria to match.\n        default: Value to return if no matches are found.\n        any: Used to define how many values to return. Can be set to:\n            `False` (default): return the value or raise UniquenessError\n            if there is more than one matching value.\n            `True`: return any matching value if there is more than one.\n            `None`: return a generator over all matching values.\n        lang: If provided, require that the value must be a localised\n            literal with the given language code.\n\n    Returns:\n        The value of the `subject`, `predicate` or `object` that is\n        None.\n    \"\"\"\n    spo = (subject, predicate, object)\n    if sum(iri is None for iri in spo) != 1:\n        raise ValueError(\n            \"Exactly one of `subject`, `predicate` or `object` must be \"\n            \"None.\"\n        )\n\n    # Index of subject-predicate-object argument that is None\n    (idx,) = [i for i, v in enumerate(spo) if v is None]\n\n    triples = self.triples(subject, predicate, object)\n\n    if lang:\n        triples = (\n            t\n            for t in triples\n            if isinstance(t[idx], Literal)\n            and t[idx].lang == lang  # type: ignore\n        )\n\n    if any is None:\n        return (t[idx] for t in triples)  # type: ignore\n\n    try:\n        value = next(triples)[idx]\n    except StopIteration:\n        return default\n\n    try:\n        next(triples)\n    except StopIteration:\n        return value\n\n    if any is True:\n        return value\n    raise UniquenessError(\n        f\"More than one match: {(subject, predicate, object)}\"\n    )\n</code></pre>"},{"location":"api_reference/triplestore_extend/","title":"triplestore_extend","text":"<p>A module that adds additional functionality to triplestore</p>"},{"location":"api_reference/triplestore_extend/#tripper.triplestore_extend.Tripper","title":"<code> Tripper            (Triplestore)         </code>","text":"<p>Class that provides additional methods for handling data in the triplestore, such as get_value, add_data and add_interpolation_source.</p> Source code in <code>tripper/triplestore_extend.py</code> <pre><code>class Tripper(Triplestore):\n    \"\"\"\n    Class that provides additional\n    methods for handling data in the triplestore,\n    such as get_value, add_data and add_interpolation_source.\n    \"\"\"\n\n    def add_data(\n        self,\n        func: \"Union[Callable, Literal]\",\n        iri: \"Optional[Union[str, Sequence]]\" = None,\n        configurations: \"Optional[dict]\" = None,\n        base_iri: \"Optional[str]\" = None,\n        standard: str = \"emmo\",\n        cost: \"Optional[Union[float, Callable]]\" = None,\n    ) -&gt; str:\n        \"\"\"Register a data source to the triplestore.\n\n        Parameters:\n            func: A callable that should return the value of the registered\n                data source.  It is called with following protopype:\n\n                    func(returns, configurations, triplestore)\n\n                The returned value may in principle be of any type, but for\n                values with unit, it is recommended to return a\n                tripper.mappings.Value object.\n                Alternatively, `func` may also be a literal value.\n            iri: IRI of ontological concept or individual that the data\n                returned by `func` should be mapped to.  If `func` is a\n                callable and multiple values are returned, it may also be\n                given as a sequenceof IRIs.\n                If not given, it will default to a new blank node.\n            configurations: Configurations passed on to `func`.\n            base_iri: Base of the IRI representing the function in the\n                knowledge base.  Defaults to the base IRI of the triplestore.\n            standard: Name of ontological standard to use when describing the\n                function.  Valid values are:\n                - \"emmo\": Elementary Multiperspective Material Ontology (EMMO)\n                - \"fno\": Function Ontology (FnO)\n            cost: User-defined cost of following this mapping relation\n                represented as a float.  It may be given either as a\n                float or as a callable taking the same arguments as `func`\n                returning the cost as a float.\n\n        Returns:\n            IRI of data source.\n        \"\"\"\n        if iri is None:\n            # pylint complains about uuid being unused if we make this an\n            # f-string\n            iri = \"_bnode_\" + str(uuid.uuid4())\n        data_source = \"_data_source_\" + random_string(8)\n        self.add((data_source, RDF.type, DataSource))\n\n        if isinstance(func, Literal):\n            self.add((data_source, hasDataValue, func))\n            if cost is not None:\n                self._add_cost(cost, data_source)\n            if isinstance(iri, str):\n                self.map(data_source, iri)\n            else:\n                raise TypeError(\"literal data can only have a single `iri`\")\n\n        elif callable(func):\n\n            def fn():\n                return func(iri, configurations, self)\n\n            # Include data source IRI in documentation to ensure that the\n            # function_id of `fn()` will differ for different data sources...\n            fn.__doc__ = (\n                f\"Function for data source: {data_source}.\\n\\n{func.__doc__}\"\n            )\n            fn.__name__ = func.__name__\n\n            func_iri = self.add_function(\n                fn,\n                expects=(),\n                returns=iri,\n                base_iri=base_iri,\n                standard=standard,\n                cost=cost,\n            )\n            self.add((data_source, hasAccessFunction, func_iri))\n        else:\n            raise TypeError(\n                f\"`func` must be a callable or literal, got {type(func)}\"\n            )\n\n        return data_source\n\n    def get_value(\n        self,\n        iri,\n        routeno=0,\n        unit: \"Optional[str]\" = None,\n        magnitude: bool = False,\n        quantity: \"Optional[Any]\" = None,\n        **kwargs,\n    ) -&gt; \"Value\":\n        \"\"\"Return the value of an individual.\n\n        Parameters:\n            iri: IRI of individual who's value we want to return.  IRI may\n                either refer to a data source or an individual mapped to\n                an ontological concept.\n            routeno: Number identifying the mapping route to apply for\n                retrieving the individual value in case IRI does not refer\n                to a data source.\n            unit: return the result in the given unit.\n                Implies `magnitude=True`.\n            magnitude: Whether to only return the magnitude of the evaluated\n                value (with no unit).\n            quantity: Quantity class to use for evaluation.  Defaults to pint.\n            kwargs: Additional arguments passed on to `mapping_routes()`.\n\n        Returns:\n            The value of the individual.\n        \"\"\"\n        from tripper.mappings import (  # pylint: disable=import-outside-toplevel\n            Value,\n            mapping_routes,\n        )\n\n        if self.has(iri, RDF.type, DataSource):\n            # `iri` refer to a DataSource\n            if self.has(iri, hasDataValue):  # literal value\n                return Value(\n                    value=parse_literal(\n                        self.value(iri, hasDataValue)\n                    ).to_python(),\n                    unit=(\n                        parse_literal(self.value(iri, hasUnit)).to_python()\n                        if self.has(iri, hasUnit)\n                        else None\n                    ),\n                    iri=self.value(iri, MAP.mapsTo),\n                    cost=(\n                        parse_literal(self.value(iri, hasCost)).to_python()\n                        if self.has(iri, hasCost)\n                        else 0.0\n                    ),\n                ).get_value(unit=unit, magnitude=magnitude, quantity=quantity)\n\n            if self.has(iri, hasAccessFunction):  # callable\n                func_iri = self.value(iri, hasAccessFunction)\n                func = self.function_repo[func_iri]\n                assert callable(func)  # nosec\n                retval = func()\n                if isinstance(retval, Value):\n                    return retval.get_value(\n                        unit=unit, magnitude=magnitude, quantity=quantity\n                    )\n                return retval\n\n            raise TripperError(\n                f\"data source {iri} has neither a 'hasDataValue' or a \"\n                f\"'hasAccessFunction' property\"\n            )\n\n        # `iri` correspond to an individual mapped to an ontological concept.\n        # In this case we check if there exists a mapping route.\n        routes = mapping_routes(\n            target=iri,\n            sources=list(self.subjects(RDF.type, DataSource)),\n            triplestore=self,\n            **kwargs,\n        )\n        if isinstance(routes, Value):\n            return routes.get_value(\n                unit=unit, magnitude=magnitude, quantity=quantity\n            )\n        return routes.eval(\n            routeno=routeno,\n            unit=unit,\n            magnitude=magnitude,\n            quantity=quantity,\n        )\n\n    def add_interpolation_source(  # pylint: disable=too-many-arguments\n        self,\n        xcoord: str,\n        ycoord: str,\n        input_iri: str,\n        output_iri: str,\n        base_iri: \"Optional[str]\" = None,\n        standard: str = \"emmo\",\n        cost: \"Optional[Union[float, Callable]]\" = None,\n        left: \"Optional[float]\" = None,\n        right: \"Optional[float]\" = None,\n        period: \"Optional[float]\" = None,\n    ) -&gt; str:\n        \"\"\"Add data source to triplestore, such that it can be used to\n        transparently transform other data.\n\n        No data will be fetch before it is actually needed.\n\n        Parameters:\n            xcoord: IRI of data source with x-coordinates `xp`.  Must be\n                increasing if argument `period` is not specified. Otherwise,\n                `xp` is internally sorted after normalising the periodic\n                boundaries with ``xp = xp % period``.\n            ycoord: IRI of data source with y-coordinates `yp`.  Must have\n                the same length as `xp`.\n            input_iri: IRI of ontological concept that interpolation input-\n                data should be mapped to.\n            output_iri: IRI of ontological concept that interpolation output-\n                data should be mapped to.\n            base_iri: Base of the IRI representing the transformation in the\n                knowledge base.  Defaults to the base IRI of the triplestore.\n            standard: Name of ontology to use when describing the\n                transformation.  Valid values are:\n                - \"emmo\": Elementary Multiperspective Material Ontology (EMMO)\n                - \"fno\": Function Ontology (FnO)\n            cost: User-defined cost of following this mapping relation\n                represented as a float.  It may be given either as a\n                float or as a callable taking the same arguments as `func`\n                returning the cost as a float.\n            left: Value to return for `x &lt; xp[0]`, default is `fp[0]`.\n            right: Value to return for `x &gt; xp[-1]`, default is `fp[-1]`.\n            period: A period for the x-coordinates. This parameter allows the\n                proper interpolation of angular x-coordinates. Parameters\n                `left` and `right` are ignored if `period` is specified.\n\n        Returns:\n            transformation_iri: IRI of the added transformation.\n\n        \"\"\"\n        try:\n            import numpy as np  # pylint: disable=import-outside-toplevel\n        except ImportError as exc:\n            raise RuntimeError(\n                \"Triplestore.add_interpolation_source() requires numpy.\\n\"\n                \"Install it with\\n\\n\"\n                \"    pip install numpy\"\n            ) from exc\n\n        def func(x):\n            xp = self.get_value(xcoord)\n            fp = self.get_value(ycoord)\n            return np.interp(\n                x,\n                xp=xp,\n                fp=fp,\n                left=left,\n                right=right,\n                period=period,\n            )\n\n        return self.add_function(\n            func,\n            expects=input_iri,\n            returns=output_iri,\n            base_iri=base_iri,\n            standard=standard,\n            cost=cost,\n        )\n</code></pre>"},{"location":"api_reference/triplestore_extend/#tripper.triplestore_extend.Tripper.add_data","title":"<code>add_data(self, func, iri=None, configurations=None, base_iri=None, standard='emmo', cost=None)</code>","text":"<p>Register a data source to the triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>'Union[Callable, Literal]'</code> <p>A callable that should return the value of the registered data source.  It is called with following protopype:</p> <pre><code>func(returns, configurations, triplestore)\n</code></pre> <p>The returned value may in principle be of any type, but for values with unit, it is recommended to return a tripper.mappings.Value object. Alternatively, <code>func</code> may also be a literal value.</p> required <code>iri</code> <code>'Optional[Union[str, Sequence]]'</code> <p>IRI of ontological concept or individual that the data returned by <code>func</code> should be mapped to.  If <code>func</code> is a callable and multiple values are returned, it may also be given as a sequenceof IRIs. If not given, it will default to a new blank node.</p> <code>None</code> <code>configurations</code> <code>'Optional[dict]'</code> <p>Configurations passed on to <code>func</code>.</p> <code>None</code> <code>base_iri</code> <code>'Optional[str]'</code> <p>Base of the IRI representing the function in the knowledge base.  Defaults to the base IRI of the triplestore.</p> <code>None</code> <code>standard</code> <code>str</code> <p>Name of ontological standard to use when describing the function.  Valid values are: - \"emmo\": Elementary Multiperspective Material Ontology (EMMO) - \"fno\": Function Ontology (FnO)</p> <code>'emmo'</code> <code>cost</code> <code>'Optional[Union[float, Callable]]'</code> <p>User-defined cost of following this mapping relation represented as a float.  It may be given either as a float or as a callable taking the same arguments as <code>func</code> returning the cost as a float.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>IRI of data source.</p> Source code in <code>tripper/triplestore_extend.py</code> <pre><code>def add_data(\n    self,\n    func: \"Union[Callable, Literal]\",\n    iri: \"Optional[Union[str, Sequence]]\" = None,\n    configurations: \"Optional[dict]\" = None,\n    base_iri: \"Optional[str]\" = None,\n    standard: str = \"emmo\",\n    cost: \"Optional[Union[float, Callable]]\" = None,\n) -&gt; str:\n    \"\"\"Register a data source to the triplestore.\n\n    Parameters:\n        func: A callable that should return the value of the registered\n            data source.  It is called with following protopype:\n\n                func(returns, configurations, triplestore)\n\n            The returned value may in principle be of any type, but for\n            values with unit, it is recommended to return a\n            tripper.mappings.Value object.\n            Alternatively, `func` may also be a literal value.\n        iri: IRI of ontological concept or individual that the data\n            returned by `func` should be mapped to.  If `func` is a\n            callable and multiple values are returned, it may also be\n            given as a sequenceof IRIs.\n            If not given, it will default to a new blank node.\n        configurations: Configurations passed on to `func`.\n        base_iri: Base of the IRI representing the function in the\n            knowledge base.  Defaults to the base IRI of the triplestore.\n        standard: Name of ontological standard to use when describing the\n            function.  Valid values are:\n            - \"emmo\": Elementary Multiperspective Material Ontology (EMMO)\n            - \"fno\": Function Ontology (FnO)\n        cost: User-defined cost of following this mapping relation\n            represented as a float.  It may be given either as a\n            float or as a callable taking the same arguments as `func`\n            returning the cost as a float.\n\n    Returns:\n        IRI of data source.\n    \"\"\"\n    if iri is None:\n        # pylint complains about uuid being unused if we make this an\n        # f-string\n        iri = \"_bnode_\" + str(uuid.uuid4())\n    data_source = \"_data_source_\" + random_string(8)\n    self.add((data_source, RDF.type, DataSource))\n\n    if isinstance(func, Literal):\n        self.add((data_source, hasDataValue, func))\n        if cost is not None:\n            self._add_cost(cost, data_source)\n        if isinstance(iri, str):\n            self.map(data_source, iri)\n        else:\n            raise TypeError(\"literal data can only have a single `iri`\")\n\n    elif callable(func):\n\n        def fn():\n            return func(iri, configurations, self)\n\n        # Include data source IRI in documentation to ensure that the\n        # function_id of `fn()` will differ for different data sources...\n        fn.__doc__ = (\n            f\"Function for data source: {data_source}.\\n\\n{func.__doc__}\"\n        )\n        fn.__name__ = func.__name__\n\n        func_iri = self.add_function(\n            fn,\n            expects=(),\n            returns=iri,\n            base_iri=base_iri,\n            standard=standard,\n            cost=cost,\n        )\n        self.add((data_source, hasAccessFunction, func_iri))\n    else:\n        raise TypeError(\n            f\"`func` must be a callable or literal, got {type(func)}\"\n        )\n\n    return data_source\n</code></pre>"},{"location":"api_reference/triplestore_extend/#tripper.triplestore_extend.Tripper.add_interpolation_source","title":"<code>add_interpolation_source(self, xcoord, ycoord, input_iri, output_iri, base_iri=None, standard='emmo', cost=None, left=None, right=None, period=None)</code>","text":"<p>Add data source to triplestore, such that it can be used to transparently transform other data.</p> <p>No data will be fetch before it is actually needed.</p> <p>Parameters:</p> Name Type Description Default <code>xcoord</code> <code>str</code> <p>IRI of data source with x-coordinates <code>xp</code>.  Must be increasing if argument <code>period</code> is not specified. Otherwise, <code>xp</code> is internally sorted after normalising the periodic boundaries with <code>xp = xp % period</code>.</p> required <code>ycoord</code> <code>str</code> <p>IRI of data source with y-coordinates <code>yp</code>.  Must have the same length as <code>xp</code>.</p> required <code>input_iri</code> <code>str</code> <p>IRI of ontological concept that interpolation input- data should be mapped to.</p> required <code>output_iri</code> <code>str</code> <p>IRI of ontological concept that interpolation output- data should be mapped to.</p> required <code>base_iri</code> <code>'Optional[str]'</code> <p>Base of the IRI representing the transformation in the knowledge base.  Defaults to the base IRI of the triplestore.</p> <code>None</code> <code>standard</code> <code>str</code> <p>Name of ontology to use when describing the transformation.  Valid values are: - \"emmo\": Elementary Multiperspective Material Ontology (EMMO) - \"fno\": Function Ontology (FnO)</p> <code>'emmo'</code> <code>cost</code> <code>'Optional[Union[float, Callable]]'</code> <p>User-defined cost of following this mapping relation represented as a float.  It may be given either as a float or as a callable taking the same arguments as <code>func</code> returning the cost as a float.</p> <code>None</code> <code>left</code> <code>'Optional[float]'</code> <p>Value to return for <code>x &lt; xp[0]</code>, default is <code>fp[0]</code>.</p> <code>None</code> <code>right</code> <code>'Optional[float]'</code> <p>Value to return for <code>x &gt; xp[-1]</code>, default is <code>fp[-1]</code>.</p> <code>None</code> <code>period</code> <code>'Optional[float]'</code> <p>A period for the x-coordinates. This parameter allows the proper interpolation of angular x-coordinates. Parameters <code>left</code> and <code>right</code> are ignored if <code>period</code> is specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>transformation_iri</code> <p>IRI of the added transformation.</p> Source code in <code>tripper/triplestore_extend.py</code> <pre><code>def add_interpolation_source(  # pylint: disable=too-many-arguments\n    self,\n    xcoord: str,\n    ycoord: str,\n    input_iri: str,\n    output_iri: str,\n    base_iri: \"Optional[str]\" = None,\n    standard: str = \"emmo\",\n    cost: \"Optional[Union[float, Callable]]\" = None,\n    left: \"Optional[float]\" = None,\n    right: \"Optional[float]\" = None,\n    period: \"Optional[float]\" = None,\n) -&gt; str:\n    \"\"\"Add data source to triplestore, such that it can be used to\n    transparently transform other data.\n\n    No data will be fetch before it is actually needed.\n\n    Parameters:\n        xcoord: IRI of data source with x-coordinates `xp`.  Must be\n            increasing if argument `period` is not specified. Otherwise,\n            `xp` is internally sorted after normalising the periodic\n            boundaries with ``xp = xp % period``.\n        ycoord: IRI of data source with y-coordinates `yp`.  Must have\n            the same length as `xp`.\n        input_iri: IRI of ontological concept that interpolation input-\n            data should be mapped to.\n        output_iri: IRI of ontological concept that interpolation output-\n            data should be mapped to.\n        base_iri: Base of the IRI representing the transformation in the\n            knowledge base.  Defaults to the base IRI of the triplestore.\n        standard: Name of ontology to use when describing the\n            transformation.  Valid values are:\n            - \"emmo\": Elementary Multiperspective Material Ontology (EMMO)\n            - \"fno\": Function Ontology (FnO)\n        cost: User-defined cost of following this mapping relation\n            represented as a float.  It may be given either as a\n            float or as a callable taking the same arguments as `func`\n            returning the cost as a float.\n        left: Value to return for `x &lt; xp[0]`, default is `fp[0]`.\n        right: Value to return for `x &gt; xp[-1]`, default is `fp[-1]`.\n        period: A period for the x-coordinates. This parameter allows the\n            proper interpolation of angular x-coordinates. Parameters\n            `left` and `right` are ignored if `period` is specified.\n\n    Returns:\n        transformation_iri: IRI of the added transformation.\n\n    \"\"\"\n    try:\n        import numpy as np  # pylint: disable=import-outside-toplevel\n    except ImportError as exc:\n        raise RuntimeError(\n            \"Triplestore.add_interpolation_source() requires numpy.\\n\"\n            \"Install it with\\n\\n\"\n            \"    pip install numpy\"\n        ) from exc\n\n    def func(x):\n        xp = self.get_value(xcoord)\n        fp = self.get_value(ycoord)\n        return np.interp(\n            x,\n            xp=xp,\n            fp=fp,\n            left=left,\n            right=right,\n            period=period,\n        )\n\n    return self.add_function(\n        func,\n        expects=input_iri,\n        returns=output_iri,\n        base_iri=base_iri,\n        standard=standard,\n        cost=cost,\n    )\n</code></pre>"},{"location":"api_reference/triplestore_extend/#tripper.triplestore_extend.Tripper.get_value","title":"<code>get_value(self, iri, routeno=0, unit=None, magnitude=False, quantity=None, **kwargs)</code>","text":"<p>Return the value of an individual.</p> <p>Parameters:</p> Name Type Description Default <code>iri</code> <p>IRI of individual who's value we want to return.  IRI may either refer to a data source or an individual mapped to an ontological concept.</p> required <code>routeno</code> <p>Number identifying the mapping route to apply for retrieving the individual value in case IRI does not refer to a data source.</p> <code>0</code> <code>unit</code> <code>'Optional[str]'</code> <p>return the result in the given unit. Implies <code>magnitude=True</code>.</p> <code>None</code> <code>magnitude</code> <code>bool</code> <p>Whether to only return the magnitude of the evaluated value (with no unit).</p> <code>False</code> <code>quantity</code> <code>'Optional[Any]'</code> <p>Quantity class to use for evaluation.  Defaults to pint.</p> <code>None</code> <code>kwargs</code> <p>Additional arguments passed on to <code>mapping_routes()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>'Value'</code> <p>The value of the individual.</p> Source code in <code>tripper/triplestore_extend.py</code> <pre><code>def get_value(\n    self,\n    iri,\n    routeno=0,\n    unit: \"Optional[str]\" = None,\n    magnitude: bool = False,\n    quantity: \"Optional[Any]\" = None,\n    **kwargs,\n) -&gt; \"Value\":\n    \"\"\"Return the value of an individual.\n\n    Parameters:\n        iri: IRI of individual who's value we want to return.  IRI may\n            either refer to a data source or an individual mapped to\n            an ontological concept.\n        routeno: Number identifying the mapping route to apply for\n            retrieving the individual value in case IRI does not refer\n            to a data source.\n        unit: return the result in the given unit.\n            Implies `magnitude=True`.\n        magnitude: Whether to only return the magnitude of the evaluated\n            value (with no unit).\n        quantity: Quantity class to use for evaluation.  Defaults to pint.\n        kwargs: Additional arguments passed on to `mapping_routes()`.\n\n    Returns:\n        The value of the individual.\n    \"\"\"\n    from tripper.mappings import (  # pylint: disable=import-outside-toplevel\n        Value,\n        mapping_routes,\n    )\n\n    if self.has(iri, RDF.type, DataSource):\n        # `iri` refer to a DataSource\n        if self.has(iri, hasDataValue):  # literal value\n            return Value(\n                value=parse_literal(\n                    self.value(iri, hasDataValue)\n                ).to_python(),\n                unit=(\n                    parse_literal(self.value(iri, hasUnit)).to_python()\n                    if self.has(iri, hasUnit)\n                    else None\n                ),\n                iri=self.value(iri, MAP.mapsTo),\n                cost=(\n                    parse_literal(self.value(iri, hasCost)).to_python()\n                    if self.has(iri, hasCost)\n                    else 0.0\n                ),\n            ).get_value(unit=unit, magnitude=magnitude, quantity=quantity)\n\n        if self.has(iri, hasAccessFunction):  # callable\n            func_iri = self.value(iri, hasAccessFunction)\n            func = self.function_repo[func_iri]\n            assert callable(func)  # nosec\n            retval = func()\n            if isinstance(retval, Value):\n                return retval.get_value(\n                    unit=unit, magnitude=magnitude, quantity=quantity\n                )\n            return retval\n\n        raise TripperError(\n            f\"data source {iri} has neither a 'hasDataValue' or a \"\n            f\"'hasAccessFunction' property\"\n        )\n\n    # `iri` correspond to an individual mapped to an ontological concept.\n    # In this case we check if there exists a mapping route.\n    routes = mapping_routes(\n        target=iri,\n        sources=list(self.subjects(RDF.type, DataSource)),\n        triplestore=self,\n        **kwargs,\n    )\n    if isinstance(routes, Value):\n        return routes.get_value(\n            unit=unit, magnitude=magnitude, quantity=quantity\n        )\n    return routes.eval(\n        routeno=routeno,\n        unit=unit,\n        magnitude=magnitude,\n        quantity=quantity,\n    )\n</code></pre>"},{"location":"api_reference/utils/","title":"utils","text":"<p>Utility functions.</p>"},{"location":"api_reference/utils/#tripper.utils.AttrDict","title":"<code> AttrDict            (dict)         </code>","text":"<p>Dict with attribute access.</p> Source code in <code>tripper/utils.py</code> <pre><code>class AttrDict(dict):\n    \"\"\"Dict with attribute access.\"\"\"\n\n    def __getattr__(self, name):\n        if name in self:\n            return self[name]\n        if name == \"__wrapped__\":\n            # Hack to work around a pytest bug.  During its collection\n            # phase pytest tries to mock namespace objects with an\n            # attribute `__wrapped__`.\n            return None\n        raise KeyError(name)\n\n    def __setattr__(self, name, value):\n        self[name] = value\n\n    def __repr__(self):\n        return self._pprint()\n\n    def __dir__(self):\n        return dict.__dir__(self) + list(self.keys())\n\n    def __getstate__(self):  # For pickle support\n        return dict(self)\n\n    def __setstate__(self, state):  # For pickle support\n        pass\n\n    def __deepcopy__(self, memo):  # For supporting deepcopy\n        return AttrDict((k, deepcopy(v, memo)) for k, v in self.items())\n\n    def _pprint(self, obj=None, indent=0):\n        \"\"\"Help method for pretty printing.\"\"\"\n        if obj is None:\n            obj = self\n        if not obj:\n            return \"AttrDict()\"\n        n = indent + 2\n        s = [\"AttrDict({\"]\n        for k, v in obj.items():\n            val = self._pprint(v, n) if isinstance(v, AttrDict) else repr(v)\n            s.append(f\"{' '*n}{k!r}: {val},\")\n        s.append(\" \" * indent + \"})\")\n        return \"\\n\".join(s)\n\n    def copy(self):\n        \"\"\"Return a shallow copy of self.\"\"\"\n        return AttrDict(self)\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.AttrDict.copy","title":"<code>copy(self)</code>","text":"<p>Return a shallow copy of self.</p> Source code in <code>tripper/utils.py</code> <pre><code>def copy(self):\n    \"\"\"Return a shallow copy of self.\"\"\"\n    return AttrDict(self)\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.as_python","title":"<code>as_python(value)</code>","text":"<p>Converts <code>value</code> to a native Python representation.</p> <p>If <code>value</code> is a Literal, its Python representation will be returned. If <code>value</code> is a string, it will first be converted to a Literal, before its Python representation is returned. Otherwise, <code>value</code> will be returned as-is.</p> Source code in <code>tripper/utils.py</code> <pre><code>def as_python(value: \"Any\") -&gt; \"Any\":\n    \"\"\"Converts `value` to a native Python representation.\n\n    If `value` is a Literal, its Python representation will be returned.\n    If `value` is a string, it will first be converted to a Literal, before\n    its Python representation is returned.\n    Otherwise, `value` will be returned as-is.\n    \"\"\"\n    if isinstance(value, Literal):\n        return value.to_python()\n    if isinstance(value, str):\n        return parse_literal(value).to_python()\n    return value\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.bnode_iri","title":"<code>bnode_iri(prefix='', source='', length=5)</code>","text":"<p>Returns a new IRI for a blank node.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>A prefix to insert between \"_:\" and the hash.</p> <code>''</code> <code>source</code> <code>str</code> <p>An unique string that the returned bnode will be a hash of. The default is to generate a random hash.</p> <code>''</code> <code>length</code> <code>int</code> <p>Length is the number of bytes in the hash.  The default length of 5 is a compromise between readability (10 characters) and safety (corresponding to about 1e12 possibilites).  You can change it to 16 to get 128 bits, corresponding to the uniqueness of UUIDs).  It makes no sense to go beyond 32, because that is the maximum of the underlying shake_128 algorithm.</p> <code>5</code> <p>Returns:</p> Type Description <code>A new bnode IRI of the form \"_</code> <p>\", where <code>&lt;prefix&gt;</code> is <code>prefix</code> and <code>&lt;hash&gt;</code> is a hex-encoded hash of <code>source</code>. Source code in <code>tripper/utils.py</code> <pre><code>def bnode_iri(prefix: str = \"\", source: str = \"\", length: int = 5) -&gt; str:\n    \"\"\"Returns a new IRI for a blank node.\n\n    Parameters:\n        prefix: A prefix to insert between \"_:\" and the hash.\n        source: An unique string that the returned bnode will be a hash of.\n            The default is to generate a random hash.\n        length: Length is the number of bytes in the hash.  The default\n            length of 5 is a compromise between readability (10 characters)\n            and safety (corresponding to about 1e12 possibilites).  You can\n            change it to 16 to get 128 bits, corresponding to the uniqueness\n            of UUIDs).  It makes no sense to go beyond 32, because that is\n            the maximum of the underlying shake_128 algorithm.\n\n    Returns:\n        A new bnode IRI of the form \"_:&lt;prefix&gt;&lt;hash&gt;\", where `&lt;prefix&gt;`\n        is `prefix` and `&lt;hash&gt;` is a hex-encoded hash of `source`.\n    \"\"\"\n    if source:\n        hash = hashlib.shake_128(source.encode()).hexdigest(length)\n    else:\n        # From Python 3.9 we can use random.randbytes(length).hex()\n        hash = \"\".join(\n            hex(random.randint(0, 255))[2:] for i in range(length)  # nosec\n        )\n    return f\"_:{prefix}{hash}\"\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.check_function","title":"<code>check_function(func, s, exceptions)</code>","text":"<p>Help function returning true if <code>func(s)</code> does not raise an exception.</p> <p>False is returned if <code>func(s)</code> raises an exception listed in <code>exceptions</code>. Otherwise the exception is propagated.</p> Source code in <code>tripper/utils.py</code> <pre><code>def check_function(func: \"Callable\", s: str, exceptions) -&gt; bool:\n    \"\"\"Help function returning true if `func(s)` does not raise an exception.\n\n    False is returned if `func(s)` raises an exception listed in `exceptions`.\n    Otherwise the exception is propagated.\n    \"\"\"\n    # Note that the missing type hint on `exceptions` is deliberate, see\n    # https://peps.python.org/pep-0484/#exceptions\n    try:\n        func(s)\n    except exceptions:\n        return False\n    return True\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.check_service_availability","title":"<code>check_service_availability(url, timeout=5, interval=1)</code>","text":"<p>Check whether the service with given URL is available.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the service to check.</p> required <code>timeout</code> <code>float</code> <p>Total time in seconds to wait for a respond.</p> <code>5</code> <code>interval</code> <code>float</code> <p>Internal time interval in seconds between checking if the service has responded.</p> <code>1</code> <p>Returns:</p> Type Description <code>bool</code> <p>Returns true if the service responds with code 200, otherwise false is returned.</p> Source code in <code>tripper/utils.py</code> <pre><code>def check_service_availability(\n    url: str, timeout: float = 5, interval: float = 1\n) -&gt; bool:\n    \"\"\"Check whether the service with given URL is available.\n\n    Arguments:\n        url: URL of the service to check.\n        timeout: Total time in seconds to wait for a respond.\n        interval: Internal time interval in seconds between checking if the\n            service has responded.\n\n    Returns:\n        Returns true if the service responds with code 200,\n        otherwise false is returned.\n    \"\"\"\n    import time  # pylint: disable=import-outside-toplevel\n\n    import requests  # pylint: disable=import-outside-toplevel\n\n    # Interval should never be larger than timeout\n    interval = min(interval, timeout)\n\n    start_time = time.time()\n\n    while True:\n        try:\n            response = requests.get(url, timeout=timeout)\n            if response.status_code == 200:\n                return True\n        except requests.exceptions.RequestException:\n            pass\n\n        if time.time() - start_time &gt;= timeout:\n            return False\n        time.sleep(interval)\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.en","title":"<code>en(value)</code>","text":"<p>Convenience function that returns value as a plain english literal.</p> <p>Equivalent to <code>Literal(value, lang=\"en\")</code>.</p> Source code in <code>tripper/utils.py</code> <pre><code>def en(value) -&gt; \"Literal\":  # pylint: disable=invalid-name\n    \"\"\"Convenience function that returns value as a plain english literal.\n\n    Equivalent to ``Literal(value, lang=\"en\")``.\n    \"\"\"\n    return Literal(value, lang=\"en\")\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.expand_iri","title":"<code>expand_iri(iri, prefixes, strict=False)</code>","text":"<p>Return the full IRI if <code>iri</code> is prefixed.  Otherwise <code>iri</code> is returned.</p> Source code in <code>tripper/utils.py</code> <pre><code>def expand_iri(iri: str, prefixes: dict, strict: bool = False) -&gt; str:\n    \"\"\"Return the full IRI if `iri` is prefixed.  Otherwise `iri` is\n    returned.\"\"\"\n    match = re.match(MATCH_PREFIXED_IRI, iri)\n    if match:\n        prefix, name, _ = match.groups()\n        if prefix in prefixes:\n            return f\"{prefixes[prefix]}{name}\"\n        if strict:\n            raise NamespaceError(f'Undefined prefix \"{prefix}\" in IRI: {iri}')\n        # warnings.warn(f'Undefined prefix \"{prefix}\" in IRI: {iri}')\n    return iri\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.extend_namespace","title":"<code>extend_namespace(namespace, triplestore, format=None)</code>","text":"<p>Extend a namespace with additional known names.</p> <p>This makes only sense if the namespace was created with <code>label_annotations</code> or <code>check</code> set to true.</p> <p>Parameters:</p> Name Type Description Default <code>namespace</code> <code>Namespace</code> <p>The namespace to extend.</p> required <code>triplestore</code> <code>Union[Triplestore, str, Path, dict]</code> <p>Source from which to extend the namespace. It can be of one of the following types:   - Triplestore: triplestore object to read from   - str: URL to a triplestore to read from.  May also be a     path to a local file to read from   - Path: path to a local file to read from   - dict: dict mapping new IRI names to their corresponding IRIs</p> required <code>format</code> <code>Optional[str]</code> <p>Format to use when loading from a triplestore.</p> <code>None</code> Source code in <code>tripper/utils.py</code> <pre><code>def extend_namespace(\n    namespace: Namespace,\n    triplestore: \"Union[Triplestore, str, Path, dict]\",\n    format: \"Optional[str]\" = None,\n):\n    \"\"\"Extend a namespace with additional known names.\n\n    This makes only sense if the namespace was created with\n    `label_annotations` or `check` set to true.\n\n    Arguments:\n        namespace: The namespace to extend.\n        triplestore: Source from which to extend the namespace. It can be\n            of one of the following types:\n              - Triplestore: triplestore object to read from\n              - str: URL to a triplestore to read from.  May also be a\n                path to a local file to read from\n              - Path: path to a local file to read from\n              - dict: dict mapping new IRI names to their corresponding IRIs\n        format: Format to use when loading from a triplestore.\n    \"\"\"\n    if namespace._iris is None:  # pylint: disable=protected-access\n        raise TypeError(\n            \"only namespaces created with `label_annotations` or `check` set \"\n            \"to true can be extend\"\n        )\n    if isinstance(triplestore, dict):\n        namespace._iris.update(triplestore)  # pylint: disable=protected-access\n    else:\n        namespace._update_iris(  # pylint: disable=protected-access\n            triplestore, reload=True, format=format\n        )\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.function_id","title":"<code>function_id(func, length=4)</code>","text":"<p>Return a checksum for function <code>func</code>.</p> <p>The returned object is a string of hexadecimal digits.</p> <p><code>length</code> is the number of bytes in the returned checksum.  Since the current implementation is based on the shake_128 algorithm, it make no sense to set <code>length</code> larger than 32 bytes.</p> Source code in <code>tripper/utils.py</code> <pre><code>def function_id(func: \"Callable\", length: int = 4) -&gt; str:\n    \"\"\"Return a checksum for function `func`.\n\n    The returned object is a string of hexadecimal digits.\n\n    `length` is the number of bytes in the returned checksum.  Since\n    the current implementation is based on the shake_128 algorithm,\n    it make no sense to set `length` larger than 32 bytes.\n    \"\"\"\n    source = inspect.getsource(func)\n    doc = func.__doc__ if func.__doc__ else \"\"\n    return hashlib.shake_128(  # pylint: disable=too-many-function-args\n        (source + doc).encode()\n    ).hexdigest(length)\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.get_entry_points","title":"<code>get_entry_points(group)</code>","text":"<p>Consistent interface to entry points for the given group.</p> <p>Works for all supported versions of Python.</p> <p>Examples:</p> <p>get_entry_points(\"tripper.backends\")  # doctest: +SKIP [     EntryPoint(         name='fuseki',         value='pybacktrip.backends.fuseki',         group='tripper.backends',     ),     EntryPoint(         name='stardog',         value='pybacktrip.backends.stardog',         group='tripper.backends',    ),    ... ]</p> Source code in <code>tripper/utils.py</code> <pre><code>def get_entry_points(group: str):\n    \"\"\"Consistent interface to entry points for the given group.\n\n    Works for all supported versions of Python.\n\n    Examples:\n\n    &gt;&gt;&gt; get_entry_points(\"tripper.backends\")  # doctest: +SKIP\n    [\n        EntryPoint(\n            name='fuseki',\n            value='pybacktrip.backends.fuseki',\n            group='tripper.backends',\n        ),\n        EntryPoint(\n            name='stardog',\n            value='pybacktrip.backends.stardog',\n            group='tripper.backends',\n       ),\n       ...\n    ]\n\n    \"\"\"\n    if sys.version_info &lt; (3, 10):\n        # Fallback for Python &lt; 3.10\n        eps = entry_points().get(group, ())  # pylint: disable=no-member\n    else:\n        # New entry_point interface from Python 3.10+\n        eps = entry_points(  # pylint: disable=unexpected-keyword-arg\n            group=group\n        )\n    return eps\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.infer_iri","title":"<code>infer_iri(obj)</code>","text":"<p>Return IRI of the individual that stands for Python object <code>obj</code>.</p> <p>Valid Python objects are DLite and Pydantic instances.</p> <p>References:</p> Source code in <code>tripper/utils.py</code> <pre><code>def infer_iri(obj):\n    \"\"\"Return IRI of the individual that stands for Python object `obj`.\n\n    Valid Python objects are [DLite] and [Pydantic] instances.\n\n    References:\n\n    [DLite]: https://github.com/SINTEF/dlite\n    [Pydantic]: https://docs.pydantic.dev/\n    \"\"\"\n\n    # Please note that tripper does not depend on neither DLite nor Pydantic.\n    # Hence neither of these packages are imported.  However, due to duck-\n    # typing, infer_iri() is still able to recognise DLite and Pydantic\n    # objects and infer their IRIs.\n\n    if isinstance(obj, str):\n        iri = obj\n    elif hasattr(obj, \"uri\") and isinstance(obj.uri, str):\n        # dlite.Metadata or dataclass (or instance with uri)\n        iri = obj.uri\n    elif hasattr(obj, \"uuid\") and obj.uuid:\n        # dlite.Instance or dataclass\n        iri = str(obj.uuid)\n    elif hasattr(obj, \"schema\") and callable(obj.schema):\n        # pydantic.BaseModel\n        if hasattr(obj, \"identity\") and isinstance(obj.identity, str):\n            # soft7 pydantic model\n            iri = obj.identity\n        else:\n            # pydantic instance\n            schema = obj.schema()\n            properties = schema[\"properties\"]\n            if \"uri\" in properties and isinstance(properties[\"uri\"], str):\n                iri = properties[\"uri\"]\n            elif \"identity\" in properties and isinstance(\n                properties[\"identity\"], str\n            ):\n                iri = properties[\"identity\"]\n            elif \"uuid\" in properties and properties[\"uuid\"]:\n                iri = str(properties[\"uuid\"])\n            else:\n                raise TypeError(\n                    f\"cannot infer IRI from pydantic object: {obj!r}\"\n                )\n    else:\n        raise TypeError(f\"cannot infer IRI from object: {obj!r}\")\n    return str(iri)\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.is_curie","title":"<code>is_curie(curie, exclude_netloc=True)</code>","text":"<p>Returns whether <code>curie</code> is a CURIE (compact URI).</p> <p>Parameters:</p> Name Type Description Default <code>curie</code> <code>str</code> <p>CURIE to validate.</p> required <code>exclude_netloc</code> <p>Whether to exclude CURIEs with two slashes following the first colon. If true, the part before the colon should also correspond to a valid URI schema name.</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; is_curie(\"http://example.com\")\nFalse\n&gt;&gt;&gt; is_curie(\"rdf:type\")\nTrue\n</code></pre> Source code in <code>tripper/utils.py</code> <pre><code>def is_curie(curie: str, exclude_netloc=True) -&gt; bool:\n    \"\"\"Returns whether `curie` is a CURIE (compact URI).\n\n    Arguments:\n        curie: CURIE to validate.\n        exclude_netloc: Whether to exclude CURIEs with two slashes following\n            the first colon. If true, the part before the colon should also\n            correspond to a valid URI schema name.\n\n    Example:\n\n        &gt;&gt;&gt; is_curie(\"http://example.com\")\n        False\n        &gt;&gt;&gt; is_curie(\"rdf:type\")\n        True\n    \"\"\"\n    if len(curie) &lt; 3:\n        return False\n    if curie[0] == \"[\" and curie[-1] == \"]\":\n        curie = curie[1:-1]\n\n    if not exclude_netloc:\n        raise NotImplementedError(\n            \"is_curie() with argument `exclude_netloc=False` isn't implemented\"\n        )\n    return bool(re.match(r\"^[a-zA-Z][a-zA-Z0-9+.-]*:(?!//)\\S*$\", curie))\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.is_uri","title":"<code>is_uri(uri, require_netloc=True, allow_unescaped=False, safe='%:~/?&amp;;=#')</code>","text":"<p>Returns true if <code>uri</code> is a valid URI, otherwise false.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>URI to validate.</p> required <code>require_netloc</code> <code>bool</code> <p>Whether to require <code>uri</code> to contain a network location. Setting this to true will exclude URNs (although they are valid URIs). However, in most practical cases, you would expect the URI to contain a network location.</p> <code>True</code> <code>allow_unescaped</code> <code>bool</code> <p>Whether to allow <code>uri</code> to contain unescaped special characters. Any character not in <code>safe</code> except for letters, digits and '_.-' are considered to be special. Escaping is expected to use standard URI %xx escape codes.</p> <code>False</code> <code>safe</code> <code>str</code> <p>Characters in addition to '_.-' that doesn't need to be escaped when <code>allow_unescaped</code> is false.</p> <code>'%:~/?&amp;;=#'</code> Source code in <code>tripper/utils.py</code> <pre><code>def is_uri(\n    uri: str,\n    require_netloc: bool = True,\n    allow_unescaped: bool = False,\n    safe: str = \"%:~/?&amp;;=#\",\n):\n    \"\"\"Returns true if `uri` is a valid URI, otherwise false.\n\n    Arguments:\n        uri: URI to validate.\n        require_netloc: Whether to require `uri` to contain a network location.\n            Setting this to true will exclude URNs (although they are valid\n            URIs). However, in most practical cases, you would expect the URI\n            to contain a network location.\n        allow_unescaped: Whether to allow `uri` to contain unescaped special\n            characters. Any character not in `safe` except for letters, digits\n            and '_.-' are considered to be special.\n            Escaping is expected to use standard URI %xx escape codes.\n        safe: Characters in addition to '_.-' that doesn't need to be\n            escaped when `allow_unescaped` is false.\n    \"\"\"\n    if not allow_unescaped and urllib.parse.quote(uri, safe=safe) != uri:\n        return False\n    try:\n        result = urllib.parse.urlparse(uri)\n    except ValueError:\n        return False\n    return bool(result.scheme) and (not require_netloc or bool(result.netloc))\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.openfile","title":"<code>openfile(url, mode='rt', timeout=3, **kwargs)</code>","text":"<p>Like open(), but allows opening remote files using HTTP GET requests.</p> <p>Should always be used in a with-statement.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Union[str, Path, IO]</code> <p>File path, URL or stream to open.</p> required <code>mode</code> <code>str</code> <p>See <code>mode</code> argument of open().</p> <code>'rt'</code> <code>timeout</code> <code>float</code> <p>Timeout for accessing the file in seconds.</p> <code>3</code> <code>kwargs</code> <p>Additional passed to open().</p> <code>{}</code> <p>Yields:</p> Type Description <code>Iterator[IO]</code> <p>A stream object returned by open().</p> Source code in <code>tripper/utils.py</code> <pre><code>@contextmanager\ndef openfile(\n    url: \"Union[str, Path, IO]\", mode: str = \"rt\", timeout: float = 3, **kwargs\n) -&gt; \"Iterator[IO]\":\n    \"\"\"Like open(), but allows opening remote files using HTTP GET requests.\n\n    Should always be used in a with-statement.\n\n    Arguments:\n        url: File path, URL or stream to open.\n        mode: See `mode` argument of open().\n        timeout: Timeout for accessing the file in seconds.\n        kwargs: Additional passed to open().\n\n    Yields:\n        A stream object returned by open().\n\n    \"\"\"\n    if isinstance(url, (IO, io.IOBase)):\n        yield url\n        return\n\n    url = str(url)\n    u = url.lower()\n    tmpfile = False\n    f = None\n\n    if u.startswith(\"file:\"):\n        fname = url[7:] if u.startswith(\"file://\") else url[5:]\n\n    elif u.startswith(\"http://\") or u.startswith(\"https://\"):\n        import requests  # pylint: disable=import-outside-toplevel\n\n        tmpfile = True\n        r = requests.get(url, timeout=timeout)\n        r.raise_for_status()\n        with tempfile.NamedTemporaryFile(delete=False) as f:\n            fname = f.name\n            f.write(r.content)\n\n    elif re.match(r\"[a-zA-Z][a-zA-Z0-9+.-]*://\", url):\n        raise IOError(f\"unknown scheme: {url.split(':', 1)[0]}\")\n\n    else:\n        fname = url\n\n    try:\n        # pylint: disable=unspecified-encoding\n        f = open(fname, mode=mode, **kwargs)  # type: ignore\n        yield f  # type: ignore\n    finally:\n        if f is not None:\n            f.close()\n        if tmpfile:\n            Path(fname).unlink()\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.parse_literal","title":"<code>parse_literal(literal)</code>","text":"<p>Parse Python object <code>literal</code> and return it as an instance of Literal.</p> <p>The main difference between this function and the Literal constructor, is that this function correctly interprets n3-encoded literal strings.</p> Source code in <code>tripper/utils.py</code> <pre><code>def parse_literal(literal: \"Any\") -&gt; \"Any\":\n    \"\"\"Parse Python object `literal` and return it as an instance of Literal.\n\n    The main difference between this function and the Literal constructor,\n    is that this function correctly interprets n3-encoded literal strings.\n    \"\"\"\n    # pylint: disable=invalid-name,too-many-branches,too-many-statements\n    # pylint: disable=too-many-return-statements\n    lang, datatype = None, None\n\n    if isinstance(literal, Literal):\n        return literal\n\n    if hasattr(literal, \"lang\"):\n        lang = literal.lang\n    elif hasattr(literal, \"language\"):\n        lang = literal.language\n\n    if (\n        not lang\n        and hasattr(literal, \"datatype\")\n        and literal.datatype is not None\n    ):\n        datatype = str(literal.datatype)\n\n    # This should handle rdflib literals correctly (and probably most other\n    # literal representations as well)\n    if hasattr(literal, \"value\"):\n        # Note that in rdflib 6.3, the `value` attribute may be None for some\n        # datatypes (like rdf:JSON) even though a non-empty value exists.\n        # As a workaround, we use the string representation if the value\n        # attribute is None.\n        if literal.value is not None:\n            return Literal(literal.value, lang=lang, datatype=datatype)\n        return Literal(str(literal), lang=lang, datatype=datatype)\n\n    if not isinstance(literal, str):\n        if isinstance(literal, tuple(Literal.datatypes)):\n            if type(literal) in Literal.datatypes:\n                datatype = Literal.datatypes[type(literal)][0]\n            else:\n                # literal is in instance of a subclass of one of the datatypes\n                for k, v in Literal.datatypes.items():\n                    if isinstance(literal, k):\n                        datatype = v[0]\n                        break\n                else:\n                    assert False, \"should never be reached\"  # nosec\n            return Literal(literal, lang=lang, datatype=datatype)\n        raise TypeError(f\"unsupported literal type: {type(literal)}\")\n\n    if hasattr(literal, \"n3\") and callable(literal.n3):\n        return parse_literal(literal.n3())\n\n    match = re.match(r'^\\s*(\"\"\"(.*)\"\"\"|\"(.*)\")\\s*$', literal, flags=re.DOTALL)\n    if match:\n        _, v1, v2 = match.groups()\n        value, datatype = v1 if v1 else v2, XSD.string\n    else:\n        match = re.match(\n            r'^\\s*(\"\"\"(.*)\"\"\"|\"(.*)\")\\^\\^(&lt;([^&gt;]+)&gt;|([^&lt;].*))\\s*$',\n            literal,\n            flags=re.DOTALL,\n        )\n        if match:\n            _, v1, v2, _, d1, d2 = match.groups()\n            value = v1 if v1 else v2\n            datatype = d1 if d1 else d2\n        else:\n            match = re.match(\n                r'^\\s*(\"\"\"(.*)\"\"\"|\"(.*)\")@(.*)\\s*$', literal, flags=re.DOTALL\n            )\n            if match:\n                _, v1, v2, lang = match.groups()\n                value = v1 if v1 else v2\n            else:\n                value = literal\n\n    if lang or datatype:\n        if datatype:\n            types = {}\n            for pytype, datatypes in Literal.datatypes.items():\n                types.update({t: pytype for t in datatypes})\n            type_ = types.get(datatype, str)\n            if type_ is bool and value in (\"False\", \"false\", \"0\", 0, False):\n                return Literal(False)\n            try:\n                value = type_(value)\n            except TypeError:\n                pass\n        return Literal(value, lang=lang, datatype=datatype)\n\n    for type_, datatypes in Literal.datatypes.items():\n        if type_ is not bool:\n            try:\n                return Literal(\n                    type_(literal), lang=lang, datatype=datatypes[0]\n                )\n            except (ValueError, TypeError):\n                pass\n\n    raise ValueError(f'cannot parse literal \"{literal}\"')\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.parse_object","title":"<code>parse_object(obj)</code>","text":"<p>Applies heuristics to parse RDF object <code>obj</code> to an IRI or literal.</p> <p>The following heuristics is performed (in the given order): - If <code>obj</code> is a Literal, it is returned. - If <code>obj</code> is a string and   - starts with \"_:\", it is assumed to be a blank node and returned.   - starts with a scheme, it is asumed to be an IRI and returned.   - can be converted to a float, int or datetime, it is returned     converted to a literal of the corresponding type.   - it is a valid n3 representation, return it as the given type.   - otherwise, return it as a xsd:string literal. - Otherwise, raise an ValueError.</p> <p>Returns     A string if <code>obj</code> is considered to be an IRI, otherwise a     literal.</p> Source code in <code>tripper/utils.py</code> <pre><code>def parse_object(obj: \"Union[str, Literal]\") -&gt; \"Union[str, Any]\":\n    \"\"\"Applies heuristics to parse RDF object `obj` to an IRI or literal.\n\n    The following heuristics is performed (in the given order):\n    - If `obj` is a Literal, it is returned.\n    - If `obj` is a string and\n      - starts with \"_:\", it is assumed to be a blank node and returned.\n      - starts with a scheme, it is asumed to be an IRI and returned.\n      - can be converted to a float, int or datetime, it is returned\n        converted to a literal of the corresponding type.\n      - it is a valid n3 representation, return it as the given type.\n      - otherwise, return it as a xsd:string literal.\n    - Otherwise, raise an ValueError.\n\n    Returns\n        A string if `obj` is considered to be an IRI, otherwise a\n        literal.\n    \"\"\"\n    # pylint: disable=too-many-return-statements\n    if isinstance(obj, Literal):\n        return obj\n    if isinstance(obj, str):\n        if obj.startswith(\"_:\") or re.match(r\"^[a-z]+://\", obj):  # IRI\n            return obj\n        if obj in (\"true\", \"false\"):  # boolean\n            return Literal(obj, datatype=XSD.boolean)\n        if re.match(r\"^\\s*[+-]?\\d+\\s*$\", obj):  # integer\n            return Literal(obj, datatype=XSD.integer)\n        if check_function(float, obj, ValueError):  #  float\n            return Literal(obj, datatype=XSD.double)\n        if check_function(\n            datetime.datetime.fromisoformat, obj, ValueError\n        ):  #  datetime\n            return Literal(obj, datatype=XSD.dateTime)\n        return parse_literal(obj)\n    raise ValueError(\"`obj` should be a literal or a string.\")\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.prefix_iri","title":"<code>prefix_iri(iri, prefixes, strict=False)</code>","text":"<p>Return prefixed IRI.</p> <p>This is the reverse of expand_iri().</p> <p>If <code>strict</code> is true, a NamespaceError exception is raised if no prefix can be found.</p> Source code in <code>tripper/utils.py</code> <pre><code>def prefix_iri(iri: str, prefixes: dict, strict: bool = False) -&gt; str:\n    \"\"\"Return prefixed IRI.\n\n    This is the reverse of expand_iri().\n\n    If `strict` is true, a NamespaceError exception is raised\n    if no prefix can be found.\n\n    \"\"\"\n    if not re.match(MATCH_PREFIXED_IRI, iri):\n        for prefix, ns in prefixes.items():\n            if iri.startswith(str(ns)):\n                return f\"{prefix}:{iri[len(str(ns)):]}\"\n        if strict:\n            raise NamespaceError(f\"No prefix defined for IRI: {iri}\")\n    return iri\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.random_string","title":"<code>random_string(length=8)</code>","text":"<p>Return a random string of the given length.</p> Source code in <code>tripper/utils.py</code> <pre><code>def random_string(length=8):\n    \"\"\"Return a random string of the given length.\"\"\"\n    letters = string.ascii_letters + string.digits\n    return \"\".join(random.choice(letters) for i in range(length))  # nosec\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.recursive_update","title":"<code>recursive_update(d, other, append=True, cls=None)</code>","text":"<p>Recursively update dict <code>d</code> with dict <code>other</code>.</p> <p>Parameters:</p> Name Type Description Default <code>d</code> <code>dict</code> <p>Dict to update.</p> required <code>other</code> <code>Union[dict, List[Union[dict, list]]]</code> <p>The source to update <code>d</code> from.</p> required <code>append</code> <code>bool</code> <p>If <code>append</code> is true and <code>other</code> has a key that also exists in <code>d</code>, then the value in <code>d</code> will be converted to a list with the value from <code>other</code> appended to it. If <code>append</code> is false, the values in <code>d</code> will be replaced by corresponding values in <code>other</code>.</p> <code>True</code> <code>cls</code> <code>Optional[type]</code> <p>Dict subclass for new sub-dicts in <code>d</code>. Defaults to the class of <code>d</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {\"a\": 1}\n&gt;&gt;&gt; recursive_update(d, {\"a\": 2})\n&gt;&gt;&gt; d\n{'a': [1, 2]}\n</code></pre> <pre><code>&gt;&gt;&gt; d = {\"a\": 1}\n&gt;&gt;&gt; recursive_update(d, {\"a\": 2}, append=False)\n&gt;&gt;&gt; d\n{'a': 2}\n</code></pre> Source code in <code>tripper/utils.py</code> <pre><code>def recursive_update(\n    d: dict,\n    other: \"Union[dict, List[Union[dict, list]]]\",\n    append: bool = True,\n    cls: \"Optional[type]\" = None,\n):\n    \"\"\"Recursively update dict `d` with dict `other`.\n\n    Arguments:\n        d: Dict to update.\n        other: The source to update `d` from.\n        append: If `append` is true and `other` has a key that also exists\n            in `d`, then the value in `d` will be converted to a list with\n            the value from `other` appended to it.\n            If `append` is false, the values in `d` will be replaced by\n            corresponding values in `other`.\n        cls: Dict subclass for new sub-dicts in `d`. Defaults to the class\n            of `d`.\n\n    Example:\n\n        &gt;&gt;&gt; d = {\"a\": 1}\n        &gt;&gt;&gt; recursive_update(d, {\"a\": 2})\n        &gt;&gt;&gt; d\n        {'a': [1, 2]}\n\n        &gt;&gt;&gt; d = {\"a\": 1}\n        &gt;&gt;&gt; recursive_update(d, {\"a\": 2}, append=False)\n        &gt;&gt;&gt; d\n        {'a': 2}\n\n    \"\"\"\n    # pylint: disable=too-many-branches\n    if cls is None:\n        cls = d.__class__\n\n    new = _rec(d, other, append=append, cls=cls)\n    d.update(new)\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.split_iri","title":"<code>split_iri(iri)</code>","text":"<p>Split iri into namespace and name parts and return them as a tuple.</p> <p>Parameters:</p> Name Type Description Default <code>iri</code> <code>str</code> <p>The IRI to be split.</p> required <p>Returns:</p> Type Description <code>Tuple[str, str]</code> <p>A split IRI. Split into namespace and name.</p> Source code in <code>tripper/utils.py</code> <pre><code>def split_iri(iri: str) -&gt; \"Tuple[str, str]\":\n    \"\"\"Split iri into namespace and name parts and return them as a tuple.\n\n    Parameters:\n        iri: The IRI to be split.\n\n    Returns:\n        A split IRI. Split into namespace and name.\n\n    \"\"\"\n    if \"#\" in iri:\n        namespace, name = iri.rsplit(\"#\", 1)\n        return f\"{namespace}#\", name\n\n    if \"/\" in iri:\n        namespace, name = iri.rsplit(\"/\", 1)\n        return f\"{namespace}/\", name\n\n    raise ValueError(\"all IRIs should contain a slash\")\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.substitute_query","title":"<code>substitute_query(query, iris=None, literals=None, prefixes=None, iriquote='&lt;&gt;')</code>","text":"<p>Substitute IRI and literal variables in a SPARQL query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>String with the SPARQL query.</p> required <code>iris</code> <code>Optional[dict]</code> <p>Dict used for query substitutions that maps IRI variables to IRIs. The IRIs may be provided as fully expanded or prefixed with the prefix defined in <code>prefixes</code>.</p> <code>None</code> <code>literals</code> <code>Optional[dict]</code> <p>Dict used for query substitutions that maps literal variables to literals.  For common datatypes, like strings and numbers, the values can just be normal Python objects. For special cases or more control, provide the values as instances of <code>tripper.Literal</code>.</p> <code>None</code> <code>prefixes</code> <code>Optional[dict]</code> <p>Dict mapping prefixes to namespace URLs.</p> <code>None</code> <code>iriquote</code> <code>str</code> <p>Quote characters to use for IRIs. Should be a string of length 2, with the start and end quote.</p> <code>'&lt;&gt;'</code> <p>Notes</p> <p>The <code>query</code> argument may contain variables for IRIs and literals, to be substituted using the <code>iris</code> and <code>literals</code> arguments. These variables are prefixed <code>$</code>. This makes them easy to distinguish from query variables, that are typically prefixed with <code>?</code>.</p> <p>The query substitutions may be useful when the query is constructed from user input, since they are properly escaped and will be inserted in the query as a single token.  This may prevent sparql injection attacks.</p> Source code in <code>tripper/utils.py</code> <pre><code>def substitute_query(\n    query: str,\n    iris: \"Optional[dict]\" = None,\n    literals: \"Optional[dict]\" = None,\n    prefixes: \"Optional[dict]\" = None,\n    iriquote: str = \"&lt;&gt;\",\n) -&gt; str:\n    \"\"\"Substitute IRI and literal variables in a SPARQL query.\n\n    Arguments:\n        query: String with the SPARQL query.\n        iris: Dict used for query substitutions that maps IRI variables\n            to IRIs. The IRIs may be provided as fully expanded or\n            prefixed with the prefix defined in `prefixes`.\n        literals: Dict used for query substitutions that maps literal\n            variables to literals.  For common datatypes, like strings\n            and numbers, the values can just be normal Python objects.\n            For special cases or more control, provide the values as\n            instances of `tripper.Literal`.\n        prefixes: Dict mapping prefixes to namespace URLs.\n        iriquote: Quote characters to use for IRIs. Should be a string of\n            length 2, with the start and end quote.\n\n    Notes:\n        The `query` argument may contain variables for IRIs and literals,\n        to be substituted using the `iris` and `literals` arguments. These\n        variables are prefixed `$`. This makes them easy to distinguish from\n        query variables, that are typically prefixed with `?`.\n\n        The query substitutions may be useful when the query is constructed\n        from user input, since they are properly escaped and will be inserted\n        in the query as a single token.  This may prevent sparql injection\n        attacks.\n    \"\"\"\n    safe = \"-._~:/?#@+&amp;;=\"  # special IRI characters that are not escaped\n    mapping = {}\n\n    if iriquote:\n        if len(iriquote) == 1:\n            iriquote = iriquote[0] * 2\n        elif len(iriquote) &gt; 2:\n            raise ValueError(\n                f\"`iriquote` cannot be more than 2 characters: '{iriquote}'\"\n            )\n        if iriquote[1].isalnum() or iriquote[1] in safe:\n            warnings.warn(\n                f\"End quote '{iriquote[1]}' is alphanumeric or in '{safe}'\"\n            )\n\n    if iris:\n        if prefixes is None:\n            prefixes = {}\n        for k, v in iris.items():\n            expanded = expand_iri(v, prefixes=prefixes)\n            quoted = urllib.parse.quote(expanded, safe=safe)\n            q1, q2 = iriquote if iriquote else (\"\", \"\")  # type: ignore[misc]\n            mapping[k] = f\"{q1}{quoted}{q2}\"\n\n    if literals:\n        for k, v in literals.items():\n            mapping[k] = Literal(v).n3()\n\n    return string.Template(query).safe_substitute(mapping)\n</code></pre>"},{"location":"api_reference/utils/#tripper.utils.tfilter","title":"<code>tfilter(triples, subject=None, predicate=None, object=None)</code>","text":"<p>Filters out non-matching triples.</p> <p>Parameters:</p> Name Type Description Default <code>triples</code> <code>Iterable[Triple]</code> <p>Triples to filter from.</p> required <code>subject</code> <code>Optional[Union[Iterable[str], str]]</code> <p>If given, only keep triples whos subject matches <code>subject</code>. Can be an iterable of subjects.</p> <code>None</code> <code>predicate</code> <code>Optional[Union[Iterable[str], str]]</code> <p>If given, only keep triples whos predicate matches <code>predicate</code>.  Can be an iterable of subjects.</p> <code>None</code> <code>object</code> <code>Optional[Union[Iterable, str, Literal]]</code> <p>If given, only keep triples whos subject matches <code>object</code>. Can be an iterable of objects.</p> <code>None</code> <p>Returns:</p> Type Description <code>Generator[Triple, None, None]</code> <p>A generator over matching triples.</p> Source code in <code>tripper/utils.py</code> <pre><code>def tfilter(\n    triples: \"Iterable[Triple]\",\n    subject: \"Optional[Union[Iterable[str], str]]\" = None,\n    predicate: \"Optional[Union[Iterable[str], str]]\" = None,\n    object: \"Optional[Union[Iterable, str, Literal]]\" = None,\n) -&gt; \"Generator[Triple, None, None]\":\n    \"\"\"Filters out non-matching triples.\n\n    Parameters:\n        triples: Triples to filter from.\n        subject: If given, only keep triples whos subject matches `subject`.\n            Can be an iterable of subjects.\n        predicate: If given, only keep triples whos predicate matches\n            `predicate`.  Can be an iterable of subjects.\n        object: If given, only keep triples whos subject matches `object`.\n            Can be an iterable of objects.\n\n    Returns:\n        A generator over matching triples.\n    \"\"\"\n    for s, p, o in triples:\n        if subject and (\n            s != subject if isinstance(subject, str) else s not in subject\n        ):\n            continue\n        if predicate and (\n            p != predicate\n            if isinstance(predicate, str)\n            else p not in predicate\n        ):\n            continue\n        if object and (\n            o != object\n            if isinstance(object, (str, Literal))\n            else o not in object\n        ):\n            continue\n        yield s, p, o\n</code></pre>"},{"location":"api_reference/backends/collection/","title":"collection","text":"<p>Backend for DLite collections.</p>"},{"location":"api_reference/backends/collection/#tripper.backends.collection.CollectionStrategy","title":"<code> CollectionStrategy        </code>","text":"<p>Triplestore strategy for DLite collections.</p> <p>Parameters:</p> Name Type Description Default <code>base_iri</code> <code>Optional[str]</code> <p>Unused.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Unused - collection does not support multiple databases.</p> <code>None</code> <code>collection</code> <code>Optional[Union[dlite.Collection, str]]</code> <p>Optional collection from which to initialise the triplestore from.</p> <code>None</code> Source code in <code>tripper/backends/collection.py</code> <pre><code>class CollectionStrategy:\n    \"\"\"Triplestore strategy for DLite collections.\n\n    Arguments:\n        base_iri: Unused.\n        database: Unused - collection does not support multiple databases.\n        collection: Optional collection from which to initialise the\n            triplestore from.\n    \"\"\"\n\n    prefer_sparql = False\n\n    def __init__(\n        self,\n        base_iri: \"Optional[str]\" = None,\n        database: \"Optional[str]\" = None,\n        collection: \"Optional[Union[dlite.Collection, str]]\" = None,\n    ):\n        # pylint: disable=unused-argument,import-outside-toplevel\n        import dlite\n\n        if collection is None:\n            self.collection = dlite.Collection()\n        elif isinstance(collection, str):\n            self.collection = dlite.get_instance(collection)\n            if self.collection.meta.uri != dlite.COLLECTION_ENTITY:\n                raise TypeError(\n                    f\"expected '{collection}' to be a collection, was a \"\n                    f\"{self.collection.meta.uri}\"\n                )\n        elif isinstance(collection, dlite.Collection):\n            self.collection = collection\n        else:\n            raise TypeError(\n                \"`collection` should be None, string or a collection\"\n            )\n\n    def triples(self, triple: \"Triple\") -&gt; \"Generator[Triple, None, None]\":\n        \"\"\"Returns a generator over matching triples.\"\"\"\n        for s, p, o, d in self.collection.get_relations(*triple, rettype=\"T\"):\n            if d:\n                lang = d[1:] if d[0] == \"@\" else None\n                dt = None if lang else d\n                yield s, p, Literal(o, lang=lang, datatype=dt)\n            else:\n                yield s, p, o\n\n    def add_triples(\n        self, triples: \"Union[Sequence[Triple], Generator[Triple, None, None]]\"\n    ):\n        \"\"\"Add a sequence of triples.\"\"\"\n        for s, p, o in triples:\n            v = parse_object(o)\n            obj = v if isinstance(v, str) else str(v.value)\n            d = (\n                None\n                if not isinstance(v, Literal)\n                else f\"@{v.lang}\" if v.lang else v.datatype\n            )\n            self.collection.add_relation(s, p, obj, d)\n\n    def remove(self, triple: \"Triple\"):\n        \"\"\"Remove all matching triples from the backend.\"\"\"\n        s, p, o = triple\n        v = parse_object(o)\n        obj = v if isinstance(v, str) else str(v.value)\n        d = (\n            None\n            if not isinstance(v, Literal)\n            else f\"@{v.lang}\" if v.lang else v.datatype\n        )\n        self.collection.remove_relations(s, p, obj, d)\n</code></pre>"},{"location":"api_reference/backends/collection/#tripper.backends.collection.CollectionStrategy.add_triples","title":"<code>add_triples(self, triples)</code>","text":"<p>Add a sequence of triples.</p> Source code in <code>tripper/backends/collection.py</code> <pre><code>def add_triples(\n    self, triples: \"Union[Sequence[Triple], Generator[Triple, None, None]]\"\n):\n    \"\"\"Add a sequence of triples.\"\"\"\n    for s, p, o in triples:\n        v = parse_object(o)\n        obj = v if isinstance(v, str) else str(v.value)\n        d = (\n            None\n            if not isinstance(v, Literal)\n            else f\"@{v.lang}\" if v.lang else v.datatype\n        )\n        self.collection.add_relation(s, p, obj, d)\n</code></pre>"},{"location":"api_reference/backends/collection/#tripper.backends.collection.CollectionStrategy.remove","title":"<code>remove(self, triple)</code>","text":"<p>Remove all matching triples from the backend.</p> Source code in <code>tripper/backends/collection.py</code> <pre><code>def remove(self, triple: \"Triple\"):\n    \"\"\"Remove all matching triples from the backend.\"\"\"\n    s, p, o = triple\n    v = parse_object(o)\n    obj = v if isinstance(v, str) else str(v.value)\n    d = (\n        None\n        if not isinstance(v, Literal)\n        else f\"@{v.lang}\" if v.lang else v.datatype\n    )\n    self.collection.remove_relations(s, p, obj, d)\n</code></pre>"},{"location":"api_reference/backends/collection/#tripper.backends.collection.CollectionStrategy.triples","title":"<code>triples(self, triple)</code>","text":"<p>Returns a generator over matching triples.</p> Source code in <code>tripper/backends/collection.py</code> <pre><code>def triples(self, triple: \"Triple\") -&gt; \"Generator[Triple, None, None]\":\n    \"\"\"Returns a generator over matching triples.\"\"\"\n    for s, p, o, d in self.collection.get_relations(*triple, rettype=\"T\"):\n        if d:\n            lang = d[1:] if d[0] == \"@\" else None\n            dt = None if lang else d\n            yield s, p, Literal(o, lang=lang, datatype=dt)\n        else:\n            yield s, p, o\n</code></pre>"},{"location":"api_reference/backends/ontopy/","title":"ontopy","text":"<p>Backend for EMMOntoPy.</p> <p>For developers: The usage of <code>s</code>, <code>p</code>, and <code>o</code> represent the different parts of an RDF Triple: subject, predicate, and object.</p>"},{"location":"api_reference/backends/ontopy/#tripper.backends.ontopy.OntopyStrategy","title":"<code> OntopyStrategy        </code>","text":"<p>Triplestore strategy for EMMOntoPy.</p> <p>Parameters:</p> Name Type Description Default <code>base_iri</code> <code>Optional[str]</code> <p>The base iri of the ontology. Default to \"http://example.com/onto#\" if <code>onto</code> is not given.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Unused - ontopy does not support multiple databases.</p> <code>None</code> <code>onto</code> <code>Optional[Ontology]</code> <p>Ontology to initiate the triplestore from.  Defaults to an new ontology with the given <code>base_iri</code>.</p> <code>None</code> <code>load</code> <code>bool</code> <p>Whether to load the ontology.</p> <code>False</code> <code>kwargs</code> <p>Keyword arguments passed to the ontology load() method.</p> <code>{}</code> <p>Either the <code>base_iri</code> or <code>onto</code> argument must be provided.</p> Source code in <code>tripper/backends/ontopy.py</code> <pre><code>class OntopyStrategy:\n    \"\"\"Triplestore strategy for EMMOntoPy.\n\n    Arguments:\n        base_iri: The base iri of the ontology.\n            Default to \"http://example.com/onto#\" if `onto` is not given.\n        database: Unused - ontopy does not support multiple databases.\n        onto: Ontology to initiate the triplestore from.  Defaults to an new\n            ontology with the given `base_iri`.\n        load: Whether to load the ontology.\n        kwargs: Keyword arguments passed to the ontology load() method.\n\n    Either the `base_iri` or `onto` argument must be provided.\n    \"\"\"\n\n    prefer_sparql = False\n\n    def __init__(\n        self,\n        base_iri: \"Optional[str]\" = None,\n        database: \"Optional[str]\" = None,\n        onto: \"Optional[Ontology]\" = None,\n        load: bool = False,\n        **kwargs,\n    ):\n        # pylint: disable=unused-argument\n        if onto is None:\n            if base_iri is None:\n                base_iri = \"http://example.com/onto#\"\n            self.onto = get_ontology(base_iri)\n        elif isinstance(onto, Ontology):\n            self.onto = onto\n        else:\n            raise TypeError(\"`onto` must be either an ontology or None\")\n\n        if load:\n            self.onto.load(**kwargs)\n\n    def triples(self, triple: \"Triple\") -&gt; \"Generator[Triple, None, None]\":\n        \"\"\"Returns a generator over matching triples.\"\"\"\n\n        def to_literal(o, datatype) -&gt; Literal:\n            \"\"\"Returns a literal from (o, datatype).\"\"\"\n            if isinstance(datatype, str) and datatype.startswith(\"@\"):\n                return Literal(o, lang=datatype[1:], datatype=None)\n            return Literal(o, lang=None, datatype=datatype)\n\n        s, p, o = triple\n        abb = (\n            None if (s) is None else self.onto._abbreviate(s),\n            None if (p) is None else self.onto._abbreviate(p),\n            None if (o) is None else self.onto._abbreviate(o),\n        )\n        for s, p, o in self.onto._get_obj_triples_spo_spo(*abb):\n            yield (\n                _unabbreviate(self.onto, s),\n                _unabbreviate(self.onto, p),\n                _unabbreviate(self.onto, o),\n            )\n        for s, p, o, datatype in self.onto._get_data_triples_spod_spod(\n            *abb, d=\"\"\n        ):\n            yield (\n                _unabbreviate(self.onto, s),\n                _unabbreviate(self.onto, p),\n                to_literal(o, datatype),\n            )\n\n    def add_triples(self, triples: \"Sequence[Triple]\"):\n        \"\"\"Add a sequence of triples.\"\"\"\n        if TYPE_CHECKING:  # pragma: no cover\n            datatype: \"Union[int, str]\"\n        for s, p, o in triples:\n            if isinstance(o, Literal):\n                if o.lang:\n                    datatype = f\"@{o.lang}\"\n                elif o.datatype:\n                    datatype = f\"^^{o.datatype}\"\n                else:\n                    datatype = 0\n                self.onto._add_data_triple_spod(\n                    self.onto._abbreviate(s),\n                    self.onto._abbreviate(p),\n                    self.onto._abbreviate(o),\n                    datatype,\n                )\n            else:\n                self.onto._add_obj_triple_spo(\n                    self.onto._abbreviate(s),\n                    self.onto._abbreviate(p),\n                    self.onto._abbreviate(o),\n                )\n\n    def remove(self, triple: \"Triple\"):\n        \"\"\"Remove all matching triples from the backend.\"\"\"\n        s, p, o = triple\n        to_remove = list(\n            self.onto._get_triples_spod_spod(\n                self.onto._abbreviate(s) if (s) is not None else None,\n                self.onto._abbreviate(p) if (s) is not None else None,\n                self.onto._abbreviate(o) if (s) is not None else None,\n            )\n        )\n        for s, p, o, datatype in to_remove:\n            if datatype:\n                self.onto._del_data_triple_spod(s, p, o, datatype)\n            else:\n                self.onto._del_obj_triple_spo(s, p, o)\n\n    # Optional methods\n    def parse(\n        self,\n        source=None,\n        location=None,\n        data=None,\n        format=None,  # pylint: disable=redefined-builtin\n        encoding=None,\n        **kwargs,\n    ):\n        \"\"\"Parse source and add the resulting triples to triplestore.\n\n        The source is specified using one of `source`, `location` or `data`.\n\n        Parameters:\n            source: File-like object or file name.\n            location: String with relative or absolute URL to source.\n            data: String containing the data to be parsed.\n            format: Needed if format can not be inferred from source.\n            encoding: Encoding argument to io.open().\n            kwargs: Additional keyword arguments passed to Ontology.load().\n        \"\"\"\n        if source:\n            self.onto.load(filename=source, format=format, **kwargs)\n        elif location:\n            self.onto.load(filename=location, format=format, **kwargs)\n        elif data:\n            # s = io.StringIO(data)\n            # self.onto.load(filename=s, format=format, **kwargs)\n\n            # Could have been done much nicer if it hasn't been for Windows\n            filename = None\n            try:\n                tmpfile_options = {\"delete\": False}\n                if isinstance(data, str):\n                    tmpfile_options.update(mode=\"w+t\", encoding=encoding)\n                with tempfile.NamedTemporaryFile(**tmpfile_options) as handle:\n                    handle.write(data)\n                    filename = handle.name\n                self.onto.load(filename=filename, format=format, **kwargs)\n            finally:\n                if filename:\n                    os.remove(filename)\n\n        else:\n            raise ValueError(\n                \"either `source`, `location` or `data` must be given\"\n            )\n\n    def serialize(\n        self,\n        destination=None,\n        format=\"turtle\",  # pylint: disable=redefined-builtin\n        **kwargs,\n    ) -&gt; \"Union[None, str]\":\n        \"\"\"Serialise to destination.\n\n        Parameters:\n            destination: File name or object to write to.  If None, the\n                serialisation is returned.\n            format: Format to serialise as.  Supported formats, depends on\n                the backend.\n            kwargs: Passed to the Ontology.save() method.\n\n        Returns:\n            Serialised string if `destination` is None.\n        \"\"\"\n        if destination:\n            self.onto.save(destination, format=format, **kwargs)\n        else:\n            # Clumsy implementation due to Windows file locking...\n            filename = None\n            try:\n                with tempfile.NamedTemporaryFile(delete=False) as handle:\n                    filename = handle.name\n                    self.onto.save(filename, format=format, **kwargs)\n                with open(filename, \"rt\", encoding=\"utf8\") as handle:\n                    return handle.read()\n            finally:\n                if filename:\n                    os.remove(filename)\n        return None\n\n    def query(\n        self, query_object, native=True, **kwargs\n    ) -&gt; \"Union[List, Result]\":\n        \"\"\"SPARQL query.\n\n        Parameters:\n            query_object: String with the SPARQL query.\n            native: Whether or not to use EMMOntoPy/Owlready2 or RDFLib.\n            kwargs: Keyword arguments passed to rdflib.Graph.query().\n\n        Returns:\n            SPARQL query results.\n\n        \"\"\"\n        if TYPE_CHECKING:  # pragma: no cover\n            res: \"Union[List, Result]\"\n\n        if native:\n            res = self.onto.world.sparql(query_object)\n        else:\n            graph = self.onto.world.as_rdflib_graph()\n            res = graph.query(query_object, **kwargs)\n        # TODO: Convert result to expected type\n        return res\n\n    def update(self, update_object, native=True, **kwargs) -&gt; None:\n        \"\"\"Update triplestore with SPARQL.\n\n        Parameters:\n            update_object: String with the SPARQL query.\n            native: Whether or not to use EMMOntoPy/Owlready2 or RDFLib.\n            kwargs: Keyword arguments passed to rdflib.Graph.update().\n\n        Note:\n            This method is intended for INSERT and DELETE queries. Use\n            the query() method for SELECT queries.\n\n        \"\"\"\n        if native:\n            self.onto.world.sparql(update_object)\n        else:\n            graph = self.onto.world.as_rdflib_graph()\n            graph.update(update_object, **kwargs)\n</code></pre>"},{"location":"api_reference/backends/ontopy/#tripper.backends.ontopy.OntopyStrategy.add_triples","title":"<code>add_triples(self, triples)</code>","text":"<p>Add a sequence of triples.</p> Source code in <code>tripper/backends/ontopy.py</code> <pre><code>def add_triples(self, triples: \"Sequence[Triple]\"):\n    \"\"\"Add a sequence of triples.\"\"\"\n    if TYPE_CHECKING:  # pragma: no cover\n        datatype: \"Union[int, str]\"\n    for s, p, o in triples:\n        if isinstance(o, Literal):\n            if o.lang:\n                datatype = f\"@{o.lang}\"\n            elif o.datatype:\n                datatype = f\"^^{o.datatype}\"\n            else:\n                datatype = 0\n            self.onto._add_data_triple_spod(\n                self.onto._abbreviate(s),\n                self.onto._abbreviate(p),\n                self.onto._abbreviate(o),\n                datatype,\n            )\n        else:\n            self.onto._add_obj_triple_spo(\n                self.onto._abbreviate(s),\n                self.onto._abbreviate(p),\n                self.onto._abbreviate(o),\n            )\n</code></pre>"},{"location":"api_reference/backends/ontopy/#tripper.backends.ontopy.OntopyStrategy.parse","title":"<code>parse(self, source=None, location=None, data=None, format=None, encoding=None, **kwargs)</code>","text":"<p>Parse source and add the resulting triples to triplestore.</p> <p>The source is specified using one of <code>source</code>, <code>location</code> or <code>data</code>.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>File-like object or file name.</p> <code>None</code> <code>location</code> <p>String with relative or absolute URL to source.</p> <code>None</code> <code>data</code> <p>String containing the data to be parsed.</p> <code>None</code> <code>format</code> <p>Needed if format can not be inferred from source.</p> <code>None</code> <code>encoding</code> <p>Encoding argument to io.open().</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments passed to Ontology.load().</p> <code>{}</code> Source code in <code>tripper/backends/ontopy.py</code> <pre><code>def parse(\n    self,\n    source=None,\n    location=None,\n    data=None,\n    format=None,  # pylint: disable=redefined-builtin\n    encoding=None,\n    **kwargs,\n):\n    \"\"\"Parse source and add the resulting triples to triplestore.\n\n    The source is specified using one of `source`, `location` or `data`.\n\n    Parameters:\n        source: File-like object or file name.\n        location: String with relative or absolute URL to source.\n        data: String containing the data to be parsed.\n        format: Needed if format can not be inferred from source.\n        encoding: Encoding argument to io.open().\n        kwargs: Additional keyword arguments passed to Ontology.load().\n    \"\"\"\n    if source:\n        self.onto.load(filename=source, format=format, **kwargs)\n    elif location:\n        self.onto.load(filename=location, format=format, **kwargs)\n    elif data:\n        # s = io.StringIO(data)\n        # self.onto.load(filename=s, format=format, **kwargs)\n\n        # Could have been done much nicer if it hasn't been for Windows\n        filename = None\n        try:\n            tmpfile_options = {\"delete\": False}\n            if isinstance(data, str):\n                tmpfile_options.update(mode=\"w+t\", encoding=encoding)\n            with tempfile.NamedTemporaryFile(**tmpfile_options) as handle:\n                handle.write(data)\n                filename = handle.name\n            self.onto.load(filename=filename, format=format, **kwargs)\n        finally:\n            if filename:\n                os.remove(filename)\n\n    else:\n        raise ValueError(\n            \"either `source`, `location` or `data` must be given\"\n        )\n</code></pre>"},{"location":"api_reference/backends/ontopy/#tripper.backends.ontopy.OntopyStrategy.query","title":"<code>query(self, query_object, native=True, **kwargs)</code>","text":"<p>SPARQL query.</p> <p>Parameters:</p> Name Type Description Default <code>query_object</code> <p>String with the SPARQL query.</p> required <code>native</code> <p>Whether or not to use EMMOntoPy/Owlready2 or RDFLib.</p> <code>True</code> <code>kwargs</code> <p>Keyword arguments passed to rdflib.Graph.query().</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List, Result]</code> <p>SPARQL query results.</p> Source code in <code>tripper/backends/ontopy.py</code> <pre><code>def query(\n    self, query_object, native=True, **kwargs\n) -&gt; \"Union[List, Result]\":\n    \"\"\"SPARQL query.\n\n    Parameters:\n        query_object: String with the SPARQL query.\n        native: Whether or not to use EMMOntoPy/Owlready2 or RDFLib.\n        kwargs: Keyword arguments passed to rdflib.Graph.query().\n\n    Returns:\n        SPARQL query results.\n\n    \"\"\"\n    if TYPE_CHECKING:  # pragma: no cover\n        res: \"Union[List, Result]\"\n\n    if native:\n        res = self.onto.world.sparql(query_object)\n    else:\n        graph = self.onto.world.as_rdflib_graph()\n        res = graph.query(query_object, **kwargs)\n    # TODO: Convert result to expected type\n    return res\n</code></pre>"},{"location":"api_reference/backends/ontopy/#tripper.backends.ontopy.OntopyStrategy.remove","title":"<code>remove(self, triple)</code>","text":"<p>Remove all matching triples from the backend.</p> Source code in <code>tripper/backends/ontopy.py</code> <pre><code>def remove(self, triple: \"Triple\"):\n    \"\"\"Remove all matching triples from the backend.\"\"\"\n    s, p, o = triple\n    to_remove = list(\n        self.onto._get_triples_spod_spod(\n            self.onto._abbreviate(s) if (s) is not None else None,\n            self.onto._abbreviate(p) if (s) is not None else None,\n            self.onto._abbreviate(o) if (s) is not None else None,\n        )\n    )\n    for s, p, o, datatype in to_remove:\n        if datatype:\n            self.onto._del_data_triple_spod(s, p, o, datatype)\n        else:\n            self.onto._del_obj_triple_spo(s, p, o)\n</code></pre>"},{"location":"api_reference/backends/ontopy/#tripper.backends.ontopy.OntopyStrategy.serialize","title":"<code>serialize(self, destination=None, format='turtle', **kwargs)</code>","text":"<p>Serialise to destination.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <p>File name or object to write to.  If None, the serialisation is returned.</p> <code>None</code> <code>format</code> <p>Format to serialise as.  Supported formats, depends on the backend.</p> <code>'turtle'</code> <code>kwargs</code> <p>Passed to the Ontology.save() method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[None, str]</code> <p>Serialised string if <code>destination</code> is None.</p> Source code in <code>tripper/backends/ontopy.py</code> <pre><code>def serialize(\n    self,\n    destination=None,\n    format=\"turtle\",  # pylint: disable=redefined-builtin\n    **kwargs,\n) -&gt; \"Union[None, str]\":\n    \"\"\"Serialise to destination.\n\n    Parameters:\n        destination: File name or object to write to.  If None, the\n            serialisation is returned.\n        format: Format to serialise as.  Supported formats, depends on\n            the backend.\n        kwargs: Passed to the Ontology.save() method.\n\n    Returns:\n        Serialised string if `destination` is None.\n    \"\"\"\n    if destination:\n        self.onto.save(destination, format=format, **kwargs)\n    else:\n        # Clumsy implementation due to Windows file locking...\n        filename = None\n        try:\n            with tempfile.NamedTemporaryFile(delete=False) as handle:\n                filename = handle.name\n                self.onto.save(filename, format=format, **kwargs)\n            with open(filename, \"rt\", encoding=\"utf8\") as handle:\n                return handle.read()\n        finally:\n            if filename:\n                os.remove(filename)\n    return None\n</code></pre>"},{"location":"api_reference/backends/ontopy/#tripper.backends.ontopy.OntopyStrategy.triples","title":"<code>triples(self, triple)</code>","text":"<p>Returns a generator over matching triples.</p> Source code in <code>tripper/backends/ontopy.py</code> <pre><code>def triples(self, triple: \"Triple\") -&gt; \"Generator[Triple, None, None]\":\n    \"\"\"Returns a generator over matching triples.\"\"\"\n\n    def to_literal(o, datatype) -&gt; Literal:\n        \"\"\"Returns a literal from (o, datatype).\"\"\"\n        if isinstance(datatype, str) and datatype.startswith(\"@\"):\n            return Literal(o, lang=datatype[1:], datatype=None)\n        return Literal(o, lang=None, datatype=datatype)\n\n    s, p, o = triple\n    abb = (\n        None if (s) is None else self.onto._abbreviate(s),\n        None if (p) is None else self.onto._abbreviate(p),\n        None if (o) is None else self.onto._abbreviate(o),\n    )\n    for s, p, o in self.onto._get_obj_triples_spo_spo(*abb):\n        yield (\n            _unabbreviate(self.onto, s),\n            _unabbreviate(self.onto, p),\n            _unabbreviate(self.onto, o),\n        )\n    for s, p, o, datatype in self.onto._get_data_triples_spod_spod(\n        *abb, d=\"\"\n    ):\n        yield (\n            _unabbreviate(self.onto, s),\n            _unabbreviate(self.onto, p),\n            to_literal(o, datatype),\n        )\n</code></pre>"},{"location":"api_reference/backends/ontopy/#tripper.backends.ontopy.OntopyStrategy.update","title":"<code>update(self, update_object, native=True, **kwargs)</code>","text":"<p>Update triplestore with SPARQL.</p> <p>Parameters:</p> Name Type Description Default <code>update_object</code> <p>String with the SPARQL query.</p> required <code>native</code> <p>Whether or not to use EMMOntoPy/Owlready2 or RDFLib.</p> <code>True</code> <code>kwargs</code> <p>Keyword arguments passed to rdflib.Graph.update().</p> <code>{}</code> <p>Note</p> <p>This method is intended for INSERT and DELETE queries. Use the query() method for SELECT queries.</p> Source code in <code>tripper/backends/ontopy.py</code> <pre><code>def update(self, update_object, native=True, **kwargs) -&gt; None:\n    \"\"\"Update triplestore with SPARQL.\n\n    Parameters:\n        update_object: String with the SPARQL query.\n        native: Whether or not to use EMMOntoPy/Owlready2 or RDFLib.\n        kwargs: Keyword arguments passed to rdflib.Graph.update().\n\n    Note:\n        This method is intended for INSERT and DELETE queries. Use\n        the query() method for SELECT queries.\n\n    \"\"\"\n    if native:\n        self.onto.world.sparql(update_object)\n    else:\n        graph = self.onto.world.as_rdflib_graph()\n        graph.update(update_object, **kwargs)\n</code></pre>"},{"location":"api_reference/backends/rdflib/","title":"rdflib","text":"<p>Backend for RDFLib.</p> <p>For developers: The usage of <code>s</code>, <code>p</code>, and <code>o</code> represent the different parts of an RDF Triple: subject, predicate, and object.</p> <p>There is a issue with rdflib raising an <code>urllib.error.HTTPError</code> exception if you don't have write permissions to the cache directory. See Known issues for more details.</p>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy","title":"<code> RdflibStrategy        </code>","text":"<p>Triplestore strategy for rdflib.</p> <p>Parameters:</p> Name Type Description Default <code>base_iri</code> <code>Optional[str]</code> <p>Unused by the rdflib backend.  The <code>base_iri</code> argument is still used for encapsulating the Triplestore class.</p> <code>None</code> <code>database</code> <code>Optional[str]</code> <p>Unused - rdflib does not support multiple databases.</p> <code>None</code> <code>triplestore_url</code> <code>Optional[str]</code> <p>If given, initialise the triplestore from this storage.  When <code>close()</code> is called, the storage will be overwritten with the current content of the triplestore.</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Format of storage specified with <code>base_iri</code>.</p> <code>None</code> <code>graph</code> <code>Optional[Graph]</code> <p>A rdflib.Graph instance to expose with tripper, instead of creating a new empty Graph object.</p> <code>None</code> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>class RdflibStrategy:\n    \"\"\"Triplestore strategy for rdflib.\n\n    Arguments:\n        base_iri: Unused by the rdflib backend.  The `base_iri` argument is\n            still used for encapsulating the Triplestore class.\n        database: Unused - rdflib does not support multiple databases.\n        triplestore_url: If given, initialise the triplestore from this\n            storage.  When `close()` is called, the storage will be\n            overwritten with the current content of the triplestore.\n        format: Format of storage specified with `base_iri`.\n        graph: A rdflib.Graph instance to expose with tripper, instead of\n            creating a new empty Graph object.\n    \"\"\"\n\n    prefer_sparql = False\n\n    def __init__(\n        self,\n        base_iri: \"Optional[str]\" = None,  # pylint: disable=unused-argument\n        database: \"Optional[str]\" = None,\n        triplestore_url: \"Optional[str]\" = None,\n        format: \"Optional[str]\" = None,  # pylint: disable=redefined-builtin\n        graph: \"Optional[Graph]\" = None,\n    ) -&gt; None:\n        # Note that although `base_iri` is unused in this backend, it may\n        # still be used by calling Triplestore object.\n        if database:\n            warnings.warn(\"database\", UnusedArgumentWarning, stacklevel=3)\n\n        self.graph = graph if graph else Graph()\n        self.triplestore_url = triplestore_url\n        if self.triplestore_url is not None:\n            if format is None:\n                format = guess_format(self.triplestore_url)\n            self.parse(location=self.triplestore_url, format=format)\n        self.base_format = format\n\n    def triples(self, triple: \"Triple\") -&gt; \"Generator[Triple, None, None]\":\n        \"\"\"Returns a generator over matching triples.\"\"\"\n        return _convert_triples_to_tripper(\n            self.graph.triples(totriple(triple))\n        )\n\n    def add_triples(self, triples: \"Sequence[Triple]\"):\n        \"\"\"Add a sequence of triples.\"\"\"\n        for triple in triples:\n            self.graph.add(totriple(triple))\n\n    def remove(self, triple: \"Triple\"):\n        \"\"\"Remove all matching triples from the backend.\"\"\"\n        self.graph.remove(totriple(triple))\n\n    # Optional methods\n    def close(self):\n        \"\"\"Close the internal RDFLib graph.\"\"\"\n        if self.triplestore_url:\n            self.serialize(\n                destination=self.triplestore_url, format=self.base_format\n            )\n        self.graph.close()\n\n    def parse(\n        self,\n        source=None,\n        location=None,\n        data=None,\n        format=None,  # pylint: disable=redefined-builtin\n        **kwargs,\n    ):\n        \"\"\"Parse source and add the resulting triples to triplestore.\n\n        The source is specified using one of `source`, `location` or `data`.\n\n        Parameters:\n            source: File-like object or file name.\n            location: String with relative or absolute URL to source.\n            data: String containing the data to be parsed.\n            format: Needed if format can not be inferred from source.\n            kwargs: Additional less used keyword arguments.\n                See https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.html#rdflib.Graph.parse\n        \"\"\"\n        self.graph.parse(\n            source=source,\n            location=location,\n            data=data,\n            format=format,\n            **kwargs,\n        )\n\n    def serialize(\n        self,\n        destination=None,\n        format=\"turtle\",  # pylint: disable=redefined-builtin\n        **kwargs,\n    ) -&gt; \"Union[None, str]\":\n        \"\"\"Serialise to destination.\n\n        Parameters:\n            destination: File name or object to write to. If None, the serialisation is\n                returned.\n            format: Format to serialise as. Supported formats, depends on the backend.\n            kwargs: Passed to the rdflib.Graph.serialize() method.\n                See https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.html#rdflib.Graph.serialize\n\n        Returns:\n            Serialised string if `destination` is None.\n        \"\"\"\n        result = self.graph.serialize(\n            destination=destination, format=format, **kwargs\n        )\n        if destination is None:\n            # Depending on the version of rdflib the return value of\n            # graph.serialize() man either be a string or a bytes object...\n            return result if isinstance(result, str) else result.decode()\n        return None\n\n    def query(\n        self, query_object, **kwargs\n    ) -&gt; \"Union[List[Tuple[str, ...]], bool, Generator[Triple, None, None]]\":\n        \"\"\"SPARQL query.\n\n        Parameters:\n            query_object: String with the SPARQL query.\n            kwargs: Keyword arguments passed to rdflib.Graph.query().\n\n        Returns:\n            The return type depends on type of query:\n              - SELECT: list of tuples of IRIs for each matching row\n              - ASK: bool\n              - CONSTRUCT, DESCRIBE: generator over triples\n\n            For more info, see\n            https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.html#rdflib.query.Result\n        \"\"\"\n        result = self.graph.query(query_object=query_object, **kwargs)\n\n        # The type of the result object depends not only on the type of query,\n        # but also on the version of rdflib...  We try to be general here.\n        if hasattr(result, \"type\"):\n            resulttype = result.type\n        elif result.__class__.__name__ == \"ResultRow\":\n            resulttype = \"SELECT\"\n        elif isinstance(result, bool):\n            resulttype = \"ASK\"\n        elif isinstance(result, Generator):\n            resulttype = \"CONSTRUCT\"  # also DESCRIBE\n        else:\n            warnings.warn(\n                \"Unknown return type from rdflib.query(). Return it unprocessed.\"\n            )\n            return result  # type: ignore\n\n        if resulttype == \"SELECT\":\n            # type: ignore\n            return [tuple(fromrdflib(v) for v in row) for row in result]\n        if resulttype == \"ASK\":\n            return bool(result)\n        if resulttype in (\"CONSTRUCT\", \"DESCRIBE\"):\n            return _convert_triples_to_tripper(result)\n        assert False, \"should never be reached\"  # nosec\n\n    def update(self, update_object, **kwargs) -&gt; None:\n        \"\"\"Update triplestore with SPARQL.\n\n        Parameters:\n            update_object: String with the SPARQL query.\n            kwargs: Keyword arguments passed to rdflib.Graph.update().\n\n        Note:\n            This method is intended for INSERT and DELETE queries. Use\n            the query() method for SELECT queries.\n\n        \"\"\"\n        return self.graph.update(update_object=update_object, **kwargs)\n\n    def bind(self, prefix: str, namespace: str):\n        \"\"\"Bind prefix to namespace.\n\n        Should only be defined if the backend supports namespaces.\n        Called by triplestore.bind().\n        \"\"\"\n        if namespace:\n            self.graph.bind(prefix, namespace, replace=True)\n        else:\n            warnings.warn(\n                \"rdflib does not support removing namespace prefixes\"\n            )\n\n    def namespaces(self) -&gt; dict:\n        \"\"\"Returns a dict mapping prefixes to namespaces.\n\n        Should only be defined if the backend supports namespaces.\n        Used by triplestore.parse() to get prefixes after reading\n        triples from an external source.\n        \"\"\"\n        return {\n            prefix: str(namespace)\n            for prefix, namespace in self.graph.namespaces()\n        }\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy.add_triples","title":"<code>add_triples(self, triples)</code>","text":"<p>Add a sequence of triples.</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def add_triples(self, triples: \"Sequence[Triple]\"):\n    \"\"\"Add a sequence of triples.\"\"\"\n    for triple in triples:\n        self.graph.add(totriple(triple))\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy.bind","title":"<code>bind(self, prefix, namespace)</code>","text":"<p>Bind prefix to namespace.</p> <p>Should only be defined if the backend supports namespaces. Called by triplestore.bind().</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def bind(self, prefix: str, namespace: str):\n    \"\"\"Bind prefix to namespace.\n\n    Should only be defined if the backend supports namespaces.\n    Called by triplestore.bind().\n    \"\"\"\n    if namespace:\n        self.graph.bind(prefix, namespace, replace=True)\n    else:\n        warnings.warn(\n            \"rdflib does not support removing namespace prefixes\"\n        )\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy.close","title":"<code>close(self)</code>","text":"<p>Close the internal RDFLib graph.</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def close(self):\n    \"\"\"Close the internal RDFLib graph.\"\"\"\n    if self.triplestore_url:\n        self.serialize(\n            destination=self.triplestore_url, format=self.base_format\n        )\n    self.graph.close()\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy.namespaces","title":"<code>namespaces(self)</code>","text":"<p>Returns a dict mapping prefixes to namespaces.</p> <p>Should only be defined if the backend supports namespaces. Used by triplestore.parse() to get prefixes after reading triples from an external source.</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def namespaces(self) -&gt; dict:\n    \"\"\"Returns a dict mapping prefixes to namespaces.\n\n    Should only be defined if the backend supports namespaces.\n    Used by triplestore.parse() to get prefixes after reading\n    triples from an external source.\n    \"\"\"\n    return {\n        prefix: str(namespace)\n        for prefix, namespace in self.graph.namespaces()\n    }\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy.parse","title":"<code>parse(self, source=None, location=None, data=None, format=None, **kwargs)</code>","text":"<p>Parse source and add the resulting triples to triplestore.</p> <p>The source is specified using one of <code>source</code>, <code>location</code> or <code>data</code>.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <p>File-like object or file name.</p> <code>None</code> <code>location</code> <p>String with relative or absolute URL to source.</p> <code>None</code> <code>data</code> <p>String containing the data to be parsed.</p> <code>None</code> <code>format</code> <p>Needed if format can not be inferred from source.</p> <code>None</code> <code>kwargs</code> <p>Additional less used keyword arguments. See https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.html#rdflib.Graph.parse</p> <code>{}</code> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def parse(\n    self,\n    source=None,\n    location=None,\n    data=None,\n    format=None,  # pylint: disable=redefined-builtin\n    **kwargs,\n):\n    \"\"\"Parse source and add the resulting triples to triplestore.\n\n    The source is specified using one of `source`, `location` or `data`.\n\n    Parameters:\n        source: File-like object or file name.\n        location: String with relative or absolute URL to source.\n        data: String containing the data to be parsed.\n        format: Needed if format can not be inferred from source.\n        kwargs: Additional less used keyword arguments.\n            See https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.html#rdflib.Graph.parse\n    \"\"\"\n    self.graph.parse(\n        source=source,\n        location=location,\n        data=data,\n        format=format,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy.query","title":"<code>query(self, query_object, **kwargs)</code>","text":"<p>SPARQL query.</p> <p>Parameters:</p> Name Type Description Default <code>query_object</code> <p>String with the SPARQL query.</p> required <code>kwargs</code> <p>Keyword arguments passed to rdflib.Graph.query().</p> <code>{}</code> <p>Returns:</p> Type Description <code>The return type depends on type of query</code> <ul> <li>SELECT: list of tuples of IRIs for each matching row</li> <li>ASK: bool</li> <li>CONSTRUCT, DESCRIBE: generator over triples</li> </ul> <p>For more info, see https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.html#rdflib.query.Result</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def query(\n    self, query_object, **kwargs\n) -&gt; \"Union[List[Tuple[str, ...]], bool, Generator[Triple, None, None]]\":\n    \"\"\"SPARQL query.\n\n    Parameters:\n        query_object: String with the SPARQL query.\n        kwargs: Keyword arguments passed to rdflib.Graph.query().\n\n    Returns:\n        The return type depends on type of query:\n          - SELECT: list of tuples of IRIs for each matching row\n          - ASK: bool\n          - CONSTRUCT, DESCRIBE: generator over triples\n\n        For more info, see\n        https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.html#rdflib.query.Result\n    \"\"\"\n    result = self.graph.query(query_object=query_object, **kwargs)\n\n    # The type of the result object depends not only on the type of query,\n    # but also on the version of rdflib...  We try to be general here.\n    if hasattr(result, \"type\"):\n        resulttype = result.type\n    elif result.__class__.__name__ == \"ResultRow\":\n        resulttype = \"SELECT\"\n    elif isinstance(result, bool):\n        resulttype = \"ASK\"\n    elif isinstance(result, Generator):\n        resulttype = \"CONSTRUCT\"  # also DESCRIBE\n    else:\n        warnings.warn(\n            \"Unknown return type from rdflib.query(). Return it unprocessed.\"\n        )\n        return result  # type: ignore\n\n    if resulttype == \"SELECT\":\n        # type: ignore\n        return [tuple(fromrdflib(v) for v in row) for row in result]\n    if resulttype == \"ASK\":\n        return bool(result)\n    if resulttype in (\"CONSTRUCT\", \"DESCRIBE\"):\n        return _convert_triples_to_tripper(result)\n    assert False, \"should never be reached\"  # nosec\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy.remove","title":"<code>remove(self, triple)</code>","text":"<p>Remove all matching triples from the backend.</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def remove(self, triple: \"Triple\"):\n    \"\"\"Remove all matching triples from the backend.\"\"\"\n    self.graph.remove(totriple(triple))\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy.serialize","title":"<code>serialize(self, destination=None, format='turtle', **kwargs)</code>","text":"<p>Serialise to destination.</p> <p>Parameters:</p> Name Type Description Default <code>destination</code> <p>File name or object to write to. If None, the serialisation is returned.</p> <code>None</code> <code>format</code> <p>Format to serialise as. Supported formats, depends on the backend.</p> <code>'turtle'</code> <code>kwargs</code> <p>Passed to the rdflib.Graph.serialize() method. See https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.html#rdflib.Graph.serialize</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[None, str]</code> <p>Serialised string if <code>destination</code> is None.</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def serialize(\n    self,\n    destination=None,\n    format=\"turtle\",  # pylint: disable=redefined-builtin\n    **kwargs,\n) -&gt; \"Union[None, str]\":\n    \"\"\"Serialise to destination.\n\n    Parameters:\n        destination: File name or object to write to. If None, the serialisation is\n            returned.\n        format: Format to serialise as. Supported formats, depends on the backend.\n        kwargs: Passed to the rdflib.Graph.serialize() method.\n            See https://rdflib.readthedocs.io/en/stable/apidocs/rdflib.html#rdflib.Graph.serialize\n\n    Returns:\n        Serialised string if `destination` is None.\n    \"\"\"\n    result = self.graph.serialize(\n        destination=destination, format=format, **kwargs\n    )\n    if destination is None:\n        # Depending on the version of rdflib the return value of\n        # graph.serialize() man either be a string or a bytes object...\n        return result if isinstance(result, str) else result.decode()\n    return None\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy.triples","title":"<code>triples(self, triple)</code>","text":"<p>Returns a generator over matching triples.</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def triples(self, triple: \"Triple\") -&gt; \"Generator[Triple, None, None]\":\n    \"\"\"Returns a generator over matching triples.\"\"\"\n    return _convert_triples_to_tripper(\n        self.graph.triples(totriple(triple))\n    )\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.RdflibStrategy.update","title":"<code>update(self, update_object, **kwargs)</code>","text":"<p>Update triplestore with SPARQL.</p> <p>Parameters:</p> Name Type Description Default <code>update_object</code> <p>String with the SPARQL query.</p> required <code>kwargs</code> <p>Keyword arguments passed to rdflib.Graph.update().</p> <code>{}</code> <p>Note</p> <p>This method is intended for INSERT and DELETE queries. Use the query() method for SELECT queries.</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def update(self, update_object, **kwargs) -&gt; None:\n    \"\"\"Update triplestore with SPARQL.\n\n    Parameters:\n        update_object: String with the SPARQL query.\n        kwargs: Keyword arguments passed to rdflib.Graph.update().\n\n    Note:\n        This method is intended for INSERT and DELETE queries. Use\n        the query() method for SELECT queries.\n\n    \"\"\"\n    return self.graph.update(update_object=update_object, **kwargs)\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.fromrdflib","title":"<code>fromrdflib(value)</code>","text":"<p>Help function converting an rdflib value to corresponding tripper value.</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def fromrdflib(\n    value: \"Union[URIRef, rdflibLiteral, BNode]\",\n) -&gt; \"Union[str, Literal]\":\n    \"\"\"Help function converting an rdflib value to corresponding tripper value.\"\"\"\n    if isinstance(value, rdflibLiteral):\n        return parse_literal(value)\n    if isinstance(value, BNode) and not value.startswith(\"_:\"):\n        return f\"_:{value}\"\n    return str(value)\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.tordflib","title":"<code>tordflib(value)</code>","text":"<p>Help function converting a spo-value to proper rdflib type.</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def tordflib(value: \"Union[None, Literal, str]\"):\n    \"\"\"Help function converting a spo-value to proper rdflib type.\"\"\"\n    if value is None:\n        return None\n    if isinstance(value, Literal):\n        return rdflibLiteral(value, lang=value.lang, datatype=value.datatype)\n    if value.startswith(\"_:\"):\n        return BNode(value[2:])\n    return URIRef(value)\n</code></pre>"},{"location":"api_reference/backends/rdflib/#tripper.backends.rdflib.totriple","title":"<code>totriple(triple)</code>","text":"<p>Help function converting a triple to rdflib triple.</p> Source code in <code>tripper/backends/rdflib.py</code> <pre><code>def totriple(triple: \"Triple\"):\n    \"\"\"Help function converting a triple to rdflib triple.\"\"\"\n    s, p, o = triple\n    return tordflib(s), tordflib(p), tordflib(o)\n</code></pre>"},{"location":"api_reference/backends/sparqlwrapper/","title":"sparqlwrapper","text":"<p>Backend for SPARQLWrapper</p>"},{"location":"api_reference/backends/sparqlwrapper/#tripper.backends.sparqlwrapper.SparqlwrapperStrategy","title":"<code> SparqlwrapperStrategy        </code>","text":"<p>Triplestore strategy for SPARQLWrapper.</p> <p>Parameters:</p> Name Type Description Default <code>base_iri</code> <code>str</code> <p>SPARQL endpoint.</p> required <code>update_iri</code> <code>Optional[str]</code> <p>Update SPARQL endpoint. For some triplestores (e.g. GraphDB), update endpoint is different from base endpoint. Defaults to base_iri.</p> <code>None</code> <code>check_iri</code> <code>Optional[str]</code> <p>IRI to use for checking that the triplestore is available. Defaults to base_iri.</p> <code>None</code> <code>username</code> <code>Optional[str]</code> <p>User name.</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Password.</p> <code>None</code> <code>kwargs</code> <p>Additional arguments passed to the SPARQLWrapper constructor.</p> <code>{}</code> Source code in <code>tripper/backends/sparqlwrapper.py</code> <pre><code>class SparqlwrapperStrategy:\n    \"\"\"Triplestore strategy for SPARQLWrapper.\n\n    Arguments:\n        base_iri: SPARQL endpoint.\n        update_iri: Update SPARQL endpoint. For some triplestores (e.g.\n            GraphDB), update endpoint is different from base endpoint.\n            Defaults to base_iri.\n        check_iri: IRI to use for checking that the triplestore is available.\n            Defaults to base_iri.\n        username: User name.\n        password: Password.\n        kwargs: Additional arguments passed to the SPARQLWrapper constructor.\n\n    \"\"\"\n\n    prefer_sparql = True\n\n    def __init__(\n        self,\n        base_iri: str,\n        update_iri: \"Optional[str]\" = None,\n        check_iri: \"Optional[str]\" = None,\n        username: \"Optional[str]\" = None,\n        password: \"Optional[str]\" = None,\n        **kwargs,\n    ) -&gt; None:\n        self.update_iri = update_iri if update_iri else base_iri\n        self.check_iri = check_iri if check_iri else base_iri\n\n        kwargs.pop(\n            \"database\", None\n        )  # database is not used in the SPARQLWrapper backend\n        self.sparql = SPARQLWrapper(\n            endpoint=base_iri, updateEndpoint=update_iri, **kwargs\n        )\n        if username and password:\n            self.sparql.setCredentials(username, password)\n\n    @property\n    def update_iri(self) -&gt; \"Optional[str]\":\n        \"\"\"Getter for the update IRI.\"\"\"\n        return self._update_iri\n\n    @update_iri.setter\n    def update_iri(self, new_update_iri: \"Optional[str]\") -&gt; None:\n        \"\"\"Setter for the update IRI that also updates the SPARQL endpoint.\"\"\"\n        self._update_iri = new_update_iri\n        # Update the SPARQLWrapper's updateEndpoint only if it was initialized.\n        if hasattr(self, \"sparql\"):\n            self.sparql.updateEndpoint = new_update_iri\n\n    def query(\n        self,\n        query_object: str,\n        **kwargs,  # pylint: disable=unused-argument\n    ) -&gt; \"Union[List[Tuple[str, ...]], bool, Generator[Triple, None, None]]\":\n        \"\"\"SPARQL query.\n\n        Parameters:\n            query_object: String with the SPARQL query.\n            kwargs: Keyword arguments passed to rdflib.Graph.query().\n\n        Returns:\n            The return type depends on type of query:\n              - ASK: whether there is a match\n              - CONSTRUCT: generator over triples\n              - DESCRIBE: generator over triples\n              - SELECT: list of tuples of IRIs\n        \"\"\"\n        query_type = self._get_sparql_query_type(query_object)\n\n        if query_type == \"ASK\":\n            self.sparql.setReturnFormat(JSON)\n            self.sparql.setMethod(GET)\n            self.sparql.setQuery(query_object)\n            result = self.sparql.queryAndConvert()\n            value = result[\"boolean\"]\n            return value\n\n        if query_type == \"CONSTRUCT\":\n            self.sparql.setReturnFormat(TURTLE)\n            self.sparql.setMethod(GET)\n            self.sparql.setQuery(query_object)\n            results = self.sparql.queryAndConvert()\n            graph = Graph()\n            graph.parse(data=results.decode(\"utf-8\"), format=\"turtle\")\n            return _convert_triples_to_tripper(graph)\n\n        if query_type == \"DESCRIBE\":\n            self.sparql.setReturnFormat(TURTLE)\n            self.sparql.setMethod(GET)\n            self.sparql.setQuery(query_object)\n            results = self.sparql.queryAndConvert()\n            graph = Graph()\n            graph.parse(data=results.decode(\"utf-8\"), format=\"turtle\")\n            return _convert_triples_to_tripper(graph)\n\n        if query_type == \"SELECT\":\n            self.sparql.setReturnFormat(JSON)\n            self.sparql.setMethod(GET)\n            self.sparql.setQuery(query_object)\n            ret = self.sparql.queryAndConvert()\n            bindings = ret[\"results\"][\"bindings\"]\n            return [\n                tuple(convert_json_entrydict(v) for v in row.values())\n                for row in bindings\n            ]\n\n        raise NotImplementedError(\n            f\"Query type '{query_type}' not implemented.\"\n        )\n\n    def _get_sparql_query_type(self, query: str) -&gt; str:\n        \"\"\"\n        Returns the SPARQL query type (e.g., SELECT, ASK, CONSTRUCT, DESCRIBE)\n        by finding the first word of a sentence that matches one of these\n        keywords.\n        If none is found, it returns 'UNKNOWN'.\n        \"\"\"\n        # A regex that looks for a sentence start:\n        # either the beginning of the string or following a newline\n        # or a period. It then matches one of the keywords.\n        pattern = re.compile(\n            r\"(?:(?&lt;=^)|(?&lt;=[\\.\\n]))\\s*(ASK|CONSTRUCT|SELECT|DESCRIBE|\"\n            r\"DELETE|INSERT)\\b\",\n            re.IGNORECASE,\n        )\n        match = pattern.search(query)\n        if match:\n            return match.group(1).upper()\n        return \"UNKNOWN\"\n\n    def update(\n        self,\n        update_object: str,\n        **kwargs,  # pylint: disable=unused-argument\n    ) -&gt; None:\n        \"\"\"Update triplestore with SPARQL query.\n\n        Arguments:\n            update_object: String with the SPARQL query.\n            kwargs: Additional backend-specific keyword arguments.\n\n        Note:\n            This method is intended for INSERT and DELETE queries.  Use\n            the query() method for ASK/CONSTRUCT/SELECT/DESCRIBE queries.\n        \"\"\"\n        query_type = self._get_sparql_query_type(update_object)\n        if query_type not in (\"DELETE\", \"INSERT\"):\n            raise NotImplementedError(\n                f\"Update query type '{query_type}' not implemented.\"\n            )\n        self.sparql.setReturnFormat(TURTLE)\n        self.sparql.setMethod(POST)\n        self.sparql.setQuery(update_object)\n        self.sparql.query()\n\n    def is_available(self, timeout: float = 5, interval: float = 1) -&gt; bool:\n        \"\"\"Checks if the backend is available.\n\n        This is done by sending a request to the URL specified\n        in the `check_iri` attribute and checking for the response.\n\n        Arguments:\n            timeout: Total time in seconds to wait for a response.\n            interval: Internal time interval in seconds between checking if\n                the service has responded.\n\n        Returns:\n            Returns true if the service responds with code 200,\n            otherwise false is returned.\n\n        \"\"\"\n        if self.check_iri is None:\n            raise ValueError(\n                \"`check_iri` must be assigned before calling is_available()\"\n            )\n        return check_service_availability(\n            self.check_iri, timeout=timeout, interval=interval\n        )\n\n    def triples(self, triple: \"Triple\") -&gt; \"Generator[Triple, None, None]\":\n        \"\"\"Returns a generator over matching triples.\"\"\"\n        variables = [\n            f\"?{triple_name}\"\n            for triple_name, triple_value in zip(\"spo\", triple)\n            if triple_value is None\n        ]\n        where_spec = \" \".join(\n            (\n                f\"?{triple_name}\"\n                if triple_value is None\n                else (\n                    triple_value\n                    if triple_value.startswith(\"&lt;\")\n                    else f\"&lt;{triple_value}&gt;\"\n                )\n            )\n            for triple_name, triple_value in zip(\"spo\", triple)\n        )\n        query = \"\\n\".join(\n            [\n                f\"SELECT {' '.join(variables)} WHERE {{\",\n                f\"  {where_spec} .\",\n                \"}\",\n            ]\n        )\n        self.sparql.setReturnFormat(JSON)\n        self.sparql.setMethod(GET)\n        self.sparql.setQuery(query)\n        ret = self.sparql.queryAndConvert()\n        for binding in ret[\"results\"][\"bindings\"]:\n            yield tuple(\n                (\n                    convert_json_entrydict(binding[name])\n                    if name in binding\n                    else value\n                )\n                for name, value in zip(\"spo\", triple)\n            )\n\n    def add_triples(self, triples: \"Sequence[Triple]\") -&gt; \"QueryResult\":\n        \"\"\"Add a sequence of triples.\"\"\"\n\n        self._check_endpoint()\n\n        spec = \"\\n\".join(\n            \"  \"\n            + \" \".join(\n                (\n                    value.n3()\n                    if isinstance(value, Literal)\n                    else value if value.startswith(\"&lt;\") else f\"&lt;{value}&gt;\"\n                )\n                for value in triple\n            )\n            + \" .\"\n            for triple in triples\n        )\n        query = f\"INSERT DATA {{\\n{spec}\\n}}\"\n\n        self.sparql.setReturnFormat(TURTLE)\n        self.sparql.setMethod(POST)\n        self.sparql.setQuery(query)\n        return self.sparql.query()\n\n    def remove(self, triple: \"Triple\") -&gt; \"QueryResult\":\n        \"\"\"Remove all matching triples from the backend.\"\"\"\n        self._check_endpoint()\n        spec = \" \".join(\n            (\n                f\"?{name}\"\n                if value is None\n                else (\n                    value.n3()\n                    if isinstance(value, Literal)\n                    else value if value.startswith(\"&lt;\") else f\"&lt;{value}&gt;\"\n                )\n            )\n            for name, value in zip(\"spo\", triple)\n        )\n        query = f\"\"\"\n        DELETE WHERE {{\n          {spec} .\n        }}\n        \"\"\"\n\n        self.sparql.setReturnFormat(TURTLE)\n        self.sparql.setMethod(POST)\n        self.sparql.setQuery(query)\n        return self.sparql.query()\n\n    def _check_endpoint(self):\n        \"\"\"Check if the update endpoint is valid\"\"\"\n        if not self.sparql.isSparqlUpdateRequest() and self.update_iri is None:\n            raise TripperError(\n                f\"The base_iri '{self.sparql.updateEndpoint}' \"\n                \"is not a valid update endpoint. \"\n                \"For updates it is necessary to give the \"\n                \"'update_iri' as argument to the triplestore directly,\"\n                \"or update it with ts.backend.update_iri = ...\"\n            )\n</code></pre>"},{"location":"api_reference/backends/sparqlwrapper/#tripper.backends.sparqlwrapper.SparqlwrapperStrategy.update_iri","title":"<code>update_iri: Optional[str]</code>  <code>property</code> <code>writable</code>","text":"<p>Getter for the update IRI.</p>"},{"location":"api_reference/backends/sparqlwrapper/#tripper.backends.sparqlwrapper.SparqlwrapperStrategy.add_triples","title":"<code>add_triples(self, triples)</code>","text":"<p>Add a sequence of triples.</p> Source code in <code>tripper/backends/sparqlwrapper.py</code> <pre><code>def add_triples(self, triples: \"Sequence[Triple]\") -&gt; \"QueryResult\":\n    \"\"\"Add a sequence of triples.\"\"\"\n\n    self._check_endpoint()\n\n    spec = \"\\n\".join(\n        \"  \"\n        + \" \".join(\n            (\n                value.n3()\n                if isinstance(value, Literal)\n                else value if value.startswith(\"&lt;\") else f\"&lt;{value}&gt;\"\n            )\n            for value in triple\n        )\n        + \" .\"\n        for triple in triples\n    )\n    query = f\"INSERT DATA {{\\n{spec}\\n}}\"\n\n    self.sparql.setReturnFormat(TURTLE)\n    self.sparql.setMethod(POST)\n    self.sparql.setQuery(query)\n    return self.sparql.query()\n</code></pre>"},{"location":"api_reference/backends/sparqlwrapper/#tripper.backends.sparqlwrapper.SparqlwrapperStrategy.is_available","title":"<code>is_available(self, timeout=5, interval=1)</code>","text":"<p>Checks if the backend is available.</p> <p>This is done by sending a request to the URL specified in the <code>check_iri</code> attribute and checking for the response.</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>float</code> <p>Total time in seconds to wait for a response.</p> <code>5</code> <code>interval</code> <code>float</code> <p>Internal time interval in seconds between checking if the service has responded.</p> <code>1</code> <p>Returns:</p> Type Description <code>bool</code> <p>Returns true if the service responds with code 200, otherwise false is returned.</p> Source code in <code>tripper/backends/sparqlwrapper.py</code> <pre><code>def is_available(self, timeout: float = 5, interval: float = 1) -&gt; bool:\n    \"\"\"Checks if the backend is available.\n\n    This is done by sending a request to the URL specified\n    in the `check_iri` attribute and checking for the response.\n\n    Arguments:\n        timeout: Total time in seconds to wait for a response.\n        interval: Internal time interval in seconds between checking if\n            the service has responded.\n\n    Returns:\n        Returns true if the service responds with code 200,\n        otherwise false is returned.\n\n    \"\"\"\n    if self.check_iri is None:\n        raise ValueError(\n            \"`check_iri` must be assigned before calling is_available()\"\n        )\n    return check_service_availability(\n        self.check_iri, timeout=timeout, interval=interval\n    )\n</code></pre>"},{"location":"api_reference/backends/sparqlwrapper/#tripper.backends.sparqlwrapper.SparqlwrapperStrategy.query","title":"<code>query(self, query_object, **kwargs)</code>","text":"<p>SPARQL query.</p> <p>Parameters:</p> Name Type Description Default <code>query_object</code> <code>str</code> <p>String with the SPARQL query.</p> required <code>kwargs</code> <p>Keyword arguments passed to rdflib.Graph.query().</p> <code>{}</code> <p>Returns:</p> Type Description <code>The return type depends on type of query</code> <ul> <li>ASK: whether there is a match</li> <li>CONSTRUCT: generator over triples</li> <li>DESCRIBE: generator over triples</li> <li>SELECT: list of tuples of IRIs</li> </ul> Source code in <code>tripper/backends/sparqlwrapper.py</code> <pre><code>def query(\n    self,\n    query_object: str,\n    **kwargs,  # pylint: disable=unused-argument\n) -&gt; \"Union[List[Tuple[str, ...]], bool, Generator[Triple, None, None]]\":\n    \"\"\"SPARQL query.\n\n    Parameters:\n        query_object: String with the SPARQL query.\n        kwargs: Keyword arguments passed to rdflib.Graph.query().\n\n    Returns:\n        The return type depends on type of query:\n          - ASK: whether there is a match\n          - CONSTRUCT: generator over triples\n          - DESCRIBE: generator over triples\n          - SELECT: list of tuples of IRIs\n    \"\"\"\n    query_type = self._get_sparql_query_type(query_object)\n\n    if query_type == \"ASK\":\n        self.sparql.setReturnFormat(JSON)\n        self.sparql.setMethod(GET)\n        self.sparql.setQuery(query_object)\n        result = self.sparql.queryAndConvert()\n        value = result[\"boolean\"]\n        return value\n\n    if query_type == \"CONSTRUCT\":\n        self.sparql.setReturnFormat(TURTLE)\n        self.sparql.setMethod(GET)\n        self.sparql.setQuery(query_object)\n        results = self.sparql.queryAndConvert()\n        graph = Graph()\n        graph.parse(data=results.decode(\"utf-8\"), format=\"turtle\")\n        return _convert_triples_to_tripper(graph)\n\n    if query_type == \"DESCRIBE\":\n        self.sparql.setReturnFormat(TURTLE)\n        self.sparql.setMethod(GET)\n        self.sparql.setQuery(query_object)\n        results = self.sparql.queryAndConvert()\n        graph = Graph()\n        graph.parse(data=results.decode(\"utf-8\"), format=\"turtle\")\n        return _convert_triples_to_tripper(graph)\n\n    if query_type == \"SELECT\":\n        self.sparql.setReturnFormat(JSON)\n        self.sparql.setMethod(GET)\n        self.sparql.setQuery(query_object)\n        ret = self.sparql.queryAndConvert()\n        bindings = ret[\"results\"][\"bindings\"]\n        return [\n            tuple(convert_json_entrydict(v) for v in row.values())\n            for row in bindings\n        ]\n\n    raise NotImplementedError(\n        f\"Query type '{query_type}' not implemented.\"\n    )\n</code></pre>"},{"location":"api_reference/backends/sparqlwrapper/#tripper.backends.sparqlwrapper.SparqlwrapperStrategy.remove","title":"<code>remove(self, triple)</code>","text":"<p>Remove all matching triples from the backend.</p> Source code in <code>tripper/backends/sparqlwrapper.py</code> <pre><code>def remove(self, triple: \"Triple\") -&gt; \"QueryResult\":\n    \"\"\"Remove all matching triples from the backend.\"\"\"\n    self._check_endpoint()\n    spec = \" \".join(\n        (\n            f\"?{name}\"\n            if value is None\n            else (\n                value.n3()\n                if isinstance(value, Literal)\n                else value if value.startswith(\"&lt;\") else f\"&lt;{value}&gt;\"\n            )\n        )\n        for name, value in zip(\"spo\", triple)\n    )\n    query = f\"\"\"\n    DELETE WHERE {{\n      {spec} .\n    }}\n    \"\"\"\n\n    self.sparql.setReturnFormat(TURTLE)\n    self.sparql.setMethod(POST)\n    self.sparql.setQuery(query)\n    return self.sparql.query()\n</code></pre>"},{"location":"api_reference/backends/sparqlwrapper/#tripper.backends.sparqlwrapper.SparqlwrapperStrategy.triples","title":"<code>triples(self, triple)</code>","text":"<p>Returns a generator over matching triples.</p> Source code in <code>tripper/backends/sparqlwrapper.py</code> <pre><code>def triples(self, triple: \"Triple\") -&gt; \"Generator[Triple, None, None]\":\n    \"\"\"Returns a generator over matching triples.\"\"\"\n    variables = [\n        f\"?{triple_name}\"\n        for triple_name, triple_value in zip(\"spo\", triple)\n        if triple_value is None\n    ]\n    where_spec = \" \".join(\n        (\n            f\"?{triple_name}\"\n            if triple_value is None\n            else (\n                triple_value\n                if triple_value.startswith(\"&lt;\")\n                else f\"&lt;{triple_value}&gt;\"\n            )\n        )\n        for triple_name, triple_value in zip(\"spo\", triple)\n    )\n    query = \"\\n\".join(\n        [\n            f\"SELECT {' '.join(variables)} WHERE {{\",\n            f\"  {where_spec} .\",\n            \"}\",\n        ]\n    )\n    self.sparql.setReturnFormat(JSON)\n    self.sparql.setMethod(GET)\n    self.sparql.setQuery(query)\n    ret = self.sparql.queryAndConvert()\n    for binding in ret[\"results\"][\"bindings\"]:\n        yield tuple(\n            (\n                convert_json_entrydict(binding[name])\n                if name in binding\n                else value\n            )\n            for name, value in zip(\"spo\", triple)\n        )\n</code></pre>"},{"location":"api_reference/backends/sparqlwrapper/#tripper.backends.sparqlwrapper.SparqlwrapperStrategy.update","title":"<code>update(self, update_object, **kwargs)</code>","text":"<p>Update triplestore with SPARQL query.</p> <p>Parameters:</p> Name Type Description Default <code>update_object</code> <code>str</code> <p>String with the SPARQL query.</p> required <code>kwargs</code> <p>Additional backend-specific keyword arguments.</p> <code>{}</code> <p>Note</p> <p>This method is intended for INSERT and DELETE queries.  Use the query() method for ASK/CONSTRUCT/SELECT/DESCRIBE queries.</p> Source code in <code>tripper/backends/sparqlwrapper.py</code> <pre><code>def update(\n    self,\n    update_object: str,\n    **kwargs,  # pylint: disable=unused-argument\n) -&gt; None:\n    \"\"\"Update triplestore with SPARQL query.\n\n    Arguments:\n        update_object: String with the SPARQL query.\n        kwargs: Additional backend-specific keyword arguments.\n\n    Note:\n        This method is intended for INSERT and DELETE queries.  Use\n        the query() method for ASK/CONSTRUCT/SELECT/DESCRIBE queries.\n    \"\"\"\n    query_type = self._get_sparql_query_type(update_object)\n    if query_type not in (\"DELETE\", \"INSERT\"):\n        raise NotImplementedError(\n            f\"Update query type '{query_type}' not implemented.\"\n        )\n    self.sparql.setReturnFormat(TURTLE)\n    self.sparql.setMethod(POST)\n    self.sparql.setQuery(update_object)\n    self.sparql.query()\n</code></pre>"},{"location":"api_reference/backends/sparqlwrapper/#tripper.backends.sparqlwrapper.convert_json_entrydict","title":"<code>convert_json_entrydict(entrydict)</code>","text":"<p>Convert SPARQLWrapper json entry dict (representing a single IRI or literal) to a tripper type.</p> Source code in <code>tripper/backends/sparqlwrapper.py</code> <pre><code>def convert_json_entrydict(entrydict: dict) -&gt; str:\n    \"\"\"Convert SPARQLWrapper json entry dict (representing a single IRI or\n    literal) to a tripper type.\"\"\"\n    if entrydict[\"type\"] == \"uri\":\n        return entrydict[\"value\"]\n    if entrydict[\"type\"] == \"literal\":\n        return Literal(\n            entrydict[\"value\"],\n            lang=entrydict.get(\"xml:lang\"),\n            datatype=entrydict.get(\"datatype\"),\n        )\n    if entrydict[\"type\"] == \"bnode\":\n        return (\n            entrydict[\"value\"]\n            if entrydict[\"value\"].startswith(\"_:\")\n            else f\"_:{entrydict['value']}\"\n        )\n\n    raise ValueError(f\"unexpected type in entrydict: {entrydict}\")\n</code></pre>"},{"location":"api_reference/convert/convert/","title":"convert","text":"<p>Tripper module for converting between RDF and other repetations.</p> <p>Example use:</p> <pre><code>&gt;&gt;&gt; from tripper import DCTERMS, Literal, Triplestore\n&gt;&gt;&gt; from tripper.convert import load_container, save_container\n\n&gt;&gt;&gt; ts = Triplestore(\"rdflib\")\n&gt;&gt;&gt; dataset = {\"a\": 1, \"b\": 2}\n&gt;&gt;&gt; save_container(ts, dataset, \":data_indv\")\n&gt;&gt;&gt; load_container(ts, \":data_indv\")\n{'a': 1, 'b': 2}\n\n# Add additional context to our data individual\n&gt;&gt;&gt; ts.add((\":data_indv\", DCTERMS.title, Literal(\"My wonderful data\")))\n\n&gt;&gt;&gt; load_container(ts, \":data_indv\")  # doctest: +IGNORE_EXCEPTION_DETAIL\nTraceback (most recent call last):\nValueError: Unrecognised predicate 'http://purl.org/dc/terms/title' in dict: :data_indv\n\n&gt;&gt;&gt; load_container(ts, \":data_indv\", ignore_unrecognised=True)\n{'a': 1, 'b': 2}\n</code></pre>"},{"location":"api_reference/convert/convert/#tripper.convert.convert.from_container","title":"<code>from_container(container, iri, lang='en', recognised_keys=None, keep=False)</code>","text":"<p>Serialise a basic Python container type (mapping or sequence) as RDF.</p> <p>Parameters:</p> Name Type Description Default <code>container</code> <code>Union[Mapping[str, Any], Sequence[Any]]</code> <p>The container to be saved.  Should be a mapping or sequence.  The <code>load_container()</code> function will deserialise them as dict and list, respectively.</p> required <code>iri</code> <code>str</code> <p>IRI of individual that stands for the container.</p> required <code>lang</code> <code>str</code> <p>Language to use for mapping keys.</p> <code>'en'</code> <code>recognised_keys</code> <code>Optional[Union[Dict, str]]</code> <p>An optional dict that maps mapping keys that correspond to IRIs of recognised RDF properties. If set to the special string \"basic\", the <code>BASIC_RECOGNISED_KEYS</code> module will be used.</p> <code>None</code> <code>keep</code> <code>bool</code> <p>Whether to keep the key-value pair representation for mapping items serialised with recognised_keys.  Note that this may duplicate potential large literal values.</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>List of RDF triples.</p> <p>Note</p> <p><code>container</code> should not be an empty sequence.  The reason for this is that is represented with rdf:nil, which is a single IRI and not a triple.</p> Source code in <code>tripper/convert/convert.py</code> <pre><code>def from_container(\n    container: \"Union[Mapping[str, Any], Sequence[Any]]\",\n    iri: str,\n    lang: str = \"en\",\n    recognised_keys: \"Optional[Union[Dict, str]]\" = None,\n    keep: bool = False,\n) -&gt; list:\n    \"\"\"Serialise a basic Python container type (mapping or sequence) as RDF.\n\n    Arguments:\n        container: The container to be saved.  Should be a mapping or\n            sequence.  The `load_container()` function will deserialise\n            them as dict and list, respectively.\n        iri: IRI of individual that stands for the container.\n        lang: Language to use for mapping keys.\n        recognised_keys: An optional dict that maps mapping keys that\n            correspond to IRIs of recognised RDF properties.\n            If set to the special string \"basic\", the\n            `BASIC_RECOGNISED_KEYS` module will be used.\n        keep: Whether to keep the key-value pair representation for\n            mapping items serialised with recognised_keys.  Note that this\n            may duplicate potential large literal values.\n\n    Returns:\n        List of RDF triples.\n\n    Note:\n        `container` should not be an empty sequence.  The reason for this\n        is that is represented with rdf:nil, which is a single IRI and not\n        a triple.\n    \"\"\"\n    if recognised_keys == \"basic\":\n        recognised_keys = BASIC_RECOGNISED_KEYS\n\n    rdf = []\n\n    def get_obj_iri(obj, uuid):\n        \"\"\"Return IRI for Python object `obj`.  The `uuid` argument is\n        appended to blank nodes for uniques.\"\"\"\n        if isinstance(obj, Mapping):\n            if not obj:\n                return OTEIO.Dictionary\n            obj_iri = f\"_:dict_{uuid}\"\n        elif isinstance(obj, Sequence) and not isinstance(obj, str):\n            if not obj:\n                return RDF.List\n            obj_iri = f\"_:list_{uuid}\"\n        elif obj is None:\n            return OWL.Nothing\n        else:\n            return parse_literal(obj)\n\n        rdf.extend(\n            from_container(\n                obj,\n                obj_iri,\n                lang=lang,\n                recognised_keys=recognised_keys,\n                keep=keep,\n            )\n        )\n        return obj_iri\n\n    if isinstance(container, Sequence):\n        assert not isinstance(container, str)  # nosec\n        if not container:\n            raise ValueError(\"empty sequence is not supported\")\n\n        rdf.append((iri, RDF.type, RDF.List))\n\n        for i, element in enumerate(container):\n            uuid = uuid4()\n            first_iri = get_obj_iri(element, uuid)\n            rest_iri = RDF.nil if i &gt;= len(container) - 1 else f\"_:rest_{uuid}\"\n            rdf.append((iri, RDF.first, first_iri))\n            rdf.append((iri, RDF.rest, rest_iri))\n            iri = rest_iri\n\n    elif isinstance(container, Mapping):\n        rdf.append((iri, RDF.type, OTEIO.Dictionary))\n\n        for key, value in container.items():\n            uuid = uuid4()\n            recognised = recognised_keys and key in recognised_keys\n            value_iri = get_obj_iri(value, uuid)\n            if recognised:\n                rdf.append(\n                    (iri, recognised_keys[key], value_iri)  # type: ignore\n                )\n            if not recognised or keep:\n                key_indv = f\"_:key_{uuid}\"\n                value_indv = f\"_:value_{uuid}\"\n                pair = f\"_:pair_{uuid}\"\n                rdf.extend(\n                    [\n                        (key_indv, RDF.type, OTEIO.DictionaryKey),\n                        (\n                            key_indv,\n                            EMMO.hasStringValue,\n                            Literal(key, lang=lang),\n                        ),\n                        (value_indv, RDF.type, OTEIO.DictionaryValue),\n                        (value_indv, EMMO.hasDataValue, value_iri),\n                        (pair, RDF.type, OTEIO.KeyValuePair),\n                        (pair, OTEIO.hasDictionaryKey, key_indv),\n                        (pair, OTEIO.hasDictionaryValue, value_indv),\n                        (iri, OTEIO.hasKeyValuePair, pair),\n                    ]\n                )\n    else:\n        raise TypeError(\"container must be a mapping or sequence\")\n\n    return rdf\n</code></pre>"},{"location":"api_reference/convert/convert/#tripper.convert.convert.from_dict","title":"<code>from_dict(dct, iri, bases=None, lang='en', recognised_keys=None, keep=False)</code>","text":"<p>Serialise a dict as RDF.</p> <p>Parameters:</p> Name Type Description Default <code>dct</code> <code>dict</code> <p>The dict to be saved.</p> required <code>iri</code> <code>str</code> <p>IRI of individual that stands for the dict.</p> required <code>bases</code> <code>Optional[Sequence]</code> <p>Parent class(es) or the dict.  Unused.</p> <code>None</code> <code>lang</code> <code>str</code> <p>Language to use for keys.</p> <code>'en'</code> <code>recognised_keys</code> <code>Optional[Union[Dict, str]]</code> <p>An optional dict that maps dict keys that correspond to IRIs of recognised RDF properties. If set to the special string \"basic\", the <code>BASIC_RECOGNISED_KEYS</code> module will be used.</p> <code>None</code> <code>keep</code> <code>bool</code> <p>Whether to keep the key-value pair representation for items serialised with recognised_keys.  Note that this will duplicate potential large literal values.</p> <code>False</code> <p>Returns:</p> Type Description <code>list</code> <p>List of RDF triples.</p> Source code in <code>tripper/convert/convert.py</code> <pre><code>def from_dict(\n    dct: dict,\n    iri: str,\n    bases: \"Optional[Sequence]\" = None,\n    lang: str = \"en\",\n    recognised_keys: \"Optional[Union[Dict, str]]\" = None,\n    keep: bool = False,\n) -&gt; list:\n    \"\"\"Serialise a dict as RDF.\n\n    Arguments:\n        dct: The dict to be saved.\n        iri: IRI of individual that stands for the dict.\n        bases: Parent class(es) or the dict.  Unused.\n        lang: Language to use for keys.\n        recognised_keys: An optional dict that maps dict keys that\n            correspond to IRIs of recognised RDF properties.\n            If set to the special string \"basic\", the\n            `BASIC_RECOGNISED_KEYS` module will be used.\n        keep: Whether to keep the key-value pair representation for\n            items serialised with recognised_keys.  Note that this\n            will duplicate potential large literal values.\n\n    Returns:\n        List of RDF triples.\n    \"\"\"\n    del bases  # silence pylint about unused variable\n    warnings.warn(\n        \"from_dict() is deprecated.  Use from_container() instead\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return from_container(\n        dct, iri, lang=lang, recognised_keys=recognised_keys, keep=keep\n    )\n</code></pre>"},{"location":"api_reference/convert/convert/#tripper.convert.convert.load_container","title":"<code>load_container(ts, iri, recognised_keys=None, ignore_unrecognised=False)</code>","text":"<p>Deserialise a Python container object from a triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore from which to fetch the dict.</p> required <code>iri</code> <code>str</code> <p>IRI of individual that stands for the dict to fetch.</p> required <code>recognised_keys</code> <code>Optional[Union[Dict, str]]</code> <p>An optional dict that maps dict keys that correspond to IRIs of recognised RDF properties. If set to the special string \"basic\", the <code>BASIC_RECOGNISED_KEYS</code> module will be used.</p> <code>None</code> <code>ignore_unrecognised</code> <code>bool</code> <p>Ignore relations that has <code>iri</code> as subject and a predicate that is not among the values of <code>recognised_keys</code> or <code>rdf:type</code>.  The default is to raise an exception.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[dict, list]</code> <p>A Python container object corresponding to <code>iri</code>.</p> Source code in <code>tripper/convert/convert.py</code> <pre><code>def load_container(\n    ts: \"Triplestore\",\n    iri: str,\n    recognised_keys: \"Optional[Union[Dict, str]]\" = None,\n    ignore_unrecognised: bool = False,\n) -&gt; \"Union[dict, list]\":\n    \"\"\"Deserialise a Python container object from a triplestore.\n\n    Arguments:\n        ts: Triplestore from which to fetch the dict.\n        iri: IRI of individual that stands for the dict to fetch.\n        recognised_keys: An optional dict that maps dict keys that\n            correspond to IRIs of recognised RDF properties.\n            If set to the special string \"basic\", the\n            `BASIC_RECOGNISED_KEYS` module will be used.\n        ignore_unrecognised: Ignore relations that has `iri` as subject and\n            a predicate that is not among the values of `recognised_keys`\n            or `rdf:type`.  The default is to raise an exception.\n\n    Returns:\n        A Python container object corresponding to `iri`.\n    \"\"\"\n    # pylint: disable=too-many-branches\n    if iri == RDF.nil:\n        return []\n\n    if recognised_keys == \"basic\":\n        recognised_keys = BASIC_RECOGNISED_KEYS\n\n    recognised_iris = (\n        {v: k for k, v in recognised_keys.items()}  # type: ignore\n        if recognised_keys\n        else {}\n    )\n    parents = set(ts.objects(iri, RDF.type))\n\n    def get_obj(value):\n        \"\"\"Return Python object for `value`.\"\"\"\n        value_type = ts.value(value, RDF.type)\n        if value_type == OTEIO.Dictionary:\n            return load_container(ts, value, recognised_keys=recognised_keys)\n        if value_type == RDF.List:\n            return load_container(ts, value, recognised_keys=recognised_keys)\n        if value == OWL.Nothing:\n            return None\n        return value.value if isinstance(value, Literal) else value\n\n    if OTEIO.Dictionary in parents:\n        container = {}\n        for pred, obj in ts.predicate_objects(iri):\n            if pred == OTEIO.hasKeyValuePair:\n                key_iri = ts.value(obj, OTEIO.hasDictionaryKey)\n                key = ts.value(key_iri, EMMO.hasStringValue)\n                value_iri = ts.value(obj, OTEIO.hasDictionaryValue)\n                value = ts.value(value_iri, EMMO.hasDataValue)\n                container[str(key)] = get_obj(value)\n            elif pred in recognised_iris:\n                container[recognised_iris[pred]] = get_obj(obj)\n            elif not ignore_unrecognised and pred not in (\n                RDF.type,\n                RDFS.subClassOf,\n            ):\n                raise ValueError(\n                    f\"Unrecognised predicate '{pred}' in dict: {iri}\"\n                )\n\n        # Recognised IRIs\n        if recognised_keys:\n            iris = {v: k for k, v in recognised_keys.items()}  # type: ignore\n            for _, p, o in ts.triples(subject=iri):\n                key = iris.get(p)  # type: ignore\n                if key and p in iris and key not in container:\n                    container[key] = (\n                        o.value\n                        if isinstance(o, Literal)  # type: ignore\n                        else o\n                    )\n\n    elif RDF.List in parents:\n        container = []  # type: ignore\n        while True:\n            first = ts.value(iri, RDF.first)\n            rest = ts.value(iri, RDF.rest)\n            container.append(get_obj(first))  # type: ignore\n            if rest == RDF.nil:\n                break\n            iri = rest\n\n    else:\n        raise TypeError(\n            f\"iri '{iri}' should be either a rdf:List or an oteio:Dictionary\"\n        )\n\n    return container\n</code></pre>"},{"location":"api_reference/convert/convert/#tripper.convert.convert.load_dict","title":"<code>load_dict(ts, iri, recognised_keys=None)</code>","text":"<p>Deserialise a dict from an RDF triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore from which to fetch the dict.</p> required <code>iri</code> <code>str</code> <p>IRI of individual that stands for the dict to fetch.</p> required <code>recognised_keys</code> <code>Optional[Union[Dict, str]]</code> <p>An optional dict that maps dict keys that correspond to IRIs of recognised RDF properties. If set to the special string \"basic\", the <code>BASIC_RECOGNISED_KEYS</code> module will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[dict, list]</code> <p>A dict corresponding to <code>iri</code>.</p> Source code in <code>tripper/convert/convert.py</code> <pre><code>def load_dict(\n    ts: \"Triplestore\",\n    iri: str,\n    recognised_keys: \"Optional[Union[Dict, str]]\" = None,\n) -&gt; \"Union[dict, list]\":\n    \"\"\"Deserialise a dict from an RDF triplestore.\n\n    Arguments:\n        ts: Triplestore from which to fetch the dict.\n        iri: IRI of individual that stands for the dict to fetch.\n        recognised_keys: An optional dict that maps dict keys that\n            correspond to IRIs of recognised RDF properties.\n            If set to the special string \"basic\", the\n            `BASIC_RECOGNISED_KEYS` module will be used.\n\n    Returns:\n        A dict corresponding to `iri`.\n    \"\"\"\n    warnings.warn(\n        \"load_dict() is deprecated.  Use load_container() instead\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return load_container(ts, iri, recognised_keys=recognised_keys)\n</code></pre>"},{"location":"api_reference/convert/convert/#tripper.convert.convert.save_container","title":"<code>save_container(ts, container, iri, lang='en', recognised_keys=None, keep=False)</code>","text":"<p>Save a basic Python container object to a triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to which to write the container object.</p> required <code>container</code> <code>Union[Mapping[str, Any], Sequence[Any]]</code> <p>The container object to be saved.</p> required <code>iri</code> <code>str</code> <p>IRI of individual that stands for the container object.</p> required <code>lang</code> <code>str</code> <p>Language to use for keys.</p> <code>'en'</code> <code>recognised_keys</code> <code>Optional[Union[Dict, str]]</code> <p>An optional dict that maps dict mapping that correspond to IRIs of recognised RDF properties. If set to the special string \"basic\", the <code>BASIC_RECOGNISED_KEYS</code> module will be used.</p> <code>None</code> <code>keep</code> <code>bool</code> <p>Whether to keep the key-value pair representation for items serialised with recognised_keys.  Note that this will duplicate potential large literal values.</p> <code>False</code> Source code in <code>tripper/convert/convert.py</code> <pre><code>def save_container(\n    ts: \"Triplestore\",\n    container: \"Union[Mapping[str, Any], Sequence[Any]]\",\n    iri: str,\n    lang: str = \"en\",\n    recognised_keys: \"Optional[Union[Dict, str]]\" = None,\n    keep: bool = False,\n) -&gt; None:\n    \"\"\"Save a basic Python container object to a triplestore.\n\n    Arguments:\n        ts: Triplestore to which to write the container object.\n        container: The container object to be saved.\n        iri: IRI of individual that stands for the container object.\n        lang: Language to use for keys.\n        recognised_keys: An optional dict that maps dict mapping that\n            correspond to IRIs of recognised RDF properties.\n            If set to the special string \"basic\", the\n            `BASIC_RECOGNISED_KEYS` module will be used.\n        keep: Whether to keep the key-value pair representation for\n            items serialised with recognised_keys.  Note that this\n            will duplicate potential large literal values.\n    \"\"\"\n    if \"rdf\" not in ts.namespaces:\n        ts.bind(\"rdf\", RDF)\n    if \"dcat\" not in ts.namespaces:\n        ts.bind(\"dcat\", DCAT)\n    if \"emmo\" not in ts.namespaces:\n        ts.bind(\"emmo\", EMMO)\n    if \"oteio\" not in ts.namespaces:\n        ts.bind(\"oteio\", OTEIO)\n\n    ts.add_triples(\n        from_container(\n            container,\n            iri,\n            lang=lang,\n            recognised_keys=recognised_keys,\n            keep=keep,\n        )\n    )\n</code></pre>"},{"location":"api_reference/convert/convert/#tripper.convert.convert.save_dict","title":"<code>save_dict(ts, dct, iri, bases=None, lang='en', recognised_keys=None, keep=False)</code>","text":"<p>Save a dict to a triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to which to write the dict.</p> required <code>dct</code> <code>Mapping[str, Any]</code> <p>The dict to be saved.</p> required <code>iri</code> <code>str</code> <p>IRI of individual that stands for the dict.</p> required <code>bases</code> <code>Optional[Sequence]</code> <p>Parent class(es) or the dict.  Unused.</p> <code>None</code> <code>lang</code> <code>str</code> <p>Language to use for keys.</p> <code>'en'</code> <code>recognised_keys</code> <code>Optional[Union[Dict, str]]</code> <p>An optional dict that maps dict keys that correspond to IRIs of recognised RDF properties. If set to the special string \"basic\", the <code>BASIC_RECOGNISED_KEYS</code> module will be used.</p> <code>None</code> <code>keep</code> <code>bool</code> <p>Whether to keep the key-value pair representation for items serialised with recognised_keys.  Note that this will duplicate potential large literal values.</p> <code>False</code> Source code in <code>tripper/convert/convert.py</code> <pre><code>def save_dict(\n    ts: \"Triplestore\",\n    dct: \"Mapping[str, Any]\",\n    iri: str,\n    bases: \"Optional[Sequence]\" = None,\n    lang: str = \"en\",\n    recognised_keys: \"Optional[Union[Dict, str]]\" = None,\n    keep: bool = False,\n) -&gt; None:\n    \"\"\"Save a dict to a triplestore.\n\n    Arguments:\n        ts: Triplestore to which to write the dict.\n        dct: The dict to be saved.\n        iri: IRI of individual that stands for the dict.\n        bases: Parent class(es) or the dict.  Unused.\n        lang: Language to use for keys.\n        recognised_keys: An optional dict that maps dict keys that\n            correspond to IRIs of recognised RDF properties.\n            If set to the special string \"basic\", the\n            `BASIC_RECOGNISED_KEYS` module will be used.\n        keep: Whether to keep the key-value pair representation for\n            items serialised with recognised_keys.  Note that this\n            will duplicate potential large literal values.\n    \"\"\"\n    del bases  # silence pylint about unused variable\n    warnings.warn(\n        \"save_dict() is deprecated.  Use save_container() instead\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return save_container(\n        ts, dct, iri, lang=lang, recognised_keys=recognised_keys, keep=keep\n    )\n</code></pre>"},{"location":"api_reference/datadoc/clitool/","title":"clitool","text":"<p>A script for data documentation.</p>"},{"location":"api_reference/datadoc/clitool/#tripper.datadoc.clitool.main","title":"<code>main(argv=None)</code>","text":"<p>Main function.</p> Source code in <code>tripper/datadoc/clitool.py</code> <pre><code>def main(argv=None):\n    \"\"\"Main function.\"\"\"\n    retval = maincommand(argv)\n    return 1 if isinstance(retval, Exception) else 0\n</code></pre>"},{"location":"api_reference/datadoc/clitool/#tripper.datadoc.clitool.maincommand","title":"<code>maincommand(argv=None)</code>","text":"<p>Main command.</p> Source code in <code>tripper/datadoc/clitool.py</code> <pre><code>def maincommand(argv=None):\n    \"\"\"Main command.\"\"\"\n    # pylint: disable=too-many-statements\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \"Tool for data documentation.\\n\\n\"\n            \"It allows populating and searching a triplestore for existing \"\n            \"documentation.\"\n        ),\n    )\n\n    subparsers = parser.add_subparsers(required=True, help=\"Subcommands:\")\n\n    # Subcommand: add\n    parser_add = subparsers.add_parser(\n        \"add\",\n        help=\"Populate the triplestore with data documentation.\",\n    )\n    parser_add.set_defaults(func=subcommand_add)\n    parser_add.add_argument(\n        \"input\",\n        help=(\n            \"Path or URL to the input the triplestore should be populated \"\n            \"from.\"\n        ),\n    )\n    parser_add.add_argument(\n        \"--input-format\",\n        \"-i\",\n        choices=[\"yaml\", \"csv\"],\n        help=(\n            \"Input format. By default it is inferred from the file \"\n            \"extension of the `input` argument.\"\n        ),\n    )\n    parser_add.add_argument(\n        \"--csv-option\",\n        action=\"append\",\n        metavar=\"OPTION=VALUE\",\n        help=(\n            \"Options describing the CSV dialect for --input-format=csv. \"\n            \"Common options are 'dialect', 'delimiter' and 'quotechar'. \"\n            \"This option may be provided multiple times.\"\n        ),\n    )\n    parser_add.add_argument(\n        \"--context\",\n        help=\"Path or URL to custom JSON-LD context for the input.\",\n    )\n    parser_add.add_argument(\n        \"--dump\",\n        \"-d\",\n        metavar=\"FILENAME\",\n        help=\"File to dump the populated triplestore to.\",\n    )\n    parser_add.add_argument(\n        \"--format\",\n        \"-f\",\n        default=\"turtle\",\n        help='Format to use with `--dump`.  Default is \"turtle\".',\n    )\n    parser_add.add_argument(\n        \"--keywords\",\n        help=\"Path or URL to custom keywords file for the input.\",\n    )\n    parser_add.add_argument(\n        \"--type\",\n        \"-t\",\n        help=\"Add this type to rows.\",\n    )\n    parser_add.add_argument(\n        \"--redefine\",\n        default=\"raise\",\n        choices=[\"raise\", \"allow\", \"skip\"],\n        help=\"How to handle redifinition of existing keywords.\",\n    )\n\n    # Subcommand: delete\n    parser_delete = subparsers.add_parser(\n        \"delete\", help=\"Delete matching resources in the triplestore.\"\n    )\n    parser_delete.set_defaults(func=subcommand_delete)\n    parser_delete.add_argument(\n        \"--type\",\n        \"-t\",\n        help=(\n            'Either a resource type (ex: \"Dataset\", \"Distribution\", ...) '\n            \"or the IRI of a class to limit the search to.\"\n        ),\n    )\n    parser_delete.add_argument(\n        \"--criteria\",\n        \"-c\",\n        action=\"append\",\n        metavar=\"IRI=VALUE\",\n        help=(\n            \"Matching criteria for resources to delete. The IRI may be \"\n            'written with namespace prefix, like `dcterms:title=\"My title\"`. '\n            'Writing the criteria with the \"=\" operator, corresponds to '\n            \"exact match. \"\n            'If the operator is written \"=~\", regular expression matching '\n            \"will be used instead. This option can be given multiple times.\"\n        ),\n    )\n\n    # Subcommand: find\n    parser_find = subparsers.add_parser(\n        \"find\", help=\"Find documented resources in the triplestore.\"\n    )\n    parser_find.set_defaults(func=subcommand_find)\n    parser_find.add_argument(\n        \"--type\",\n        \"-t\",\n        help=(\n            'Either a resource type (ex: \"dataset\", \"distribution\", ...) '\n            \"or the IRI of a class to limit the search to.\"\n        ),\n    )\n    parser_find.add_argument(\n        \"--criteria\",\n        \"-c\",\n        action=\"append\",\n        metavar=\"IRI=VALUE\",\n        help=(\n            \"Matching criteria for resources to find. The IRI may be written \"\n            'using a namespace prefix, like `dcterms:title=\"My title\"`. '\n            'Writing the criteria with the \"=\" operator, corresponds to '\n            \"exact match. \"\n            'If the operator is written \"=~\", regular expression matching '\n            \"will be used instead. This option can be given multiple times.\"\n        ),\n    )\n    parser_find.add_argument(\n        \"--output\",\n        \"-o\",\n        metavar=\"FILENAME\",\n        help=(\n            \"Write matching output to the given file. The default is to \"\n            \"write to standard output.\"\n        ),\n    )\n    parser_find.add_argument(\n        \"--format\",\n        \"-f\",\n        default=\"iris\",\n        choices=[\"iris\", \"json\", \"turtle\", \"csv\"],\n        help=(\n            \"Output format to list the matched resources. The default is \"\n            \"to infer from the file extension if --output is given. \"\n            'Otherwise it defaults to \"iris\".'\n        ),\n    )\n\n    # Subcommand: fetch\n    parser_fetch = subparsers.add_parser(\n        \"fetch\", help=\"Fetch documented dataset from a storage.\"\n    )\n    parser_fetch.set_defaults(func=subcommand_fetch)\n    parser_fetch.add_argument(\n        \"iri\",\n        help=\"IRI of dataset to fetch.\",\n    )\n    parser_fetch.add_argument(\n        \"--output\",\n        \"-o\",\n        metavar=\"FILENAME\",\n        help=(\n            \"Write the dataset to the given file. The default is to write \"\n            \"to standard output.\"\n        ),\n    )\n\n    # General: options\n    parser.add_argument(\n        \"--config\",\n        \"-c\",\n        help=\"Session configuration file.\",\n    )\n    parser.add_argument(\n        \"--debug\", action=\"store_true\", help=\"Show Python traceback on error.\"\n    )\n    parser.add_argument(\n        \"--triplestore\",\n        \"-t\",\n        help=(\n            \"Name of triplestore to connect to. The name should be defined \"\n            \"in the session configuration file.\"\n        ),\n    )\n    parser.add_argument(\n        \"--backend\",\n        \"-b\",\n        default=\"rdflib\",\n        help=(\n            'Triplestore backend to use. Defaults to \"rdflib\" - an '\n            \"in-memory rdflib triplestore, that can be pre-loaded with \"\n            \"--parse.\"\n        ),\n    )\n    parser.add_argument(\n        \"--base-iri\",\n        \"-B\",\n        help=\"Base IRI of the triplestore.\",\n    )\n    parser.add_argument(\n        \"--database\",\n        \"-d\",\n        help=\"Name of database to connect to (for backends supporting it).\",\n    )\n    parser.add_argument(\n        \"--package\",\n        help=\"Only needed when `backend` is a relative module.\",\n    )\n    parser.add_argument(\n        \"--parse\",\n        \"-p\",\n        metavar=\"LOCATION\",\n        help=\"Load triplestore from this location.\",\n    )\n    parser.add_argument(\n        \"--parse-format\",\n        \"-F\",\n        help=\"Used with `--parse`. Format to use when parsing triplestore.\",\n    )\n    parser.add_argument(\n        \"--prefix\",\n        \"-P\",\n        action=\"append\",\n        metavar=\"PREFIX=URL\",\n        help=(\n            \"Namespace prefix to bind to the triplestore. \"\n            \"This option can be given multiple times.\"\n        ),\n    )\n\n    args = parser.parse_args(argv)\n\n    if args.triplestore:\n        session = Session(config=args.config)\n        ts = session.get_triplestore(args.triplestore)\n    else:\n        ts = Triplestore(\n            backend=args.backend,\n            base_iri=args.base_iri,\n            database=args.database,\n            package=args.package,\n        )\n\n    if args.parse:\n        ts.parse(args.parse, format=args.parse_format)\n\n    if args.prefix:\n        for token in args.prefix:\n            prefix, ns = token.split(\"=\", 1)\n            ts.bind(prefix, ns)\n\n    # Call subcommand handler\n    try:\n        return args.func(ts, args)\n    except Exception as exc:  # pylint: disable=broad-exception-caught\n        if args.debug:\n            raise\n        print(f\"{exc.__class__.__name__}: {exc}\")\n        return exc\n</code></pre>"},{"location":"api_reference/datadoc/clitool/#tripper.datadoc.clitool.subcommand_add","title":"<code>subcommand_add(ts, args)</code>","text":"<p>Subcommand for populating the triplestore</p> Source code in <code>tripper/datadoc/clitool.py</code> <pre><code>def subcommand_add(ts, args):\n    \"\"\"Subcommand for populating the triplestore\"\"\"\n    infile = Path(args.input)\n    extension = args.input_format if args.input_format else infile.suffix\n    fmt = extension.lower().lstrip(\".\")\n\n    if fmt in (\"yml\", \"yaml\"):\n        save_datadoc(ts, infile)\n    elif fmt in (\"csv\",):\n        kw = {}\n        if args.csv_option:\n            for token in args.csv_option:\n                option, value = token.split(\"=\", 1)\n                kw[option] = value\n        td = TableDoc.parse_csv(\n            infile,\n            type=args.type,\n            keywords=args.keywords,\n            context=args.context,\n            redefine=args.redefine,\n            **kw,\n        )\n        td.save(ts)\n    else:\n        raise ValueError(f\"Unknown input format: {fmt}\")\n\n    if args.dump:\n        ts.serialize(args.dump, format=args.format)\n</code></pre>"},{"location":"api_reference/datadoc/clitool/#tripper.datadoc.clitool.subcommand_delete","title":"<code>subcommand_delete(ts, args)</code>","text":"<p>Subcommand for removing matching entries in the triplestore.</p> Source code in <code>tripper/datadoc/clitool.py</code> <pre><code>def subcommand_delete(ts, args):\n    \"\"\"Subcommand for removing matching entries in the triplestore.\"\"\"\n    criteria = {}\n    regex = {}\n    if args.criteria:\n        for crit in args.criteria:\n            if \"=~\" in crit:\n                key, value = crit.split(\"=~\", 1)\n                regex[key] = value\n            else:\n                key, value = crit.split(\"=\", 1)\n                criteria[key] = value\n    delete(ts, type=args.type, criteria=criteria, regex=regex)\n</code></pre>"},{"location":"api_reference/datadoc/clitool/#tripper.datadoc.clitool.subcommand_fetch","title":"<code>subcommand_fetch(ts, args)</code>","text":"<p>Subcommand for fetching a documented dataset from a storage.</p> Source code in <code>tripper/datadoc/clitool.py</code> <pre><code>def subcommand_fetch(ts, args):\n    \"\"\"Subcommand for fetching a documented dataset from a storage.\"\"\"\n    data = load(ts, args.iri)\n\n    if args.output:\n        with open(args.output, \"wb\") as f:\n            f.write(data)\n    else:\n        print(data)\n\n    return data\n</code></pre>"},{"location":"api_reference/datadoc/clitool/#tripper.datadoc.clitool.subcommand_find","title":"<code>subcommand_find(ts, args)</code>","text":"<p>Subcommand for finding IRIs in the triplestore.</p> Source code in <code>tripper/datadoc/clitool.py</code> <pre><code>def subcommand_find(ts, args):\n    \"\"\"Subcommand for finding IRIs in the triplestore.\"\"\"\n    criteria = {}\n    regex = {}\n    if args.criteria:\n        for crit in args.criteria:\n            if \"=~\" in crit:\n                key, value = crit.split(\"=~\", 1)\n                regex[key] = value\n            else:\n                key, value = crit.split(\"=\", 1)\n                criteria[key] = value\n\n    iris = search(ts, type=args.type, criteria=criteria, regex=regex)\n\n    # Infer format\n    if args.format:\n        fmt = args.format.lower()\n    elif args.output:\n        fmt = Path(args.output).suffix.lower().lstrip(\".\")\n    else:\n        fmt = \"iris\"\n\n    # Create output\n    if fmt in (\"iris\", \"txt\"):\n        s = \"\\n\".join(iris)\n    elif fmt == \"json\":\n        s = json.dumps(\n            [acquire(ts, iri) for iri in iris if not iri.startswith(\"_:\")],\n            indent=2,\n        )\n    elif fmt in (\"turtle\", \"ttl\"):\n        ts2 = Triplestore(\"rdflib\")\n        for iri in iris:\n            d = acquire(ts, iri)\n            store(ts2, d)\n        s = ts2.serialize()\n    elif fmt == \"csv\":\n        dicts = [acquire(ts, iri) for iri in iris]\n        td = TableDoc.fromdicts(dicts)\n        with io.StringIO() as f:\n            td.write_csv(f, prefixes=ts.namespaces)\n            s = f.getvalue()\n    else:\n        raise ValueError(f\"Unknown format: {fmt}\")\n\n    if args.output:\n        with open(args.output, \"wt\", encoding=\"utf-8\") as f:\n            f.write(s + os.linesep)\n    else:\n        print(s)\n\n    return s\n</code></pre>"},{"location":"api_reference/datadoc/context/","title":"context","text":"<p>Parse and work with JSON-LD context.</p>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context","title":"<code> Context        </code>","text":"<p>A class representing a context.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>class Context:\n    \"\"\"A class representing a context.\"\"\"\n\n    def __init__(\n        self,\n        context: \"Optional[ContextType]\" = None,\n        theme: \"Optional[Union[str, Sequence[str]]]\" = \"ddoc:datadoc\",\n        keywords: \"Optional[Keywords]\" = None,\n        processingMode: str = \"json-ld-1.1\",\n        timeout: float = 3,\n    ) -&gt; None:\n        \"\"\"Initialises context object.\n\n        Arguments:\n            context: Optional context to load. Supported types:\n                - Context: The Context instance will be updated and returned,\n                  unless `copy` is true.\n                - dict: Dict-representation of a valid JSON-LD context.\n                - str: If a valid URI, the context is loaded from this URI,\n                  otherwise it is assumed to be a file path to load.\n                - sequence: A sequence of the above.\n            theme: Load initial context for this theme.\n            keywords: Initialise from this keywords instance.\n            processingMode: Either \"json-ld-1.0\" or \"json-ld-1.1\".\n            timeout: Timeout when accessing remote files.\n\n        \"\"\"\n        # pylint: disable=import-outside-toplevel\n        from tripper.datadoc.keywords import Keywords\n\n        self.ld = jsonld.JsonLdProcessor()\n        self.ctx = self.ld._get_initial_context(\n            options={\"processingMode\": processingMode}\n        )\n        self.timeout = timeout\n        self._expanded: dict = {}\n        self._prefixed: dict = {}\n        self._shortnamed: dict = {}\n\n        if keywords is not None:\n            if theme:\n                keywords.add_theme(theme)\n            self.add_context(keywords.get_context())\n        elif theme:\n            keywords = Keywords(theme=theme)\n            self.add_context(keywords.get_context())\n\n        if context:\n            self.add_context(context)\n\n    def __contains__(self, item):\n        self._create_caches()\n        return item in self._expanded\n\n    def __getitem__(self, key):\n        self._create_caches()\n        return self._expanded[key]\n\n    def __iter__(self):\n        self._create_caches()\n        return iter(set(self._shortnamed.values()))\n\n    def __dir__(self):\n        self._create_caches()\n        return dir(Context) + [\"ld\", \"ctx\"]\n\n    ## def __repr__(self):\n    ##     js = json.dumps(self.get_context_dict(), indent=4)\n    ##     return f\"Context(context={js})\"\n\n    def __str__(self):\n        return json.dumps({\"@context\": self.get_context_dict()}, indent=2)\n\n    def copy(self) -&gt; \"Context\":\n        \"\"\"Return a copy of this context.\"\"\"\n        copy = Context(theme=None)\n        copy.ctx = self.ctx  # frozendict - no need to copy\n        return copy\n\n    def add_context(self, context: \"ContextType\") -&gt; None:\n        \"\"\"Add a context to this object.\"\"\"\n        if isinstance(context, Context):\n            context = context.get_context_dict()\n        elif isinstance(context, str):\n            with openfile(context, \"rt\", timeout=self.timeout) as f:\n                context = json.load(f)\n        elif isinstance(context, Sequence):\n            for c in context:\n                self.add_context(c)\n            return\n        elif isinstance(context, dict):\n            pass\n        else:\n            raise TypeError(\n                \"`context` should be a dict, str, Context or a sequence of \"\n                f\"these.  Got: {type(context)}\"\n            )\n\n        if \"@id\" in context:\n            if \"@context\" in context:\n                context = context[\"@context\"]  # type: ignore\n            else:\n                r = repr(context)\n                c = f\"{r}...\" if len(r) &gt; 100 else r\n                raise InvalidContextError(\n                    f\"context cannot have an @id key: {c}\"\n                )\n\n        def rec(dct):\n            \"\"\"Return a copy of `dct` with all empty key names replaced\n            with \"@base\".\"\"\"\n            if not isinstance(dct, dict):\n                return dct\n\n            d = dct.copy()\n            for k, v in dct.items():\n                if k == \"\":\n                    if isinstance(v, str):\n                        d[\"@base\"] = d.pop(\"\")\n                    else:\n                        raise InvalidContextError(\n                            f\"empty key with non-string value: {v}\"\n                        )\n                if isinstance(v, dict):\n                    d[k] = rec(v)\n            return d\n\n        self.ctx = self.ld.process_context(self.ctx, rec(context), options={})\n\n        # Clear caches\n        self._expanded.clear()\n        self._prefixed.clear()\n        self._shortnamed.clear()\n\n    def get_context_dict(self) -&gt; dict:\n        \"\"\"Return a context dict.\"\"\"\n        context = {}\n        if \"@base\" in self.ctx and self.ctx[\"@base\"]:\n            context[\"@base\"] = self.ctx[\"@base\"]\n        if \"@vocab\" in self.ctx and self.ctx[\"@vocab\"]:\n            context[\"@vocab\"] = self.ctx[\"@vocab\"]\n        for name, info in self.ctx[\"mappings\"].items():\n            if \"@type\" in info:\n                context[name] = {\n                    \"@id\": info[\"@id\"],\n                    \"@type\": info[\"@type\"],\n                }\n            else:\n                context[name] = info[\"@id\"]\n        return context\n\n    # def get_context(self) -&gt; dict:\n    #     \"\"\"Return a context dict.\"\"\"\n    #     return {k: v.get(\"@id\") for k, v in self.ctx[\"mappings\"].items()}\n\n    def get_mappings(self) -&gt; dict:\n        \"\"\"Return a dict mapping keywords to IRIs.\"\"\"\n        return {\n            k: v[\"@id\"]\n            for k, v in self.ctx[\"mappings\"].items()\n            if v.get(\"_prefix\") is False and \"@id\" in v\n        }\n\n    def get_prefixes(self) -&gt; dict:\n        \"\"\"Return a dict mapping prefixes to IRIs.\"\"\"\n        prefixes = {\"\": self.base} if self.base else {}\n        for k, v in self.ctx[\"mappings\"].items():\n            if v.get(\"_prefix\") and \"@id\" in v:\n                prefixes[k] = v[\"@id\"]\n        return prefixes\n\n    def sync_prefixes(\n        self, ts: Triplestore, update: \"Optional[bool]\" = None\n    ) -&gt; None:\n        \"\"\"Syncronise prefixes between context and triplestore.\n\n        The `update` option controls how prefix inconsistencies are handeled.\n        If `update` is:\n\n        - True:  Prefixes in the context will be updated.\n        - False: Prefixes in the triplestore will be updated.\n        - None:  A PrefixMismatchError is raised.\n\n        \"\"\"\n        ns1 = self.get_prefixes().copy()\n        ns2 = {pf: str(ns) for pf, ns in ts.namespaces.items()}\n\n        if update is None:\n            mismatch = [\n                p for p in set(ns1).intersection(ns2) if ns1[p] != ns2[p]\n            ]\n            if mismatch:\n                msg = [\"Mismatch in definition of prefix(es): \"]\n                for mis in mismatch:\n                    msg.extend(\n                        [\n                            f\"* Prefix '{mis}' is defined as\",\n                            f\"  - {ns1[mis]} in context\",\n                            f\"  - {ns2[mis]} in triplestore\",\n                        ]\n                    )\n                raise PrefixMismatchError(os.linesep.join(msg))\n\n        if update:\n            ns1.update(ns2)\n            ns2 = ns1\n        else:\n            ns2.update(ns1)\n            ns1 = ns2\n\n        self.add_context(ns2)\n\n        for prefix, ns in ns1.items():\n            if prefix not in ts.namespaces:\n                ts.bind(prefix, ns)\n\n    def expand(self, name: str, strict: bool = False) -&gt; str:\n        \"\"\"Return `name` expanded to a full IRI.\n\n        If `name` is not defined in the context, a `NameSpaceError` will\n        be raised if `strict` is true. Otherwise `name` will be returned\n        unchanged.\n\n        Example:\n\n        &gt;&gt;&gt; context = Context()\n        &gt;&gt;&gt; context.expand(\"Dataset\")\n        'http://www.w3.org/ns/dcat#Dataset'\n\n        \"\"\"\n        # Check cache\n        if self._expanded and name in self._expanded:\n            return self._expanded[name]\n        # Check ctx\n        if name in self.ctx[\"mappings\"]:\n            return self.ctx[\"mappings\"][name][\"@id\"]\n        # Check if name is already expanded\n        if re.match(MATCH_IRI, name):\n            return name\n        # Check if prefixed\n        if re.match(MATCH_PREFIXED_IRI, name):\n            prefix, shortname = name.split(\":\", 1)\n            prefixes = self.get_prefixes()\n            if prefix in prefixes:\n                return f\"{prefixes[prefix]}{shortname}\"\n        # Name not defined in context\n        if strict:\n            raise NamespaceError(f\"cannot expand: {name}\")\n        return name\n\n    def prefixed(self, name: str, strict: bool = True) -&gt; str:\n        \"\"\"Return `name` as a prefixed IRI.\n\n        If `name` is not defined in the context, a `NameSpaceError` will\n        be raised if `strict` is true. Otherwise `name` will be returned\n        unchanged.\n\n        Example:\n\n        &gt;&gt;&gt; context = Context()\n        &gt;&gt;&gt; context.prefixed(\"Dataset\")\n        'dcat:Dataset'\n\n        \"\"\"\n        if not self._prefixed:\n            self._create_caches()\n        if name in self._prefixed:\n            return self._prefixed[name]\n        if strict:\n            raise NamespaceError(f\"cannot prefix: {name}\")\n        return name\n\n    def shortname(self, name: str, strict: bool = True) -&gt; str:\n        \"\"\"Return the short name (keyword) corresponding to `name`.\n\n        If `name` is not defined in the context, a `NameSpaceError` will\n        be raised if `strict` is true. Otherwise `name` will be returned\n        unchanged.\n\n        Example:\n\n        &gt;&gt;&gt; context = Context()\n        &gt;&gt;&gt; context.shortname(\"dcat:Dataset\")\n        'Dataset'\n\n        \"\"\"\n        if not self._shortnamed:\n            self._create_caches()\n        if name in self._shortnamed:\n            return self._shortnamed[name]\n        if strict:\n            raise NamespaceError(f\"no short name for: {name}\")\n        return name\n\n    def getdef(self, name: str) -&gt; dict:\n        \"\"\"Return JSON-LD definition of `name`.\"\"\"\n        shortname = self.shortname(name)\n        return self.ctx[\"mappings\"][shortname]\n\n    def isref(self, name: str) -&gt; bool:\n        \"\"\"Return wheter `name` is an object property that refers to a node.\"\"\"\n        return self.getdef(name).get(\"@type\") == \"@id\"\n\n    def is_object_property(self, name: str) -&gt; bool:\n        \"\"\"Whether `name` appears to be an object property or not.\"\"\"\n        if (\n            name\n            in (\n                \"type\",\n                \"rdf:type\",\n                RDF.type,\n                \"subClassOf\",\n                \"rdfs:subClassOf\",\n                RDFS.subClassOf,\n                \"subPropertyOf\",\n                \"rdfs:subPropertyOf\",\n                RDFS.subPropertyOf,\n            )\n            or name not in self\n        ):\n            return False\n        return self.getdef(name).get(\"@type\") == \"@id\"\n\n    def is_data_property(self, name: str) -&gt; bool:\n        \"\"\"Returns whether `name` appears to be a data property.\"\"\"\n        if name not in self or (\n            self.is_object_property(name)\n            or self.is_annotation_property(name)\n            or self.is_class(name)\n        ):\n            return False\n        type = self.getdef(name).get(\"@type\")\n        return bool(type) and type not in (\"@id\", RDFS.Class, OWL.Class)\n\n    def is_annotation_property(self, name: str) -&gt; bool:\n        \"\"\"Returns whether `name` appears to be an annotation property.\"\"\"\n        if name not in self:\n            return False\n        d = self.getdef(name)\n        return \"@type\" not in d or \"@language\" in d\n\n    def is_class(self, name: str) -&gt; bool:\n        \"\"\"Returns whether `name` appears to be a class.\"\"\"\n        return name in self and self.getdef(name).get(\"@type\") in (\n            RDFS.Class,\n            OWL.Class,\n        )\n\n    def expanddoc(self, doc: \"Union[dict, list]\") -&gt; list:\n        \"\"\"Return expanded JSON-LD document `doc`.\"\"\"\n        return self.ld.expand(self._todict(doc), options={})\n\n    def compactdoc(self, doc: dict) -&gt; dict:\n        \"\"\"Return compacted JSON-LD document `doc`.\"\"\"\n        return self.ld.compact(doc, self.get_context_dict(), options={})\n\n    def to_triplestore(self, ts, doc: \"Union[dict, list]\"):\n        \"\"\"Store JSON-LD document `doc` to triplestore `ts`.\"\"\"\n        nt = jsonld.to_rdf(\n            self._todict(doc), options={\"format\": \"application/n-quads\"}\n        )\n        ts.parse(data=nt, format=\"ntriples\")\n\n        if isinstance(doc, dict) and \"@context\" in doc:\n            ctx = self.copy()\n            ctx.add_context(doc)\n        else:\n            ctx = self\n        for prefix, ns in ctx.get_prefixes().items():\n            if prefix not in ts.namespaces:\n                ts.bind(prefix, ns)\n\n    def _todict(self, doc: \"Union[dict, list]\") -&gt; dict:\n        \"\"\"Returns a shallow copy of doc as a dict with current\n        context added.\"\"\"\n        if isinstance(doc, list):\n            return {\n                \"@context\": self.get_context_dict(),\n                \"@graph\": doc,\n            }\n\n        if \"@context\" in doc:\n            ctx = self.copy()\n            ctx.add_context(doc[\"@context\"])\n            new = doc.copy()\n            new[\"@context\"] = ctx.get_context_dict()\n        else:\n            new = {\"@context\": self.get_context_dict()}\n            new.update(doc)\n        return new\n\n    def _create_caches(self) -&gt; None:\n        \"\"\"Create _expanded dict cached.\"\"\"\n        if self._expanded:\n            return\n        prefixes = self.get_prefixes()\n        mappings = self.get_mappings()\n        self._expanded[\"@type\"] = RDF.type\n        self._expanded[\"rdf:type\"] = RDF.type\n        self._expanded[RDF.type] = RDF.type\n        self._prefixed[RDF.type] = \"rdf:type\"\n        self._prefixed[\"rdf:type\"] = \"rdf:type\"\n        self._prefixed[\"@type\"] = \"rdf:type\"\n        self._shortnamed[RDF.type] = \"@type\"\n        self._shortnamed[\"rdf:type\"] = \"@type\"\n        self._shortnamed[\"@type\"] = \"@type\"\n        for key, expanded in mappings.items():\n            prefixed = prefix_iri(expanded, prefixes)\n            self._update_caches(key, prefixed, expanded)\n\n    def _update_caches(self, shortname, prefixed, expanded):\n        self._expanded[shortname] = expanded\n        self._expanded[prefixed] = expanded\n        self._expanded[expanded] = expanded\n        self._prefixed[shortname] = prefixed\n        self._prefixed[prefixed] = prefixed\n        self._prefixed[expanded] = prefixed\n        self._shortnamed[shortname] = shortname\n        self._shortnamed[prefixed] = shortname\n        self._shortnamed[expanded] = shortname\n\n    base = property(\n        fget=lambda self: self.ctx.get(\"@base\"),\n        fset=lambda self, ns: self.add_context({\"@base\": ns}),\n        doc=\"Base IRI against which to resolve those relative IRIs.\",\n    )\n    processingMode = property(\n        lambda self: self.ctx.get(\"processingMode\"),\n        doc=\"Tag for JSON-LD version that this context is processed against.\",\n    )\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.base","title":"<code>base</code>  <code>property</code> <code>writable</code>","text":"<p>Base IRI against which to resolve those relative IRIs.</p>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.processingMode","title":"<code>processingMode</code>  <code>property</code> <code>readonly</code>","text":"<p>Tag for JSON-LD version that this context is processed against.</p>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.__init__","title":"<code>__init__(self, context=None, theme='ddoc:datadoc', keywords=None, processingMode='json-ld-1.1', timeout=3)</code>  <code>special</code>","text":"<p>Initialises context object.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Optional[ContextType]</code> <p>Optional context to load. Supported types: - Context: The Context instance will be updated and returned,   unless <code>copy</code> is true. - dict: Dict-representation of a valid JSON-LD context. - str: If a valid URI, the context is loaded from this URI,   otherwise it is assumed to be a file path to load. - sequence: A sequence of the above.</p> <code>None</code> <code>theme</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>Load initial context for this theme.</p> <code>'ddoc:datadoc'</code> <code>keywords</code> <code>Optional[Keywords]</code> <p>Initialise from this keywords instance.</p> <code>None</code> <code>processingMode</code> <code>str</code> <p>Either \"json-ld-1.0\" or \"json-ld-1.1\".</p> <code>'json-ld-1.1'</code> <code>timeout</code> <code>float</code> <p>Timeout when accessing remote files.</p> <code>3</code> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def __init__(\n    self,\n    context: \"Optional[ContextType]\" = None,\n    theme: \"Optional[Union[str, Sequence[str]]]\" = \"ddoc:datadoc\",\n    keywords: \"Optional[Keywords]\" = None,\n    processingMode: str = \"json-ld-1.1\",\n    timeout: float = 3,\n) -&gt; None:\n    \"\"\"Initialises context object.\n\n    Arguments:\n        context: Optional context to load. Supported types:\n            - Context: The Context instance will be updated and returned,\n              unless `copy` is true.\n            - dict: Dict-representation of a valid JSON-LD context.\n            - str: If a valid URI, the context is loaded from this URI,\n              otherwise it is assumed to be a file path to load.\n            - sequence: A sequence of the above.\n        theme: Load initial context for this theme.\n        keywords: Initialise from this keywords instance.\n        processingMode: Either \"json-ld-1.0\" or \"json-ld-1.1\".\n        timeout: Timeout when accessing remote files.\n\n    \"\"\"\n    # pylint: disable=import-outside-toplevel\n    from tripper.datadoc.keywords import Keywords\n\n    self.ld = jsonld.JsonLdProcessor()\n    self.ctx = self.ld._get_initial_context(\n        options={\"processingMode\": processingMode}\n    )\n    self.timeout = timeout\n    self._expanded: dict = {}\n    self._prefixed: dict = {}\n    self._shortnamed: dict = {}\n\n    if keywords is not None:\n        if theme:\n            keywords.add_theme(theme)\n        self.add_context(keywords.get_context())\n    elif theme:\n        keywords = Keywords(theme=theme)\n        self.add_context(keywords.get_context())\n\n    if context:\n        self.add_context(context)\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.add_context","title":"<code>add_context(self, context)</code>","text":"<p>Add a context to this object.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def add_context(self, context: \"ContextType\") -&gt; None:\n    \"\"\"Add a context to this object.\"\"\"\n    if isinstance(context, Context):\n        context = context.get_context_dict()\n    elif isinstance(context, str):\n        with openfile(context, \"rt\", timeout=self.timeout) as f:\n            context = json.load(f)\n    elif isinstance(context, Sequence):\n        for c in context:\n            self.add_context(c)\n        return\n    elif isinstance(context, dict):\n        pass\n    else:\n        raise TypeError(\n            \"`context` should be a dict, str, Context or a sequence of \"\n            f\"these.  Got: {type(context)}\"\n        )\n\n    if \"@id\" in context:\n        if \"@context\" in context:\n            context = context[\"@context\"]  # type: ignore\n        else:\n            r = repr(context)\n            c = f\"{r}...\" if len(r) &gt; 100 else r\n            raise InvalidContextError(\n                f\"context cannot have an @id key: {c}\"\n            )\n\n    def rec(dct):\n        \"\"\"Return a copy of `dct` with all empty key names replaced\n        with \"@base\".\"\"\"\n        if not isinstance(dct, dict):\n            return dct\n\n        d = dct.copy()\n        for k, v in dct.items():\n            if k == \"\":\n                if isinstance(v, str):\n                    d[\"@base\"] = d.pop(\"\")\n                else:\n                    raise InvalidContextError(\n                        f\"empty key with non-string value: {v}\"\n                    )\n            if isinstance(v, dict):\n                d[k] = rec(v)\n        return d\n\n    self.ctx = self.ld.process_context(self.ctx, rec(context), options={})\n\n    # Clear caches\n    self._expanded.clear()\n    self._prefixed.clear()\n    self._shortnamed.clear()\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.compactdoc","title":"<code>compactdoc(self, doc)</code>","text":"<p>Return compacted JSON-LD document <code>doc</code>.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def compactdoc(self, doc: dict) -&gt; dict:\n    \"\"\"Return compacted JSON-LD document `doc`.\"\"\"\n    return self.ld.compact(doc, self.get_context_dict(), options={})\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.copy","title":"<code>copy(self)</code>","text":"<p>Return a copy of this context.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def copy(self) -&gt; \"Context\":\n    \"\"\"Return a copy of this context.\"\"\"\n    copy = Context(theme=None)\n    copy.ctx = self.ctx  # frozendict - no need to copy\n    return copy\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.expand","title":"<code>expand(self, name, strict=False)</code>","text":"<p>Return <code>name</code> expanded to a full IRI.</p> <p>If <code>name</code> is not defined in the context, a <code>NameSpaceError</code> will be raised if <code>strict</code> is true. Otherwise <code>name</code> will be returned unchanged.</p> <p>Examples:</p> <p>context = Context() context.expand(\"Dataset\") 'http://www.w3.org/ns/dcat#Dataset'</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def expand(self, name: str, strict: bool = False) -&gt; str:\n    \"\"\"Return `name` expanded to a full IRI.\n\n    If `name` is not defined in the context, a `NameSpaceError` will\n    be raised if `strict` is true. Otherwise `name` will be returned\n    unchanged.\n\n    Example:\n\n    &gt;&gt;&gt; context = Context()\n    &gt;&gt;&gt; context.expand(\"Dataset\")\n    'http://www.w3.org/ns/dcat#Dataset'\n\n    \"\"\"\n    # Check cache\n    if self._expanded and name in self._expanded:\n        return self._expanded[name]\n    # Check ctx\n    if name in self.ctx[\"mappings\"]:\n        return self.ctx[\"mappings\"][name][\"@id\"]\n    # Check if name is already expanded\n    if re.match(MATCH_IRI, name):\n        return name\n    # Check if prefixed\n    if re.match(MATCH_PREFIXED_IRI, name):\n        prefix, shortname = name.split(\":\", 1)\n        prefixes = self.get_prefixes()\n        if prefix in prefixes:\n            return f\"{prefixes[prefix]}{shortname}\"\n    # Name not defined in context\n    if strict:\n        raise NamespaceError(f\"cannot expand: {name}\")\n    return name\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.expanddoc","title":"<code>expanddoc(self, doc)</code>","text":"<p>Return expanded JSON-LD document <code>doc</code>.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def expanddoc(self, doc: \"Union[dict, list]\") -&gt; list:\n    \"\"\"Return expanded JSON-LD document `doc`.\"\"\"\n    return self.ld.expand(self._todict(doc), options={})\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.get_context_dict","title":"<code>get_context_dict(self)</code>","text":"<p>Return a context dict.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def get_context_dict(self) -&gt; dict:\n    \"\"\"Return a context dict.\"\"\"\n    context = {}\n    if \"@base\" in self.ctx and self.ctx[\"@base\"]:\n        context[\"@base\"] = self.ctx[\"@base\"]\n    if \"@vocab\" in self.ctx and self.ctx[\"@vocab\"]:\n        context[\"@vocab\"] = self.ctx[\"@vocab\"]\n    for name, info in self.ctx[\"mappings\"].items():\n        if \"@type\" in info:\n            context[name] = {\n                \"@id\": info[\"@id\"],\n                \"@type\": info[\"@type\"],\n            }\n        else:\n            context[name] = info[\"@id\"]\n    return context\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.get_mappings","title":"<code>get_mappings(self)</code>","text":"<p>Return a dict mapping keywords to IRIs.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def get_mappings(self) -&gt; dict:\n    \"\"\"Return a dict mapping keywords to IRIs.\"\"\"\n    return {\n        k: v[\"@id\"]\n        for k, v in self.ctx[\"mappings\"].items()\n        if v.get(\"_prefix\") is False and \"@id\" in v\n    }\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.get_prefixes","title":"<code>get_prefixes(self)</code>","text":"<p>Return a dict mapping prefixes to IRIs.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def get_prefixes(self) -&gt; dict:\n    \"\"\"Return a dict mapping prefixes to IRIs.\"\"\"\n    prefixes = {\"\": self.base} if self.base else {}\n    for k, v in self.ctx[\"mappings\"].items():\n        if v.get(\"_prefix\") and \"@id\" in v:\n            prefixes[k] = v[\"@id\"]\n    return prefixes\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.getdef","title":"<code>getdef(self, name)</code>","text":"<p>Return JSON-LD definition of <code>name</code>.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def getdef(self, name: str) -&gt; dict:\n    \"\"\"Return JSON-LD definition of `name`.\"\"\"\n    shortname = self.shortname(name)\n    return self.ctx[\"mappings\"][shortname]\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.is_annotation_property","title":"<code>is_annotation_property(self, name)</code>","text":"<p>Returns whether <code>name</code> appears to be an annotation property.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def is_annotation_property(self, name: str) -&gt; bool:\n    \"\"\"Returns whether `name` appears to be an annotation property.\"\"\"\n    if name not in self:\n        return False\n    d = self.getdef(name)\n    return \"@type\" not in d or \"@language\" in d\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.is_class","title":"<code>is_class(self, name)</code>","text":"<p>Returns whether <code>name</code> appears to be a class.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def is_class(self, name: str) -&gt; bool:\n    \"\"\"Returns whether `name` appears to be a class.\"\"\"\n    return name in self and self.getdef(name).get(\"@type\") in (\n        RDFS.Class,\n        OWL.Class,\n    )\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.is_data_property","title":"<code>is_data_property(self, name)</code>","text":"<p>Returns whether <code>name</code> appears to be a data property.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def is_data_property(self, name: str) -&gt; bool:\n    \"\"\"Returns whether `name` appears to be a data property.\"\"\"\n    if name not in self or (\n        self.is_object_property(name)\n        or self.is_annotation_property(name)\n        or self.is_class(name)\n    ):\n        return False\n    type = self.getdef(name).get(\"@type\")\n    return bool(type) and type not in (\"@id\", RDFS.Class, OWL.Class)\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.is_object_property","title":"<code>is_object_property(self, name)</code>","text":"<p>Whether <code>name</code> appears to be an object property or not.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def is_object_property(self, name: str) -&gt; bool:\n    \"\"\"Whether `name` appears to be an object property or not.\"\"\"\n    if (\n        name\n        in (\n            \"type\",\n            \"rdf:type\",\n            RDF.type,\n            \"subClassOf\",\n            \"rdfs:subClassOf\",\n            RDFS.subClassOf,\n            \"subPropertyOf\",\n            \"rdfs:subPropertyOf\",\n            RDFS.subPropertyOf,\n        )\n        or name not in self\n    ):\n        return False\n    return self.getdef(name).get(\"@type\") == \"@id\"\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.isref","title":"<code>isref(self, name)</code>","text":"<p>Return wheter <code>name</code> is an object property that refers to a node.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def isref(self, name: str) -&gt; bool:\n    \"\"\"Return wheter `name` is an object property that refers to a node.\"\"\"\n    return self.getdef(name).get(\"@type\") == \"@id\"\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.prefixed","title":"<code>prefixed(self, name, strict=True)</code>","text":"<p>Return <code>name</code> as a prefixed IRI.</p> <p>If <code>name</code> is not defined in the context, a <code>NameSpaceError</code> will be raised if <code>strict</code> is true. Otherwise <code>name</code> will be returned unchanged.</p> <p>Examples:</p> <p>context = Context() context.prefixed(\"Dataset\") 'dcat:Dataset'</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def prefixed(self, name: str, strict: bool = True) -&gt; str:\n    \"\"\"Return `name` as a prefixed IRI.\n\n    If `name` is not defined in the context, a `NameSpaceError` will\n    be raised if `strict` is true. Otherwise `name` will be returned\n    unchanged.\n\n    Example:\n\n    &gt;&gt;&gt; context = Context()\n    &gt;&gt;&gt; context.prefixed(\"Dataset\")\n    'dcat:Dataset'\n\n    \"\"\"\n    if not self._prefixed:\n        self._create_caches()\n    if name in self._prefixed:\n        return self._prefixed[name]\n    if strict:\n        raise NamespaceError(f\"cannot prefix: {name}\")\n    return name\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.shortname","title":"<code>shortname(self, name, strict=True)</code>","text":"<p>Return the short name (keyword) corresponding to <code>name</code>.</p> <p>If <code>name</code> is not defined in the context, a <code>NameSpaceError</code> will be raised if <code>strict</code> is true. Otherwise <code>name</code> will be returned unchanged.</p> <p>Examples:</p> <p>context = Context() context.shortname(\"dcat:Dataset\") 'Dataset'</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def shortname(self, name: str, strict: bool = True) -&gt; str:\n    \"\"\"Return the short name (keyword) corresponding to `name`.\n\n    If `name` is not defined in the context, a `NameSpaceError` will\n    be raised if `strict` is true. Otherwise `name` will be returned\n    unchanged.\n\n    Example:\n\n    &gt;&gt;&gt; context = Context()\n    &gt;&gt;&gt; context.shortname(\"dcat:Dataset\")\n    'Dataset'\n\n    \"\"\"\n    if not self._shortnamed:\n        self._create_caches()\n    if name in self._shortnamed:\n        return self._shortnamed[name]\n    if strict:\n        raise NamespaceError(f\"no short name for: {name}\")\n    return name\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.sync_prefixes","title":"<code>sync_prefixes(self, ts, update=None)</code>","text":"<p>Syncronise prefixes between context and triplestore.</p> <p>The <code>update</code> option controls how prefix inconsistencies are handeled. If <code>update</code> is:</p> <ul> <li>True:  Prefixes in the context will be updated.</li> <li>False: Prefixes in the triplestore will be updated.</li> <li>None:  A PrefixMismatchError is raised.</li> </ul> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def sync_prefixes(\n    self, ts: Triplestore, update: \"Optional[bool]\" = None\n) -&gt; None:\n    \"\"\"Syncronise prefixes between context and triplestore.\n\n    The `update` option controls how prefix inconsistencies are handeled.\n    If `update` is:\n\n    - True:  Prefixes in the context will be updated.\n    - False: Prefixes in the triplestore will be updated.\n    - None:  A PrefixMismatchError is raised.\n\n    \"\"\"\n    ns1 = self.get_prefixes().copy()\n    ns2 = {pf: str(ns) for pf, ns in ts.namespaces.items()}\n\n    if update is None:\n        mismatch = [\n            p for p in set(ns1).intersection(ns2) if ns1[p] != ns2[p]\n        ]\n        if mismatch:\n            msg = [\"Mismatch in definition of prefix(es): \"]\n            for mis in mismatch:\n                msg.extend(\n                    [\n                        f\"* Prefix '{mis}' is defined as\",\n                        f\"  - {ns1[mis]} in context\",\n                        f\"  - {ns2[mis]} in triplestore\",\n                    ]\n                )\n            raise PrefixMismatchError(os.linesep.join(msg))\n\n    if update:\n        ns1.update(ns2)\n        ns2 = ns1\n    else:\n        ns2.update(ns1)\n        ns1 = ns2\n\n    self.add_context(ns2)\n\n    for prefix, ns in ns1.items():\n        if prefix not in ts.namespaces:\n            ts.bind(prefix, ns)\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.Context.to_triplestore","title":"<code>to_triplestore(self, ts, doc)</code>","text":"<p>Store JSON-LD document <code>doc</code> to triplestore <code>ts</code>.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def to_triplestore(self, ts, doc: \"Union[dict, list]\"):\n    \"\"\"Store JSON-LD document `doc` to triplestore `ts`.\"\"\"\n    nt = jsonld.to_rdf(\n        self._todict(doc), options={\"format\": \"application/n-quads\"}\n    )\n    ts.parse(data=nt, format=\"ntriples\")\n\n    if isinstance(doc, dict) and \"@context\" in doc:\n        ctx = self.copy()\n        ctx.add_context(doc)\n    else:\n        ctx = self\n    for prefix, ns in ctx.get_prefixes().items():\n        if prefix not in ts.namespaces:\n            ts.bind(prefix, ns)\n</code></pre>"},{"location":"api_reference/datadoc/context/#tripper.datadoc.context.get_context","title":"<code>get_context(context=None, theme=None, default_theme='ddoc:datadoc', keywords=None, prefixes=None, processingMode='json-ld-1.1', copy=False, timeout=3)</code>","text":"<p>A convinient function that returns an Context instance.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>Optional[ContextType]</code> <p>Input context.  Several types are supported: - Context: The Context instance will be updated and returned,   unless <code>copy</code> is true. - dict: Dict-representation of a valid JSON-LD context. - str: If a valid URI, the context is loaded from this URI,   otherwise it is assumed to be a file path to load. - sequence: A sequence of the above.</p> <code>None</code> <code>theme</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>Load initial context for this theme.</p> <code>None</code> <code>default_theme</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>Initialise context for this theme if neither <code>context</code> nor <code>theme</code> are provided.</p> <code>'ddoc:datadoc'</code> <code>keywords</code> <code>Optional[Keywords]</code> <p>Initialise from this keywords instance.</p> <code>None</code> <code>prefixes</code> <code>Optional[dict]</code> <p>Optional dict with additional prefixes.</p> <code>None</code> <code>processingMode</code> <code>str</code> <p>Either \"json-ld-1.0\" or \"json-ld-1.1\".</p> <code>'json-ld-1.1'</code> <code>copy</code> <code>bool</code> <p>If true, always return a new Context object.</p> <code>False</code> <code>timeout</code> <code>float</code> <p>Timeout when accessing remote files.</p> <code>3</code> <p>Returns:</p> Type Description <code>Context</code> <p>Context object.</p> Source code in <code>tripper/datadoc/context.py</code> <pre><code>def get_context(\n    context: \"Optional[ContextType]\" = None,\n    theme: \"Optional[Union[str, Sequence[str]]]\" = None,\n    default_theme: \"Optional[Union[str, Sequence[str]]]\" = \"ddoc:datadoc\",\n    keywords: \"Optional[Keywords]\" = None,\n    prefixes: \"Optional[dict]\" = None,\n    processingMode: str = \"json-ld-1.1\",\n    copy: bool = False,\n    timeout: float = 3,\n) -&gt; \"Context\":\n    \"\"\"A convinient function that returns an Context instance.\n\n    Arguments:\n        context: Input context.  Several types are supported:\n            - Context: The Context instance will be updated and returned,\n              unless `copy` is true.\n            - dict: Dict-representation of a valid JSON-LD context.\n            - str: If a valid URI, the context is loaded from this URI,\n              otherwise it is assumed to be a file path to load.\n            - sequence: A sequence of the above.\n        theme: Load initial context for this theme.\n        default_theme: Initialise context for this theme if neither\n            `context` nor `theme` are provided.\n        keywords: Initialise from this keywords instance.\n        prefixes: Optional dict with additional prefixes.\n        processingMode: Either \"json-ld-1.0\" or \"json-ld-1.1\".\n        copy: If true, always return a new Context object.\n        timeout: Timeout when accessing remote files.\n\n    Returns:\n        Context object.\n    \"\"\"\n    # pylint: disable=import-outside-toplevel\n    from tripper.datadoc.keywords import get_keywords\n\n    if isinstance(context, Context):\n        if copy:\n            context = context.copy()\n        if keywords or theme:\n            kw = get_keywords(keywords=keywords, theme=theme)\n            context.add_context(kw.get_context())\n    else:\n        context = Context(\n            context=context,\n            theme=theme if context or theme or keywords else default_theme,\n            keywords=keywords,\n            processingMode=processingMode,\n            timeout=timeout,\n        )\n    if prefixes:\n        context.add_context({k: str(v) for k, v in prefixes.items()})\n    return context\n</code></pre>"},{"location":"api_reference/datadoc/dataaccess/","title":"dataaccess","text":"<p>A module for providing access to data based on data documentation from the datasets module.</p> <p>High-level functions for accessing and storing actual data:</p> <ul> <li><code>load()</code>: Load documented dataset from its source.</li> <li><code>save()</code>: Save documented dataset to a data resource.</li> </ul> <p>Note</p> <p>This module may eventually be moved out of tripper into a separate package.</p>"},{"location":"api_reference/datadoc/dataaccess/#tripper.datadoc.dataaccess.load","title":"<code>load(ts, iri, distributions=None, use_sparql=None, retries=1)</code>","text":"<p>Load dataset with given IRI from its source.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore documenting the data to load.</p> required <code>iri</code> <code>str</code> <p>IRI of the data to load.</p> required <code>distributions</code> <code>'Optional[Union[str, Sequence[str]]]'</code> <p>Name or sequence of names of distribution(s) to try in case the dataset has multiple distributions.  The default is to try all documented distributions.</p> <code>None</code> <code>use_sparql</code> <code>'Optional[bool]'</code> <p>Whether to access the triplestore with SPARQL. Defaults to <code>ts.prefer_sparql</code>.</p> <code>None</code> <code>retries</code> <code>int</code> <p>Number of times to try accessing the dataset. After each failed access, it will sleep for 1 second before trying again. The default is to only make one attempt to access the dataset.</p> <code>1</code> <p>Returns:</p> Type Description <code>bytes</code> <p>Bytes object with the underlying data.</p> <p>Note</p> <p>For now this requires DLite.</p> Source code in <code>tripper/datadoc/dataaccess.py</code> <pre><code>def load(\n    ts: Triplestore,\n    iri: str,\n    distributions: \"Optional[Union[str, Sequence[str]]]\" = None,\n    use_sparql: \"Optional[bool]\" = None,\n    retries: int = 1,\n) -&gt; bytes:\n    \"\"\"Load dataset with given IRI from its source.\n\n    Arguments:\n        ts: Triplestore documenting the data to load.\n        iri: IRI of the data to load.\n        distributions: Name or sequence of names of distribution(s) to\n            try in case the dataset has multiple distributions.  The\n            default is to try all documented distributions.\n        use_sparql: Whether to access the triplestore with SPARQL.\n            Defaults to `ts.prefer_sparql`.\n        retries: Number of times to try accessing the dataset. After each\n            failed access, it will sleep for 1 second before trying again.\n            The default is to only make one attempt to access the dataset.\n\n    Returns:\n        Bytes object with the underlying data.\n\n    Note:\n        For now this requires DLite.\n    \"\"\"\n    # pylint: disable=import-outside-toplevel\n    # Use the Protocol plugin system from DLite.  Should we move it to tripper?\n    import dlite\n    from dlite.protocol import Protocol\n\n    dct = acquire(ts, iri=iri, use_sparql=use_sparql)\n    if DCAT.Dataset not in get(dct, \"@type\"):\n        raise TypeError(\n            f\"expected IRI '{iri}' to be a dataset, but got: \"\n            f\"{', '.join(get(dct, '@type'))}\"\n        )\n\n    if distributions is None:\n        distributions = get(dct, \"distribution\")\n\n    for dist in distributions:\n        url = dist.get(\"downloadURL\", dist.get(\"accessURL\"))  # type: ignore\n        if url:\n            p = urlparse(url)\n            # Mapping of supported schemes - should be moved into the protocol\n            # module.\n            schemes = {\n                \"https\": \"http\",\n            }\n            scheme = schemes.get(p.scheme, p.scheme) if p.scheme else \"file\"\n            location = (\n                f\"{scheme}://{p.netloc}{p.path}\"\n                if p.netloc\n                else f\"{scheme}:{p.path}\"\n            )\n            id = (\n                dist.accessService.get(\"identifier\")  # type: ignore\n                if \"accessService\" in dist\n                else None\n            )\n            for n in range(retries):\n                try:\n                    with Protocol(scheme, location, options=p.query) as pr:\n                        return pr.load(id)\n                    # pylint: disable=no-member\n                except (dlite.DLiteProtocolError, dlite.DLiteIOError):\n                    pass\n                # pylint: disable=broad-exception-caught\n                except Exception as exc:\n                    if n &lt; retries - 1:\n                        time.sleep(1)\n                    else:\n                        raise IOError(\n                            f\"cannot access dataset '{iri}' using \"\n                            f\"scheme={scheme}, location={location} \"\n                            f\"and options='{p.query}'\"\n                        ) from exc\n\n    raise IOError(f\"Cannot access dataset: {iri}\")\n</code></pre>"},{"location":"api_reference/datadoc/dataaccess/#tripper.datadoc.dataaccess.save","title":"<code>save(ts, data, class_iri=None, dataset=None, distribution=None, generator=None, prefixes=None, use_sparql=None, method='raise')</code>","text":"<p>Saves data to a dataresource and document it in the triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore that documents the data to save.</p> required <code>data</code> <code>bytes</code> <p>Bytes representation of the data to save.</p> required <code>class_iri</code> <code>'Optional[str]'</code> <p>IRI of a class in the ontology (e.g. an <code>emmo:Dataset</code> subclass) that describes the dataset that is saved. Is used to select the <code>distribution</code> if that is not given. If <code>distribution</code> is also given, a <code>dcat:distribution value &lt;distribution&gt;</code> restriction will be added to <code>class_iri</code></p> <code>None</code> <code>dataset</code> <code>'Optional[Union[str, dict]]'</code> <p>Either the IRI of the dataset individual standing for the data to be saved or or a dict that in addition to the IRI ('@id' keyword) can provide with additional documentation of the dataset. If <code>dataset</code> is None, a new blank node IRI will be created.</p> <code>None</code> <code>distribution</code> <code>'Optional[Union[str, dict]]'</code> <p>Either the IRI of distribution for the data to be saved or a dict additional documentation of the distribution, like media type, parsers, generators etc... If <code>distribution</code> is None and dataset is not a dict with a 'distribution' keyword, a new distribution will be added to the dataset.</p> <code>None</code> <code>generator</code> <code>'Optional[str]'</code> <p>Name of generator to use in case the distribution has several generators.</p> <code>None</code> <code>prefixes</code> <code>'Optional[dict]'</code> <p>Dict with prefixes in addition to those included in the JSON-LD context.  Should map namespace prefixes to IRIs.</p> <code>None</code> <code>use_sparql</code> <code>'Optional[bool]'</code> <p>Whether to access the triplestore with SPARQL. Defaults to <code>ts.prefer_sparql</code>.</p> <code>None</code> <code>method</code> <code>str</code> <p>How to handle the case where <code>ts</code> already contains a document with the same id as <code>source</code>. Possible values are: - \"overwrite\": Remove existing documentation before storing. - \"raise\": Raise an <code>IRIExistsError</code> if the IRI of <code>source</code>   already exits in the triplestore (default). - \"merge\": Merge <code>source</code> with existing documentation. This will   duplicate non-literal properties with no explicit <code>@id</code>. If this   is unwanted, merge manually and use \"overwrite\". - \"ignore\": If the IRI of <code>source</code> already exists, do nothing but   issueing an <code>IRIExistsWarning</code>.</p> <code>'raise'</code> <p>Returns:</p> Type Description <code>str</code> <p>IRI of the dataset.</p> Source code in <code>tripper/datadoc/dataaccess.py</code> <pre><code>def save(\n    ts: Triplestore,\n    data: bytes,\n    class_iri: \"Optional[str]\" = None,\n    dataset: \"Optional[Union[str, dict]]\" = None,\n    distribution: \"Optional[Union[str, dict]]\" = None,\n    generator: \"Optional[str]\" = None,\n    prefixes: \"Optional[dict]\" = None,\n    use_sparql: \"Optional[bool]\" = None,\n    method: str = \"raise\",\n) -&gt; str:\n    \"\"\"Saves data to a dataresource and document it in the triplestore.\n\n    Arguments:\n        ts: Triplestore that documents the data to save.\n        data: Bytes representation of the data to save.\n        class_iri: IRI of a class in the ontology (e.g. an `emmo:Dataset`\n            subclass) that describes the dataset that is saved.\n            Is used to select the `distribution` if that is not given.\n            If `distribution` is also given, a\n            `dcat:distribution value &lt;distribution&gt;` restriction will be\n            added to `class_iri`\n        dataset: Either the IRI of the dataset individual standing for\n            the data to be saved or or a dict that in addition to the IRI\n            ('@id' keyword) can provide with additional documentation of\n            the dataset.\n            If `dataset` is None, a new blank node IRI will be created.\n        distribution: Either the IRI of distribution for the data to be saved\n            or a dict additional documentation of the distribution,\n            like media type, parsers, generators etc...\n            If `distribution` is None and dataset is not a dict with a\n            'distribution' keyword, a new distribution will be added\n            to the dataset.\n        generator: Name of generator to use in case the distribution has\n            several generators.\n        prefixes: Dict with prefixes in addition to those included in the\n            JSON-LD context.  Should map namespace prefixes to IRIs.\n        use_sparql: Whether to access the triplestore with SPARQL.\n            Defaults to `ts.prefer_sparql`.\n        method: How to handle the case where `ts` already contains a document\n            with the same id as `source`. Possible values are:\n            - \"overwrite\": Remove existing documentation before storing.\n            - \"raise\": Raise an `IRIExistsError` if the IRI of `source`\n              already exits in the triplestore (default).\n            - \"merge\": Merge `source` with existing documentation. This will\n              duplicate non-literal properties with no explicit `@id`. If this\n              is unwanted, merge manually and use \"overwrite\".\n            - \"ignore\": If the IRI of `source` already exists, do nothing but\n              issueing an `IRIExistsWarning`.\n\n    Returns:\n        IRI of the dataset.\n\n    \"\"\"\n    # pylint: disable=too-many-locals,too-many-branches,too-many-statements\n    # pylint: disable=import-outside-toplevel\n    # Use the Protocol plugin system from DLite.  Should we move it to tripper?\n    from dlite.protocol import Protocol\n\n    triples = []\n    save_dataset = save_distribution = False\n\n    if dataset is None:\n        # __TODO__: Infer dataset from value restriction on `class_iri`\n        # This require that we make a SPARQL-version of ts.restriction().\n        newiri = f\"_:N{secrets.token_hex(16)}\"\n        typeiri = [DCAT.Dataset, class_iri] if class_iri else DCAT.Dataset\n        dataset = AttrDict({\"@id\": newiri, \"@type\": typeiri})\n        save_dataset = True\n    elif isinstance(dataset, str):\n        dset = acquire(ts, iri=dataset, use_sparql=use_sparql)\n        if dset:\n            dataset = dset\n        else:\n            typeiri = [DCAT.Dataset, class_iri] if class_iri else DCAT.Dataset\n            dataset = AttrDict({\"@id\": dataset, \"@type\": typeiri})\n            save_dataset = True\n    elif isinstance(dataset, dict):\n        save_dataset = True\n    else:\n        raise TypeError(\n            \"if given, `dataset` should be either a string or dict\"\n        )\n    dataset: dict  # Tell mypy that this now is a dict\n\n    if distribution is None:\n        if \"distribution\" in dataset:\n            distribution = get(dataset, \"distribution\")[0]\n        else:\n            newiri = f\"_:N{secrets.token_hex(16)}\"\n            distribution = AttrDict(\n                {\"@id\": newiri, \"@type\": DCAT.Distribution}\n            )\n            add(dataset, \"distribution\", distribution)\n            triples.append((dataset[\"@id\"], DCAT.distribution, newiri))\n            save_distribution = True\n    if isinstance(distribution, str):\n        distr = acquire(ts, iri=distribution, use_sparql=use_sparql)\n        if distr:\n            distribution = distr\n        else:\n            distribution = AttrDict(\n                {\"@id\": distribution, \"@type\": DCAT.Distribution}\n            )\n            add(dataset, \"distribution\", distribution)\n            triples.append((dataset[\"@id\"], DCAT.distribution, newiri))\n            save_distribution = True\n    elif isinstance(distribution, dict):\n        add(dataset, DCAT.distribution, distribution)\n        if \"@id\" in distribution:\n            triples.append(\n                (dataset[\"@id\"], DCAT.distribution, distribution[\"@id\"])\n            )\n        save_distribution = True\n    else:\n        raise TypeError(\n            \"if given, `distribution` should be either a string or dict\"\n        )\n    distribution: dict  # Tell mypy that this now is a dict\n\n    if isinstance(generator, str):\n        gen = get(distribution, \"generator\")\n        if isinstance(gen, (str, dict)):\n            gen = [gen]\n        for g in gen:\n            if isinstance(g, dict):\n                if gen.get(\"@id\") == generator:\n                    break\n            else:\n                break  # ???\n        else:\n            raise ValueError(\n                f\"dataset '{dataset}' has no such generator: {generator}\"\n            )\n    elif \"generator\" in distribution:\n        gen = get(distribution, \"generator\")[0]\n    else:\n        gen = None\n\n    # __TODO__: Check if `class_iri` already has the value restriction.\n    # If not, add it to triples\n\n    # __TODO__: Move this mapping of supported schemes into the protocol\n    # module.\n    schemes = {\n        \"https\": \"http\",\n    }\n\n    # Save data\n    url = distribution.get(\"downloadURL\", distribution.get(\"accessURL\"))\n    p = urlparse(str(url.decode() if isinstance(url, bytes) else url))\n    scheme = (\n        schemes.get(p.scheme, p.scheme) if p.scheme else \"file\"  # type: ignore\n    )\n    location = (\n        f\"{scheme}://{p.netloc}{p.path}\" if p.netloc else f\"{scheme}:{p.path}\"\n    )\n    options = [p.query] if p.query else []\n    if gen and \"configuration\" in gen and \"options\" in gen.configuration:\n        # __TODO__: allow options to also be a dict\n        options.append(gen.configuration[\"options\"])\n    id = (\n        distribution[\"accessService\"].get(\"identifier\")\n        if \"accessService\" in distribution\n        else None\n    )\n    with Protocol(scheme, location, options=\";\".join(options)) as pr:\n        pr.save(data, id)\n\n    # Update triplestore\n    ts.add_triples(triples)\n    if save_dataset:\n        store(ts, dataset, type=\"Dataset\", prefixes=prefixes, method=method)\n    elif save_distribution:\n        store(\n            ts,\n            distribution,\n            type=\"Distribution\",\n            prefixes=prefixes,\n            method=method,\n        )\n\n    return dataset[\"@id\"]\n</code></pre>"},{"location":"api_reference/datadoc/dataset/","title":"dataset","text":"<p>Module for documenting datasets with Tripper.</p> <p>The dataset documentation follows the DCAT structure and is exposed as Python dicts with attribute access in this module.  The semantic meaning of the keywords in this dict are defined by a JSON-LD context.</p> <p>High-level function for populating the triplestore from YAML documentation:</p> <ul> <li><code>save_datadoc()</code>: Save documentation from YAML file to the triplestore.</li> </ul> <p>Functions for searching the triplestore:</p> <ul> <li><code>search()</code>: Get IRIs of matching entries in the triplestore.</li> </ul> <p>Functions for working with the dict-representation:</p> <ul> <li><code>read_datadoc()</code>: Read documentation from YAML file and return it as dict.</li> <li><code>store()</code>: Store documentation to the triplestore.</li> <li><code>acquire()</code>: Load documentation from the triplestore.</li> <li><code>told()</code>: Extend documention to valid JSON-LD (represented as a Python dict)</li> <li><code>delete_iri()</code>: Remove documentation of resource with given IRI.</li> <li><code>delete()</code>: Remove documentation of matching resources.</li> </ul>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.acquire","title":"<code>acquire(ts, iri, use_sparql=None, context=None)</code>","text":"<p>Load description of a resource from the triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to load description from.</p> required <code>iri</code> <code>str</code> <p>IRI of the to resource.</p> required <code>use_sparql</code> <code>'Optional[bool]'</code> <p>Whether to access the triplestore with SPARQL. Defaults to the value of <code>ts.prefer_sparql</code>.</p> <code>None</code> <code>context</code> <code>'Optional[ContextType]'</code> <p>Context object defining keywords in addition to those defined in the default [JSON-LD context].</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict describing the resource identified by <code>iri</code>.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def acquire(\n    ts: Triplestore,\n    iri: str,\n    use_sparql: \"Optional[bool]\" = None,\n    context: \"Optional[ContextType]\" = None,\n) -&gt; dict:\n    \"\"\"Load description of a resource from the triplestore.\n\n    Arguments:\n        ts: Triplestore to load description from.\n        iri: IRI of the to resource.\n        use_sparql: Whether to access the triplestore with SPARQL.\n            Defaults to the value of `ts.prefer_sparql`.\n        context: Context object defining keywords in addition to those defined\n            in the default [JSON-LD context].\n\n    Returns:\n        Dict describing the resource identified by `iri`.\n    \"\"\"\n    context = get_context(context, theme=None)\n\n    if use_sparql is None:\n        use_sparql = ts.prefer_sparql\n    if use_sparql:\n        return _load_sparql(ts, iri)\n\n    d = AttrDict()\n    dct = _load_triples(ts, iri, context=context)\n    for key, val in dct.items():\n        if key in (\"mappings\", \"statements\"):\n            add(d, key, val)\n        else:\n            if not isinstance(val, list):\n                val = [val]\n            for v in val:\n                if key != \"@id\" and isinstance(v, str) and v.startswith(\"_:\"):\n                    add(\n                        d,\n                        key,\n                        acquire(\n                            ts, iri=v, context=context, use_sparql=use_sparql\n                        ),\n                    )\n                else:\n                    add(d, key, v)\n\n    return d\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.delete","title":"<code>delete(ts, type=None, criterias=None, criteria=None, regex=None, flags=None, keywords=None)</code>","text":"<p>Delete matching resources. See <code>search()</code> for a description of arguments.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def delete(\n    ts: Triplestore,\n    type=None,\n    criterias: \"Optional[dict]\" = None,  # deprecated\n    criteria: \"Optional[dict]\" = None,  # new preferred name\n    regex: \"Optional[dict]\" = None,\n    flags: \"Optional[str]\" = None,\n    keywords: \"Optional[Keywords]\" = None,\n) -&gt; None:\n    \"\"\"Delete matching resources. See `search()` for a description of arguments.\"\"\"\n\n    if criterias is not None:\n        warnings.warn(\n            \"`criterias` is deprecated, use `criteria` instead\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        # if caller didn\u2019t supply the new name, adopt the old one\n        if criteria is None:\n            criteria = criterias\n\n    # normalize defaults\n    criteria = criteria or {}\n    regex = regex or {}\n\n    iris = search(\n        ts=ts,\n        type=type,\n        criteria=criteria,\n        regex=regex,\n        flags=flags,\n        keywords=keywords,\n    )\n    for iri in iris:\n        delete_iri(ts, iri)\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.delete_iri","title":"<code>delete_iri(ts, iri)</code>","text":"<p>Delete <code>iri</code> from triplestore using SPARQL.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def delete_iri(ts: Triplestore, iri: str) -&gt; None:\n    \"\"\"Delete `iri` from triplestore using SPARQL.\"\"\"\n    subj = iri if iri.startswith(\"_:\") else f\"&lt;{ts.expand_iri(iri)}&gt;\"\n    query = f\"\"\"\n    # Some backends requires the prefix to be defined...\n    PREFIX : &lt;http://example.com#&gt;\n    DELETE {{ ?s ?p ?o }}\n    WHERE {{\n      {subj} (:|!:)* ?s .\n      ?s ?p ?o .\n    }}\n    \"\"\"\n    ts.update(query)\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.get_jsonld_context","title":"<code>get_jsonld_context(context=None, timeout=5, fromfile=True)</code>","text":"<p>Returns the JSON-LD context as a dict.</p> <p>The JSON-LD context maps all the keywords that can be used as keys in the dict-representation of a dataset to properties defined in common vocabularies and ontologies.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>'Optional[Union[str, dict, Sequence[Union[str, dict]]]]'</code> <p>Additional user-defined context that should be returned on top of the default context.  It may be a string with an URL to the user-defined context, a dict with the user-defined context or a sequence of strings and dicts.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Number of seconds before timing out.</p> <code>5</code> <code>fromfile</code> <code>bool</code> <p>Whether to load the context from local file.</p> <code>True</code> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def get_jsonld_context(\n    context: \"Optional[Union[str, dict, Sequence[Union[str, dict]]]]\" = None,\n    timeout: float = 5,\n    fromfile: bool = True,\n) -&gt; dict:\n    \"\"\"Returns the JSON-LD context as a dict.\n\n    The JSON-LD context maps all the keywords that can be used as keys\n    in the dict-representation of a dataset to properties defined in\n    common vocabularies and ontologies.\n\n    Arguments:\n        context: Additional user-defined context that should be returned\n            on top of the default context.  It may be a string with an URL\n            to the user-defined context, a dict with the user-defined context\n            or a sequence of strings and dicts.\n        timeout: Number of seconds before timing out.\n        fromfile: Whether to load the context from local file.\n\n    \"\"\"\n    import requests\n\n    warnings.warn(\n        \"`dataset.get_jsonld_context()` is deprecated, use `get_context()` \"\n        \"instead\",\n        category=DeprecationWarning,\n        stacklevel=2,\n    )\n\n    if fromfile:\n        with open(CONTEXT_PATH, \"r\", encoding=\"utf-8\") as f:\n            ctx = json.load(f)[\"@context\"]\n    else:\n        r = requests.get(CONTEXT_URL, allow_redirects=True, timeout=timeout)\n        ctx = json.loads(r.content)[\"@context\"]\n\n    if isinstance(context, (str, dict)):\n        context = [context]\n\n    if context:\n        for token in context:\n            if isinstance(token, str):\n                with openfile(token, timeout=timeout, mode=\"rt\") as f:\n                    content = f.read()\n                ctx.update(json.loads(content)[\"@context\"])\n            elif isinstance(token, dict):\n                ctx.update(token)\n            else:\n                raise TypeError(\n                    \"`context` must be a string (URL), dict or a sequence of \"\n                    f\"strings and dicts.  Not '{type(token)}'\"\n                )\n\n    return ctx\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.get_partial_pipeline","title":"<code>get_partial_pipeline(ts, client, iri, context=None, parser=None, generator=None, distribution=None, use_sparql=None)</code>","text":"<p>Returns a OTELib partial pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to load data from.</p> required <code>client</code> <p>OTELib client to create pipeline with.</p> required <code>iri</code> <code>str</code> <p>IRI of the dataset to load.</p> required <code>context</code> <code>'Optional[Context]'</code> <p>Context object.</p> <code>None</code> <code>parser</code> <code>'Optional[Union[bool, str]]'</code> <p>Whether to return a datasource partial pipeline. Should be True or an IRI of parser to use in case the distribution has multiple parsers.  By default the first parser will be selected.</p> <code>None</code> <code>generator</code> <code>'Optional[Union[bool, str]]'</code> <p>Whether to return a datasink partial pipeline. Should be True or an IRI of generator to use in case the distribution has multiple generators.  By default the first generator will be selected.</p> <code>None</code> <code>distribution</code> <code>'Optional[str]'</code> <p>IRI of distribution to use in case the dataset dataset has multiple distributions.  By default any of the distributions will be picked.</p> <code>None</code> <code>use_sparql</code> <code>'Optional[bool]'</code> <p>Whether to access the triplestore with SPARQL. Defaults to <code>ts.prefer_sparql</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>bytes</code> <p>OTELib partial pipeline.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def get_partial_pipeline(\n    ts: Triplestore,\n    client,\n    iri: str,\n    context: \"Optional[Context]\" = None,\n    parser: \"Optional[Union[bool, str]]\" = None,\n    generator: \"Optional[Union[bool, str]]\" = None,\n    distribution: \"Optional[str]\" = None,\n    use_sparql: \"Optional[bool]\" = None,\n) -&gt; bytes:\n    \"\"\"Returns a OTELib partial pipeline.\n\n    Arguments:\n        ts: Triplestore to load data from.\n        client: OTELib client to create pipeline with.\n        iri: IRI of the dataset to load.\n        context: Context object.\n        parser: Whether to return a datasource partial pipeline.\n            Should be True or an IRI of parser to use in case the\n            distribution has multiple parsers.  By default the first\n            parser will be selected.\n        generator: Whether to return a datasink partial pipeline.\n            Should be True or an IRI of generator to use in case the\n            distribution has multiple generators.  By default the first\n            generator will be selected.\n        distribution: IRI of distribution to use in case the dataset\n            dataset has multiple distributions.  By default any of\n            the distributions will be picked.\n        use_sparql: Whether to access the triplestore with SPARQL.\n            Defaults to `ts.prefer_sparql`.\n\n    Returns:\n        OTELib partial pipeline.\n    \"\"\"\n    # pylint: disable=too-many-branches,too-many-locals\n    context = get_context(context=context, theme=\"ddoc:datadoc\")\n\n    dct = acquire(ts, iri, use_sparql=use_sparql)\n\n    if isinstance(distribution, str):\n        for distr in get(dct, \"distribution\"):\n            if distr[\"@id\"] == distribution:\n                break\n        else:\n            raise ValueError(\n                f\"dataset '{iri}' has no such distribution: {distribution}\"\n            )\n    else:\n        distr = get(dct, \"distribution\")[0]\n\n    accessService = (\n        distr.accessService.get(\"endpointURL\")\n        if \"accessService\" in distr\n        else None\n    )\n\n    # OTEAPI still puts the parse configurations into the dataresource\n    # instead of a in a separate parse strategy...\n    if parser:\n        if parser is True:\n            par = get(distr, \"parser\")[0]\n        elif isinstance(parser, str):\n            for par in get(distr, \"parser\"):\n                if par.get(\"@id\") == parser:\n                    break\n            else:\n                raise ValueError(\n                    f\"dataset '{iri}' has no such parser: {parser}\"\n                )\n        if isinstance(par, str):\n            par = acquire(ts, par)\n        configuration = par.get(\"configuration\")\n    else:\n        configuration = None\n\n    mediaType = distr.get(\"mediaType\")\n    mediaTypeShort = (\n        mediaType[44:]\n        if mediaType.startswith(\"http://www.iana.org/assignments/media-types/\")\n        else mediaType\n    )\n    dataresource = client.create_dataresource(\n        downloadUrl=distr.get(\"downloadURL\"),\n        mediaType=mediaTypeShort,\n        accessUrl=distr.get(\"accessURL\"),\n        accessService=accessService,\n        configuration=dict(configuration) if configuration else {},\n    )\n\n    statements = dct.get(\"statements\", [])\n    statements.extend(dct.get(\"mappings\", []))\n    if statements:\n        mapping = client.create_mapping(\n            mappingType=\"triples\",\n            # The OTEAPI datamodels stupidly strict, requireing us\n            # to cast the data ts.namespaces and statements\n            prefixes={k: str(v) for k, v in ts.namespaces.items()},\n            triples=[tuple(t) for t in statements],\n        )\n\n    if parser:\n        pipeline = dataresource\n        if statements:\n            pipeline = pipeline &gt;&gt; mapping\n    elif generator:\n        if generator is True:\n            gen = get(distr, \"generator\")[0]\n        elif isinstance(generator, str):\n            for gen in get(distr, \"generator\"):\n                if gen.get(\"@id\") == generator:\n                    break\n            else:\n                raise ValueError(\n                    f\"dataset '{iri}' has no such generator: {generator}\"\n                )\n\n        conf = gen.get(\"configuration\")\n        if gen.generatorType == \"application/vnd.dlite-generate\":\n            conf.setdefault(\"datamodel\", dct.get(\"datamodel\"))\n\n        function = client.create_function(\n            functionType=gen.generatorType,\n            configuration=conf,\n        )\n        if statements:\n            pipeline = mapping &gt;&gt; function &gt;&gt; dataresource\n        else:\n            pipeline = function &gt;&gt; dataresource\n\n    return pipeline\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.get_prefixes","title":"<code>get_prefixes(context=None, timeout=5, fromfile=True)</code>","text":"<p>Loads the JSON-LD context and returns a dict mapping prefixes to their namespace URL.</p> <p>Arguments are passed to <code>get_jsonld_context()</code>.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def get_prefixes(\n    context: \"Optional[Union[str, dict, Sequence[Union[str, dict]]]]\" = None,\n    timeout: float = 5,\n    fromfile: bool = True,\n) -&gt; dict:\n    \"\"\"Loads the JSON-LD context and returns a dict mapping prefixes to\n    their namespace URL.\n\n    Arguments are passed to `get_jsonld_context()`.\n    \"\"\"\n    warnings.warn(\n        \"`dataset.get_prefixes()` is deprecated, use `Context.prefixes()` \"\n        \"instead\",\n        category=DeprecationWarning,\n        stacklevel=2,\n    )\n\n    if isinstance(context, Context):\n        return context.get_prefixes()\n\n    ctx = get_jsonld_context(\n        context=context, timeout=timeout, fromfile=fromfile\n    )\n    prefixes = {\n        k: str(v)\n        for k, v in ctx.items()\n        if isinstance(v, (str, Namespace)) and str(v).endswith((\"#\", \"/\"))\n    }\n    return prefixes\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.get_shortnames","title":"<code>get_shortnames(context=None, timeout=5, fromfile=True)</code>","text":"<p>Loads the JSON-LD context and returns a dict mapping IRIs to their short names defined in the context.</p> <p>Arguments are passed to <code>get_jsonld_context()</code>.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def get_shortnames(\n    context: \"Optional[Union[str, dict, Sequence[Union[str, dict]]]]\" = None,\n    timeout: float = 5,\n    fromfile: bool = True,\n) -&gt; dict:\n    \"\"\"Loads the JSON-LD context and returns a dict mapping IRIs to their\n    short names defined in the context.\n\n    Arguments are passed to `get_jsonld_context()`.\n    \"\"\"\n    warnings.warn(\n        \"`dataset.get_shortnames()` is deprecated, use `Context.shortname()` \"\n        \"instead\",\n        category=DeprecationWarning,\n        stacklevel=2,\n    )\n\n    ctx = get_jsonld_context(\n        context=context, timeout=timeout, fromfile=fromfile\n    )\n    prefixes = get_prefixes(context=ctx)\n    shortnames = {\n        expand_iri(v[\"@id\"] if isinstance(v, dict) else v, prefixes): k\n        for k, v in ctx.items()\n        if (\n            (isinstance(v, str) and not v.endswith((\"#\", \"/\")))\n            or isinstance(v, dict)\n        )\n    }\n    shortnames.setdefault(RDF.type, \"@type\")\n    return shortnames\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.get_values","title":"<code>get_values(data, key, extend=True)</code>","text":"<p>Parse <code>data</code> recursively and return a list with the values corresponding to the given key.</p> <p>If <code>extend</code> is true, the returned list will be extended with values that themselves are list, instead of appending them in a nested manner.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def get_values(\n    data: \"Union[dict, list]\", key: str, extend: bool = True\n) -&gt; list:\n    \"\"\"Parse `data` recursively and return a list with the values\n    corresponding to the given key.\n\n    If `extend` is true, the returned list will be extended with\n    values that themselves are list, instead of appending them in a\n    nested manner.\n\n    \"\"\"\n    values = []\n    if isinstance(data, dict):\n        val = data.get(key)\n        if extend and isinstance(val, list):\n            values.extend(val)\n        elif val:\n            values.append(val)\n        for k, v in data.items():\n            if k != \"@context\" and isinstance(v, (dict, list)):\n                values.extend(get_values(v, key))\n    elif isinstance(data, list):\n        for ele in data:\n            if isinstance(ele, (dict, list)):\n                values.extend(get_values(ele, key))\n    return values\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.infer_restriction_types","title":"<code>infer_restriction_types(source, context=None)</code>","text":"<p>Return a dict that describes what type of restriction the properties of a class in <code>source</code> should be converted to.</p> <p>The following algorithm is used:   - Suggested restriction will only be provided for classes, i.e.     if the value of <code>@type</code> is <code>owl:Class</code> or <code>rdfs:Class</code>.   - For each property:       - If the value is a class, assume an existential         restriction.       - Otherwise, if the value is an IRI (to presumably an         individual), assume a value restriction.       - Otherwise, if the value is a literal with no language         specification, (indicating a data property) assume a value         restriction.       - Otherwise, don't add a restriction type for the property         (assuming that it is an annotation property).</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>'Union[dict, list]'</code> <p>JSON-LD dict or list of JSON-LD dicts to analyse.</p> required <code>context</code> <code>'Optional[Context]'</code> <p>Optional JSON-LD context object used for providing hints about classes and relation types.</p> <code>None</code> <p>Returns:</p> Type Description <code>A dict that maps class IRIs to dicts that associate property IRIs to restriction types. For example</code> <p>{       \"ClassA\": {\"prop1\": \"some\", \"prop2\": \"value\"},       \"ClassB\": {\"prop3\": \"some\"},     }</p> <p>The following restriction types are supported:   - \"some\": existential restriction   - \"only\": universal restriction   - \"exactly \": exact cardinality restriction   - \"min \": minimum cardinality restriction   - \"max \": maximum cardinality restriction   - \"value\": value restriction <p>where <code>&lt;N&gt;</code> is a positive integer.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def infer_restriction_types(\n    source: \"Union[dict, list]\",\n    context: \"Optional[Context]\" = None,\n) -&gt; dict:\n    \"\"\"Return a dict that describes what type of restriction the\n    properties of a class in `source` should be converted to.\n\n    The following algorithm is used:\n      - Suggested restriction will only be provided for classes, i.e.\n        if the value of `@type` is `owl:Class` or `rdfs:Class`.\n      - For each property:\n          - If the value is a class, assume an existential\n            restriction.\n          - Otherwise, if the value is an IRI (to presumably an\n            individual), assume a value restriction.\n          - Otherwise, if the value is a literal with no language\n            specification, (indicating a data property) assume a value\n            restriction.\n          - Otherwise, don't add a restriction type for the property\n            (assuming that it is an annotation property).\n\n    Arguments:\n        source: JSON-LD dict or list of JSON-LD dicts to analyse.\n        context: Optional JSON-LD context object used for providing\n            hints about classes and relation types.\n\n    Returns:\n        A dict that maps class IRIs to dicts that associate property IRIs\n        to restriction types. For example:\n\n            {\n              \"ClassA\": {\"prop1\": \"some\", \"prop2\": \"value\"},\n              \"ClassB\": {\"prop3\": \"some\"},\n            }\n\n        The following restriction types are supported:\n          - \"some\": existential restriction\n          - \"only\": universal restriction\n          - \"exactly &lt;N&gt;\": exact cardinality restriction\n          - \"min &lt;N&gt;\": minimum cardinality restriction\n          - \"max &lt;N&gt;\": maximum cardinality restriction\n          - \"value\": value restriction\n\n        where `&lt;N&gt;` is a positive integer.\n\n    \"\"\"\n    # pylint: disable=unused-argument\n\n    def isclass(iri):\n        \"\"\"Return true if expanded IRI `iri` refer to a class.\"\"\"\n        return iri in classes or (\n            context and iri in context and context.is_class(iri)\n        )\n\n    context = get_context(context=context)\n\n    if isinstance(source, dict):\n        source = source[\"@graph\"] if \"@graph\" in source else [source]\n\n    classes = {}\n    for src in source:\n        if _isclassdoc(src) and \"@id\" in src:\n            classes[context.expand(src[\"@id\"], strict=True)] = src\n\n    restrictions: dict = {}\n    for iri, cls in classes.items():\n        d = {}\n        for k, v in cls.items():\n            if k.startswith(\"@\") or k in (\"subClassOf\", \"rdfs:subClassOf\"):\n                continue\n            kexp = context.expand(k, strict=True)\n            if isinstance(v, dict):\n                d[kexp] = \"some\" if _isclassdoc(v) else \"value\"\n                dct = infer_restriction_types(v, context=context)\n                dct.update(restrictions)\n                restrictions.update(dct)\n            elif isinstance(v, str) and (\n                is_uri(v, require_netloc=False)\n                and context\n                and kexp in context\n                and not context.is_annotation_property(kexp)\n            ):\n                vexp = context.expand(v, strict=True)\n                d[kexp] = \"some\" if isclass(vexp) else \"value\"\n            elif not isinstance(v, str) or (\n                context\n                and kexp in context\n                and not context.is_annotation_property(kexp)\n            ):\n                d[kexp] = \"value\"\n        if d:\n            restrictions[iri] = d\n\n    return restrictions\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.load_dict","title":"<code>load_dict(ts, iri, use_sparql=None)</code>","text":"<p>This function is deprecated. Use acquire() instead.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def load_dict(\n    ts: Triplestore,\n    iri: str,\n    use_sparql: \"Optional[bool]\" = None,\n) -&gt; dict:\n    \"\"\"This function is deprecated. Use acquire() instead.\"\"\"\n    warnings.warn(\n        \"tripper.datadoc.load_dict() is deprecated. \"\n        \"Please use tripper.datadoc.acquire() instead.\",\n        category=DeprecationWarning,\n        stacklevel=2,\n    )\n    return acquire(ts=ts, iri=iri, use_sparql=use_sparql)\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.load_list","title":"<code>load_list(ts, iri)</code>","text":"<p>Load and return RDF list whose first node is <code>iri</code>.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def load_list(ts: Triplestore, iri: str):\n    \"\"\"Load and return RDF list whose first node is `iri`.\"\"\"\n    lst = []\n    for p, o in ts.predicate_objects(iri):\n        if p == RDF.first:\n            lst.append(o)\n        elif p == RDF.rest:\n            lst.extend(load_list(ts, o))\n    return lst\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.make_query","title":"<code>make_query(ts, type=None, criterias=None, criteria=None, regex=None, flags=None, keywords=None, query_type='SELECT DISTINCT')</code>","text":"<p>Help function for creating a SPARQL query.</p> <p>See search() for description of arguments.</p> <p>The <code>query_type</code> argument is typically one of \"SELECT DISTINCT\" \"SELECT\", or \"DELETE\".</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def make_query(\n    ts: Triplestore,\n    type=None,\n    criterias: \"Optional[dict]\" = None,  # deprecated\n    criteria: \"Optional[dict]\" = None,  # new preferred name\n    regex: \"Optional[dict]\" = None,\n    flags: \"Optional[str]\" = None,\n    keywords: \"Optional[Keywords]\" = None,\n    query_type: \"Optional[str]\" = \"SELECT DISTINCT\",\n) -&gt; \"str\":\n    \"\"\"Help function for creating a SPARQL query.\n\n    See search() for description of arguments.\n\n    The `query_type` argument is typically one of \"SELECT DISTINCT\"\n    \"SELECT\", or \"DELETE\".\n    \"\"\"\n    # pylint: disable=too-many-statements,too-many-branches,too-many-locals\n\n    if criterias is not None:\n        warnings.warn(\n            \"`criterias` is deprecated, use `criteria` instead\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        # if caller didn\u2019t supply the new name, adopt the old one\n        if criteria is None:\n            criteria = criterias\n\n    keywords = get_keywords(keywords=keywords)\n    context = get_context(keywords=keywords)\n    context._create_caches()  # pylint: disable=protected-access\n    expanded = context._expanded  # pylint: disable=protected-access\n\n    # Add prefixes to triplestore\n    ts.namespaces.update(keywords.get_prefixes())\n\n    # Initiate variables\n    criteria = criteria or {}\n    regex = regex or {}\n    crit = []\n    filters = []\n    n = 0  # counter for creating new unique sparql variables\n    flags_arg = f\", {flags}\" if flags else \"\"\n\n    # Special handling of @id\n    cid = criteria.pop(\"@id\", criteria.pop(\"_id\", None))\n    rid = regex.pop(\"@id\", regex.pop(\"_id\", None))\n    if cid:\n        filters.append(f'FILTER(STR(?iri) = \"{ts.expand_iri(cid)}\") .')\n    elif rid:\n        filters.append(\n            f'FILTER REGEX(STR(?iri), \"{ts.expand_iri(rid)}\"{flags_arg}) .'\n        )\n\n    if type:\n        types = [type] if not isinstance(type, list) else type\n        for t in types:\n            if \":\" in t:\n                expanded_iri = ts.expand_iri(t)\n                crit.append(f\"?iri rdf:type &lt;{expanded_iri}&gt; .\")\n            else:\n                if keywords is None:\n                    keywords = Keywords()\n                typ = keywords.superclasses(t)\n                if not isinstance(typ, str):\n                    typ = typ[0]\n                crit.append(f\"?iri rdf:type &lt;{ts.expand_iri(typ)}&gt; .\")  # type: ignore\n\n    def add_crit(k, v, regex=False, s=\"iri\"):\n        \"\"\"Add criteria to SPARQL query.\"\"\"\n        nonlocal n\n        key = f\"@{k[1:]}\" if k.startswith(\"_\") else k\n        if isinstance(v, list):\n            for ele in v:\n                add_crit(key, ele, regex=regex, s=s)\n            return\n        if re.match(r\"^[_a-zA-Z0.9]+\\.\", key):\n            newkey, restkey = key.split(\".\", 1)\n            if newkey in expanded:\n                newkey = expanded[newkey]\n            n += 1\n            var = f\"v{n}\"\n            crit.append(f\"?{s} &lt;{ts.expand_iri(newkey)}&gt; ?{var} .\")\n            add_crit(restkey, v, s=var)\n        else:\n            if key in expanded:\n                key = expanded[key]\n            if v in expanded:\n                value = f\"&lt;{expanded[v]}&gt;\"\n            elif isinstance(v, str):\n                value = (\n                    f\"&lt;{v}&gt;\"\n                    if re.match(\"^[a-z][a-z0-9.+-]*://\", v)\n                    else f'\"{v}\"'\n                )\n            else:\n                value = v\n            n += 1\n            var = f\"v{n}\"\n            crit.append(f\"?{s} &lt;{ts.expand_iri(key)}&gt; ?{var} .\")\n            if regex:\n                filters.append(\n                    f\"FILTER REGEX(STR(?{var}), {value}{flags_arg}) .\"\n                )\n            else:\n                filters.append(f\"FILTER(STR(?{var}) = {value}) .\")\n\n    for k, v in criteria.items():\n        add_crit(k, v)\n\n    if not crit:\n        crit.append(\"?iri ?p ?o .\")\n\n    for k, v in regex.items():\n        add_crit(k, v, regex=True)\n\n    where_statements = \"\\n      \".join(crit + filters)\n    query = f\"\"\"\n    PREFIX rdf: &lt;{RDF}&gt;\n    {query_type} ?iri\n    WHERE {{\n      {where_statements}\n    }}\n    \"\"\"\n    return query\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.read_datadoc","title":"<code>read_datadoc(filename)</code>","text":"<p>Read YAML data documentation and return it as a dict.</p> <p>The filename may also be an URL to a file accessible with HTTP GET.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def read_datadoc(filename: \"Union[str, Path]\") -&gt; dict:\n    \"\"\"Read YAML data documentation and return it as a dict.\n\n    The filename may also be an URL to a file accessible with HTTP GET.\n    \"\"\"\n    import yaml  # type: ignore\n\n    with openfile(filename, mode=\"rt\", encoding=\"utf-8\") as f:\n        d = yaml.safe_load(f)\n    return told(d)\n    # return prepare_datadoc(d)\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.save_datadoc","title":"<code>save_datadoc(ts, file_or_dict, keywords=None, context=None)</code>","text":"<p>Populate triplestore with data documentation.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to save dataset documentation to.</p> required <code>file_or_dict</code> <code>'Union[str, Path, dict]'</code> <p>Data documentation dict or name of a YAML file to read the data documentation from.  It may also be an URL to a file accessible with HTTP GET.</p> required <code>keywords</code> <code>'Optional[Keywords]'</code> <p>Optional Keywords object with keywords definitions. The default is to infer the keywords from the <code>theme</code> or <code>keywordfile</code> keys in the YAML file.</p> <code>None</code> <code>context</code> <code>'Optional[Context]'</code> <p>Optional Context object with mappings. By default it is inferred from <code>keywords</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dict-representation of the loaded dataset.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def save_datadoc(\n    ts: Triplestore,\n    file_or_dict: \"Union[str, Path, dict]\",\n    keywords: \"Optional[Keywords]\" = None,\n    context: \"Optional[Context]\" = None,\n) -&gt; dict:\n    \"\"\"Populate triplestore with data documentation.\n\n    Arguments:\n        ts: Triplestore to save dataset documentation to.\n        file_or_dict: Data documentation dict or name of a YAML file to read\n            the data documentation from.  It may also be an URL to a file\n            accessible with HTTP GET.\n        keywords: Optional Keywords object with keywords definitions.\n            The default is to infer the keywords from the `theme` or\n            `keywordfile` keys in the YAML file.\n        context: Optional Context object with mappings. By default it is\n            inferred from `keywords`.\n\n    Returns:\n        Dict-representation of the loaded dataset.\n    \"\"\"\n    import yaml  # type: ignore\n\n    if isinstance(file_or_dict, dict):\n        d = file_or_dict\n    else:\n        with openfile(file_or_dict, mode=\"rt\", encoding=\"utf-8\") as f:\n            d = yaml.safe_load(f)\n\n    return store(ts, d, keywords=keywords, context=context)\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.save_dict","title":"<code>save_dict(ts, source, type=None, context=None, keywords=None, prefixes=None, method='merge', restrictions=None)</code>","text":"<p>This function is deprecated. Use store() instead.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def save_dict(\n    ts: Triplestore,\n    source: \"Union[dict, list]\",\n    type: \"Optional[str]\" = None,\n    context: \"Optional[Context]\" = None,\n    keywords: \"Optional[Keywords]\" = None,\n    prefixes: \"Optional[dict]\" = None,\n    method: str = \"merge\",\n    # The unnecessary strictness of the \"build documentation\" CI enforces us\n    # to add a `restrictions` argument to save_dict(), although this argument\n    # came after that save_dict() was renamed.\n    restrictions: \"Optional[dict]\" = None,\n) -&gt; dict:\n    \"\"\"This function is deprecated. Use store() instead.\"\"\"\n    warnings.warn(\n        \"tripper.datadoc.save_dict() is deprecated. \"\n        \"Please use tripper.datadoc.store() instead.\",\n        category=DeprecationWarning,\n        stacklevel=2,\n    )\n    return store(\n        ts=ts,\n        source=source,\n        type=type,\n        context=context,\n        keywords=keywords,\n        prefixes=prefixes,\n        method=method,\n        restrictions=restrictions,\n    )\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.save_extra_content","title":"<code>save_extra_content(ts, source)</code>","text":"<p>Save extra content in <code>source</code> to the triplestore.</p> <p>Currently, this includes: - statements and mappings - data models (require that DLite is installed)</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to load data from.</p> required <code>source</code> <code>dict</code> <p>Dict in multi-resource format.</p> required Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def save_extra_content(ts: Triplestore, source: dict) -&gt; None:\n    \"\"\"Save extra content in `source` to the triplestore.\n\n    Currently, this includes:\n    - statements and mappings\n    - data models (require that DLite is installed)\n\n    Arguments:\n        ts: Triplestore to load data from.\n        source: Dict in multi-resource format.\n\n    \"\"\"\n    import requests\n\n    # Save statements and mappings\n    statements = get_values(source, \"statements\")\n    statements.extend(get_values(source, \"mappings\"))\n    if statements:\n        ts.add_triples(statements)\n\n    # Save data models\n    datamodels = {\n        d[\"@id\"]: d[\"datamodel\"]\n        for d in source.get(\"Dataset\", ())\n        if \"datamodel\" in d\n    }\n    try:\n        # pylint: disable=import-outside-toplevel\n        import dlite\n        from dlite.dataset import add_dataset\n    except ModuleNotFoundError:\n        if datamodels:\n            warnings.warn(\n                \"dlite is not installed - data models will not be added to \"\n                \"the triplestore\"\n            )\n    else:\n        for url in get_values(source, \"datamodelStorage\"):\n            dlite.storage_path.append(url)\n\n        for iri, uri in datamodels.items():\n            ok = False\n            r = requests.get(uri, timeout=3)\n            if r.ok:\n                content = (\n                    r.content.decode()\n                    if isinstance(r.content, bytes)\n                    else str(r.content)\n                )\n                dm = dlite.Instance.from_json(content)\n                add_dataset(ts, dm)\n                ok = True\n            else:\n                try:\n                    dm = dlite.get_instance(uri)\n                except (\n                    dlite.DLiteMissingInstanceError  # pylint: disable=no-member\n                ):\n                    # __FIXME__: check session whether to warn or re-raise\n                    warnings.warn(f\"cannot load datamodel: {uri}\")\n                else:\n                    add_dataset(ts, dm)\n                    ok = True\n\n            if ok:\n                # Make our dataset an individual of the new dataset subclass\n                # that we have created by serialising the datamodel\n                ts.add((iri, RDF.type, uri))\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.search","title":"<code>search(ts, type=None, criterias=None, criteria=None, regex=None, flags=None, keywords=None, skipblanks=True)</code>","text":"<p>Return a list of IRIs for all matching resources.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to search.</p> required <code>type</code> <p>Either a [resource type] (ex: \"Dataset\", \"Distribution\", ...) or the IRI of a class to limit the search to.</p> <code>None</code> <code>criteria</code> <code>'Optional[dict]'</code> <p>Exact match criteria. A dict of IRI, value pairs, where the IRIs refer to data properties on the resource match. The IRIs may use any prefix defined in <code>ts</code>. E.g. if the prefix <code>dcterms</code> is in <code>ts</code>, it is expanded and the match criteria <code>dcterms:title</code> is correctly parsed.</p> <code>None</code> <code>regex</code> <code>'Optional[dict]'</code> <p>Like <code>criteria</code> but the values in the provided dict are regular expressions used for the matching.</p> <code>None</code> <code>flags</code> <code>'Optional[str]'</code> <p>Flags passed to regular expressions. - <code>s</code>: Dot-all mode. The . matches any character.  The default   doesn't match newline or carriage return. - <code>m</code>: Multi-line mode. The ^ and $ characters matches beginning   or end of line instead of beginning or end of string. - <code>i</code>: Case-insensitive mode. - <code>q</code>: Special characters representing themselves.</p> <code>None</code> <code>keywords</code> <code>'Optional[Keywords]'</code> <p>Keywords instance defining the resource types used with the <code>type</code> argument.</p> <code>None</code> <code>skipblanks</code> <code>'bool'</code> <p>Whether to skip blank nodes.</p> <code>True</code> <p>Returns:</p> Type Description <code>'List[str]'</code> <p>List of IRIs for matching resources.</p> <p>Examples:</p> <p>List all data resources IRIs:</p> <pre><code>search(ts)\n</code></pre> <p>List IRIs of all resources with John Doe as <code>contactPoint</code>:</p> <pre><code>search(ts, criteria={\"contactPoint.hasName\": \"John Doe\"})\n</code></pre> <p>List IRIs of all samples:</p> <pre><code>search(ts, type=CHAMEO.Sample)\n</code></pre> <p>List IRIs of all datasets with John Doe as <code>contactPoint</code> AND are measured on a given sample:</p> <pre><code>search(\n    ts,\n    type=DCAT.Dataset,\n    criteria={\n        \"contactPoint.hasName\": \"John Doe\",\n        \"fromSample\": SAMPLE.batch2/sample3,\n    },\n)\n</code></pre> <p>List IRIs of all datasets who's title matches the regular expression \"[Mm]agnesium\":</p> <pre><code>search(\n    ts, type=DCAT.Dataset, regex={\"title\": \"[Mm]agnesium\"},\n)\n</code></pre> <p>Seealso</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def search(\n    ts: Triplestore,\n    type=None,\n    criterias: \"Optional[dict]\" = None,  # deprecated\n    criteria: \"Optional[dict]\" = None,  # new preferred name\n    regex: \"Optional[dict]\" = None,\n    flags: \"Optional[str]\" = None,\n    keywords: \"Optional[Keywords]\" = None,\n    skipblanks: \"bool\" = True,\n) -&gt; \"List[str]\":\n    \"\"\"Return a list of IRIs for all matching resources.\n\n    Arguments:\n        ts: Triplestore to search.\n        type: Either a [resource type] (ex: \"Dataset\", \"Distribution\", ...)\n            or the IRI of a class to limit the search to.\n        criteria: Exact match criteria. A dict of IRI, value pairs, where the\n            IRIs refer to data properties on the resource match. The IRIs\n            may use any prefix defined in `ts`. E.g. if the prefix `dcterms`\n            is in `ts`, it is expanded and the match criteria `dcterms:title`\n            is correctly parsed.\n        regex: Like `criteria` but the values in the provided dict are regular\n            expressions used for the matching.\n        flags: Flags passed to regular expressions.\n            - `s`: Dot-all mode. The . matches any character.  The default\n              doesn't match newline or carriage return.\n            - `m`: Multi-line mode. The ^ and $ characters matches beginning\n              or end of line instead of beginning or end of string.\n            - `i`: Case-insensitive mode.\n            - `q`: Special characters representing themselves.\n        keywords: Keywords instance defining the resource types used with\n            the `type` argument.\n        skipblanks: Whether to skip blank nodes.\n\n    Returns:\n        List of IRIs for matching resources.\n\n    Examples:\n        List all data resources IRIs:\n\n            search(ts)\n\n        List IRIs of all resources with John Doe as `contactPoint`:\n\n            search(ts, criteria={\"contactPoint.hasName\": \"John Doe\"})\n\n        List IRIs of all samples:\n\n            search(ts, type=CHAMEO.Sample)\n\n        List IRIs of all datasets with John Doe as `contactPoint` AND are\n        measured on a given sample:\n\n            search(\n                ts,\n                type=DCAT.Dataset,\n                criteria={\n                    \"contactPoint.hasName\": \"John Doe\",\n                    \"fromSample\": SAMPLE.batch2/sample3,\n                },\n            )\n\n        List IRIs of all datasets who's title matches the regular expression\n        \"[Mm]agnesium\":\n\n            search(\n                ts, type=DCAT.Dataset, regex={\"title\": \"[Mm]agnesium\"},\n            )\n\n    SeeAlso:\n        [resource type]: https://emmc-asbl.github.io/tripper/latest/datadoc/introduction/#resource-types\n    \"\"\"\n\n    if criterias is not None:\n        warnings.warn(\n            \"`criterias` is deprecated, use `criteria` instead\",\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        # if caller didn\u2019t supply the new name, adopt the old one\n        if criteria is None:\n            criteria = criterias\n\n    # normalize defaults\n    criteria = criteria or {}\n    regex = regex or {}\n\n    query = make_query(\n        ts=ts,\n        type=type,\n        criteria=criteria,\n        regex=regex,\n        flags=flags,\n        keywords=keywords,\n        query_type=\"SELECT DISTINCT\",\n    )\n    if skipblanks:\n        return [\n            r[0] for r in ts.query(query) if not r[0].startswith(\"_:\")  # type: ignore\n        ]\n    return [r[0] for r in ts.query(query)]  # type: ignore\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.search_iris","title":"<code>search_iris(ts, type=None, criterias=None, regex=None, flags=None, keywords=None, skipblanks=True)</code>","text":"<p>This function is deprecated. Use search() instead.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def search_iris(\n    ts: Triplestore,\n    type=None,\n    criterias: \"Optional[dict]\" = None,\n    regex: \"Optional[dict]\" = None,\n    flags: \"Optional[str]\" = None,\n    keywords: \"Optional[Keywords]\" = None,\n    skipblanks: \"bool\" = True,\n) -&gt; \"List[str]\":\n    \"\"\"This function is deprecated. Use search() instead.\"\"\"\n    warnings.warn(\n        \"tripper.datadoc.search_iris() is deprecated. \"\n        \"Please use tripper.datadoc.search() instead.\",\n        category=DeprecationWarning,\n        stacklevel=2,\n    )\n    return search(\n        ts=ts,\n        type=type,\n        criterias=criterias,\n        regex=regex,\n        flags=flags,\n        keywords=keywords,\n        skipblanks=skipblanks,\n    )\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.show","title":"<code>show(obj, indent=2)</code>","text":"<p>Print object to screen as pretty json.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def show(obj, indent=2) -&gt; None:\n    \"\"\"Print object to screen as pretty json.\"\"\"\n    if isinstance(obj, (bytes, str)):\n        d = json.loads(obj)\n    elif isinstance(obj, Keywords):\n        d = obj.get_context()\n    elif isinstance(obj, Context):\n        d = obj.get_context_dict()\n    else:\n        d = obj\n    print(json.dumps(d, indent=indent))\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.store","title":"<code>store(ts, source, type=None, keywords=None, theme='ddoc:datadoc', context=None, prefixes=None, method='raise', restrictions=None)</code>","text":"<p>Store documentation of a resource to a triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to store to.</p> required <code>source</code> <code>'Union[dict, list]'</code> <p>Dict or list with the resource documentation to store.</p> required <code>type</code> <code>'Optional[str]'</code> <p>Type of documented resource.  Should be one of the resource types defined in <code>keywords</code>.</p> <code>None</code> <code>keywords</code> <code>'Optional[Keywords]'</code> <p>Keywords object with additional keywords definitions. If not provided, only default keywords are considered.</p> <code>None</code> <code>theme</code> <code>'Optional[Union[str, Sequence[str]]]'</code> <p>IRI of one of more themes to load keywords for.</p> <code>'ddoc:datadoc'</code> <code>context</code> <code>'Optional[Context]'</code> <p>Context object defining keywords in addition to those defined in the default [JSON-LD context]. Complementing the <code>keywords</code> argument.</p> <code>None</code> <code>prefixes</code> <code>'Optional[dict]'</code> <p>Dict with prefixes in addition to those included in the JSON-LD context.  Should map namespace prefixes to IRIs.</p> <code>None</code> <code>method</code> <code>str</code> <p>How to handle the case where <code>ts</code> already contains a document with the same id as <code>source</code>. Possible values are: - \"overwrite\": Remove existing documentation before storing. - \"raise\": Raise an <code>IRIExistsError</code> if the IRI of <code>source</code>   already exits in the triplestore (default). - \"merge\": Merge <code>source</code> with existing documentation. This will   duplicate non-literal properties with no explicit <code>@id</code>. If this   is unwanted, merge manually and use \"overwrite\". - \"ignore\": If the IRI of <code>source</code> already exists, do nothing but   issueing an <code>IRIExistsWarning</code>.</p> <code>'raise'</code> <code>restrictions</code> <code>'Optional[dict]'</code> <p>A dict describing how properties of classes in <code>source</code> should be mapped to restrictions.  The default is to call <code>infer_restriction_types()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A copy of <code>source</code> updated to valid JSON-LD.</p> <p>Notes</p> <p>The keywords should either be one of the default keywords or defined by the <code>context</code> or <code>keywords</code> arguments.</p> <p>References:</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def store(\n    ts: Triplestore,\n    source: \"Union[dict, list]\",\n    type: \"Optional[str]\" = None,\n    keywords: \"Optional[Keywords]\" = None,\n    theme: \"Optional[Union[str, Sequence[str]]]\" = \"ddoc:datadoc\",\n    context: \"Optional[Context]\" = None,\n    prefixes: \"Optional[dict]\" = None,\n    method: str = \"raise\",\n    restrictions: \"Optional[dict]\" = None,\n) -&gt; dict:\n    # pylint: disable=line-too-long,too-many-branches\n    \"\"\"Store documentation of a resource to a triplestore.\n\n    Arguments:\n        ts: Triplestore to store to.\n        source: Dict or list with the resource documentation to store.\n        type: Type of documented resource.  Should be one of the resource types\n            defined in `keywords`.\n        keywords: Keywords object with additional keywords definitions.\n            If not provided, only default keywords are considered.\n        theme: IRI of one of more themes to load keywords for.\n        context: Context object defining keywords in addition to those defined\n            in the default [JSON-LD context].\n            Complementing the `keywords` argument.\n        prefixes: Dict with prefixes in addition to those included in the\n            JSON-LD context.  Should map namespace prefixes to IRIs.\n        method: How to handle the case where `ts` already contains a document\n            with the same id as `source`. Possible values are:\n            - \"overwrite\": Remove existing documentation before storing.\n            - \"raise\": Raise an `IRIExistsError` if the IRI of `source`\n              already exits in the triplestore (default).\n            - \"merge\": Merge `source` with existing documentation. This will\n              duplicate non-literal properties with no explicit `@id`. If this\n              is unwanted, merge manually and use \"overwrite\".\n            - \"ignore\": If the IRI of `source` already exists, do nothing but\n              issueing an `IRIExistsWarning`.\n        restrictions: A dict describing how properties of classes in\n            `source` should be mapped to restrictions.  The default is\n            to call `infer_restriction_types()`.\n\n    Returns:\n        A copy of `source` updated to valid JSON-LD.\n\n    Notes:\n        The keywords should either be one of the [default keywords] or defined\n        by the `context` or `keywords` arguments.\n\n    References:\n    [default keywords]: https://emmc-asbl.github.io/tripper/latest/datadoc/keywords/\n    [JSON-LD context]: https://raw.githubusercontent.com/EMMC-ASBL/oteapi-dlite/refs/heads/rdf-serialisation/oteapi_dlite/context/0.3/context.json\n    \"\"\"\n    keywords = get_keywords(keywords, theme=theme)\n    context = get_context(\n        keywords=keywords, context=context, prefixes=prefixes\n    )\n    doc = told(\n        source,\n        type=type,\n        keywords=keywords,\n        context=context,\n        prefixes=prefixes,\n    )\n    docs = doc if isinstance(doc, list) else doc.get(\"@graph\", [doc])\n    for d in docs:\n        iri = d[\"@id\"]\n        if ts.has(iri):\n            if method == \"overwrite\":\n                delete_iri(ts, iri)\n            elif method == \"raise\":\n                raise IRIExistsError(f\"Cannot overwrite existing IRI: {iri}\")\n            elif method == \"merge\":\n                pass\n            elif method == \"ignore\":\n                warnings.warn(iri, category=IRIExistsWarning)\n                return doc\n            else:\n                raise ValueError(\n                    f\"Invalid storage method: '{method}'. \"\n                    \"Should be one of: 'overwrite', 'raise', 'ignore' or \"\n                    \"'merge'\"\n                )\n\n    context.sync_prefixes(ts)\n\n    update_classes(doc, context=context, restrictions=restrictions)\n    # add(doc, \"@context\", context.get_context_dict())\n\n    # Validate\n    # TODO: reenable validation\n    # validate(doc, type=type, keywords=keywords)\n\n    context.to_triplestore(ts, doc)\n\n    # Add statements and data models to triplestore\n    save_extra_content(ts, doc)  # FIXME: SLOW!!\n\n    return doc\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.told","title":"<code>told(descr, type=None, keywords=None, context=None, prefixes=None)</code>","text":"<p>Return an updated copy of data description <code>descr</code> as valid JSON-LD.</p> <p>The following transformations are performed:</p> <ul> <li>insert @type keywords</li> <li>expand IRIs and substitute @id and @type in <code>statements</code> keyword</li> <li>expand IRIs and substitute @id and @type in <code>mappings</code> keyword</li> <li>insert mappings from <code>mappingURL</code> keyword (uses <code>mappingFormat</code>)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>descr</code> <code>'Union[dict, list]'</code> <p>Documenting of one or several resources to be represented as JSON-LD.  Supports both single- and multi-resource dicts.</p> required <code>type</code> <code>'Optional[str]'</code> <p>Type of data to save.  Should be one of the resource types defined in <code>keywords</code>.</p> <code>None</code> <code>keywords</code> <code>'Optional[Keywords]'</code> <p>Keywords object with keywords definitions.  If not provided, only default keywords are considered.</p> <code>None</code> <code>context</code> <code>'Optional[Context]'</code> <p>Optional context object. It will be updated from the input data documentation <code>descr</code>.</p> <code>None</code> <code>prefixes</code> <code>'Optional[dict]'</code> <p>Dict with prefixes in addition to those known in keywords or included in the JSON-LD context.</p> <code>None</code> <p>Returns:</p> Type Description <code>'AttrDict'</code> <p>Dict with an updated copy of <code>descr</code> as valid JSON-LD.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def told(\n    descr: \"Union[dict, list]\",\n    type: \"Optional[str]\" = None,\n    keywords: \"Optional[Keywords]\" = None,\n    context: \"Optional[Context]\" = None,\n    prefixes: \"Optional[dict]\" = None,\n) -&gt; \"AttrDict\":\n    \"\"\"Return an updated copy of data description `descr` as valid JSON-LD.\n\n    The following transformations are performed:\n\n    - insert @type keywords\n    - expand IRIs and substitute @id and @type in `statements` keyword\n    - expand IRIs and substitute @id and @type in `mappings` keyword\n    - insert mappings from `mappingURL` keyword (uses `mappingFormat`)\n\n    Arguments:\n        descr: Documenting of one or several resources to be represented as\n            JSON-LD.  Supports both single- and multi-resource dicts.\n        type: Type of data to save.  Should be one of the resource types\n            defined in `keywords`.\n        keywords: Keywords object with keywords definitions.  If not provided,\n            only default keywords are considered.\n        context: Optional context object. It will be updated from the input\n            data documentation `descr`.\n        prefixes: Dict with prefixes in addition to those known in keywords\n            or included in the JSON-LD context.\n\n    Returns:\n        Dict with an updated copy of `descr` as valid JSON-LD.\n\n    \"\"\"\n    single = \"@id\", \"@type\", \"@graph\"\n    multi = \"keywordfile\", \"prefixes\", \"base\"\n    singlerepr = any(s in descr for s in single) or isinstance(descr, list)\n    multirepr = any(s in descr for s in multi)\n    if singlerepr and multirepr:\n        raise InvalidDatadocError(\n            \"invalid mixture of single- and multi-resource dict\"\n        )\n    if not singlerepr:\n        keywords = get_keywords(\n            keywords=keywords,\n            theme=descr.get(\"theme\", \"ddoc:datadoc\"),  # type: ignore\n            yamlfile=descr.get(\"keywordfile\"),  # type: ignore\n        )\n    else:\n        keywords = get_keywords(keywords=keywords)\n\n    context = get_context(\n        context=context, keywords=keywords, prefixes=prefixes\n    )\n    resources = keywords.data.resources\n\n    if singlerepr:  # single-resource representation\n        d = descr\n    else:  # multi-resource representation\n        d = {}\n        graph = []\n        for k, v in descr.items():  # type: ignore\n            if k == \"theme\":\n                pass\n            elif k == \"@context\":\n                context.add_context(v)\n                d[k] = v\n            elif k == \"prefixes\":\n                context.add_context(v)\n            elif k == \"base\":\n                context.base = v\n            elif k in resources:\n                if isinstance(v, list):\n                    for dct in v:\n                        add(dct, \"@type\", resources[k].iri)\n                        graph.append(dct)\n                else:\n                    graph.append(v)\n            else:\n                raise InvalidDatadocError(\n                    f\"Invalid keyword in root of multi-resource dict: {k}\"\n                )\n        d[\"@graph\"] = graph\n\n    return _told(\n        d,\n        type=type,\n        keywords=keywords,\n        prefixes=context.get_prefixes(),\n        root=True,\n        hasid=False,\n    )\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.update_classes","title":"<code>update_classes(source, context=None, restrictions=None)</code>","text":"<p>Update documentation of classes, ensuring that they will be correctly represented in RDF.</p> <p>Only classes, i.e. resources of type <code>owl:Class</code> and <code>rdfs:Class</code>, will be updated.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>'Union[dict, list]'</code> <p>Input documentation of one or more resources. This dict will be updated in-place. It is typically a dict returned by <code>told()</code>.</p> required <code>context</code> <code>'Optional[Context]'</code> <p>Context object defining the keywords.</p> <code>None</code> <code>restrictions</code> <code>'Optional[dict]'</code> <p>A dict describing how properties of classes in <code>source</code> should be mapped to restrictions.  The default is to call <code>infer_restriction_types()</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>'Union[dict, list]'</code> <p>The updated version of <code>source</code>.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def update_classes(\n    source: \"Union[dict, list]\",\n    context: \"Optional[Context]\" = None,\n    restrictions: \"Optional[dict]\" = None,\n) -&gt; \"Union[dict, list]\":\n    \"\"\"Update documentation of classes, ensuring that they will be\n    correctly represented in RDF.\n\n    Only classes, i.e. resources of type `owl:Class` and `rdfs:Class`,\n    will be updated.\n\n    Arguments:\n        source: Input documentation of one or more resources. This dict\n            will be updated in-place. It is typically a dict returned by\n            `told()`.\n        context: Context object defining the keywords.\n        restrictions: A dict describing how properties of classes in\n            `source` should be mapped to restrictions.  The default is\n            to call `infer_restriction_types()`.\n\n    Returns:\n        The updated version of `source`.\n\n    \"\"\"\n    # pylint: disable=too-many-statements\n\n    def addrestriction(source, prop, value):\n        \"\"\"Add restriction to `source`.\"\"\"\n        # pylint: disable=no-else-return\n\n        iri = context.expand(source[\"@id\"]) if \"@id\" in source else \"*\"\n        propiri = context.expand(prop)\n        if value is None or prop.startswith(\"@\"):\n            return\n        elif isinstance(value, dict) and _isclassdoc(value):\n            update_classes(value, context=context, restrictions=restrictions)\n        elif isinstance(value, list):\n            for val in value:\n                addrestriction(source, prop, val)\n            return\n\n        restrictionType = None\n        if iri in restrictions:\n            restrictionType = restrictions[iri].get(propiri)\n        elif \"*\" in restrictions:\n            restrictionType = restrictions[\"*\"].get(propiri)\n\n        if not restrictionType:\n            return\n\n        d = {\n            \"rdf:type\": \"owl:Restriction\",\n            # We expand here, since JSON-LD doesn't expand values.\n            \"owl:onProperty\": context.expand(prop, strict=True),\n        }\n        if restrictionType == \"value\":\n            d[\"owl:hasValue\"] = value\n        elif restrictionType == \"some\":\n            d[\"owl:someValuesFrom\"] = value\n        elif restrictionType == \"only\":\n            d[\"owl:allValuesFrom\"] = value\n        else:\n            d[\"owl:onClass\"] = value\n            ctype, n = restrictionType.split()\n            ctypes = {\n                \"exactly\": \"owl:qualifiedCardinality\",\n                \"min\": \"owl:minQualifiedCardinality\",\n                \"max\": \"owl:maxQualifiedCardinality\",\n            }\n            d[ctypes[ctype]] = int(n)\n\n        add(source, \"subClassOf\", d)\n        if prop in source:  # Avoid removing prop more than once\n            del source[prop]\n\n        # Recursively update related calsses\n        if restrictionType != \"value\" and isinstance(value, dict):\n            update_classes(value, context, restrictions)\n\n    # Local context\n    context = get_context(context=context)\n    if \"@context\" in source:\n        context = context.copy()\n        context.add_context(source[\"@context\"])  # type: ignore\n\n    if restrictions is None:\n        restrictions = infer_restriction_types(source, context)\n    else:\n        restrictions = {\n            \"*\" if ckey == \"*\" else context.expand(ckey, strict=True): {\n                context.expand(pkey, strict=True): pval\n                for pkey, pval in cval.items()\n            }\n            for ckey, cval in restrictions.items()\n        }\n\n    # Handle lists and graphs\n    if isinstance(source, list) or \"@graph\" in source:\n        sources = source if isinstance(source, list) else source[\"@graph\"]\n        if isinstance(sources, dict):\n            sources = [sources]\n        for src in sources:\n            update_classes(src, context)\n        return source\n\n    # Ensure that source is only of type owl:Class\n    # Move all other types to subClassOf\n    types = {context.expand(t): t for t in get(source, \"@type\")}\n    if OWL.Class in types:\n        for e, t in types.items():\n            if e == OWL.Class:\n                source[\"@type\"] = t\n            else:\n                add(source, \"subClassOf\", e)\n\n    # Convert relations to restrictions\n    for k, v in source.copy().items():\n        if k.startswith(\"@\") or k in (\"subClassOf\",):\n            continue\n        addrestriction(source, k, v)\n\n    return source\n</code></pre>"},{"location":"api_reference/datadoc/dataset/#tripper.datadoc.dataset.validate","title":"<code>validate(dct, type=None, context=None, keywords=None)</code>","text":"<p>Validates single-resource dict <code>dct</code>.</p> <p>Arguments     dct: Single-resource dict to validate.     type: The type of resource to validate. Ex: \"Dataset\", \"Agent\", ...     keywords: Keywords object defining the keywords used in <code>dct</code>.</p> <p>Exceptions:</p> Type Description <code>ValidateError</code> <p>If the validation fails.</p> Source code in <code>tripper/datadoc/dataset.py</code> <pre><code>def validate(\n    dct: dict,\n    type: \"Optional[str]\" = None,\n    context: \"Optional[Context]\" = None,\n    keywords: \"Optional[Keywords]\" = None,\n) -&gt; None:\n    \"\"\"Validates single-resource dict `dct`.\n\n    Arguments\n        dct: Single-resource dict to validate.\n        type: The type of resource to validate. Ex: \"Dataset\", \"Agent\", ...\n        keywords: Keywords object defining the keywords used in `dct`.\n\n    Raises:\n        ValidateError: If the validation fails.\n    \"\"\"\n    if keywords is None:\n        keywords = Keywords()\n\n    if context is None:\n        context = Context(keywords=keywords)\n\n    if type is None and \"@type\" in dct:\n        try:\n            type = keywords.typename(dct[\"@type\"])\n        except NoSuchTypeError:\n            pass\n\n    resources = keywords.data.resources\n\n    def check_keyword(keyword, type):\n        \"\"\"Check that the resource type `type` has keyword `keyword`.\"\"\"\n        typename = keywords.typename(type)\n        name = keywords.shortname(keyword)\n        if (\n            \"keywords\" in resources[typename]\n            and name in resources[typename].keywords\n        ):\n            return True\n        if \"subClassOf\" in resources[typename]:\n            subclass = resources[typename].subClassOf\n            return check_keyword(name, subclass)\n        return False\n\n    def _check_keywords(k, v):\n        if k in keywords:\n            r = keywords[k]\n            if \"datatype\" in r:\n                datatype = expand_iri(r.datatype, keywords.data.prefixes)\n                literal = parse_literal(v)\n                tr = {}\n                for t, seq in Literal.datatypes.items():\n                    for dt in seq:\n                        tr[dt] = t\n                if tr.get(literal.datatype) != tr.get(datatype):\n                    raise ValidateError(\n                        f\"invalid datatype for '{v}'. \"\n                        f\"Got '{literal.datatype}', expected '{datatype}'\"\n                    )\n            elif isinstance(v, dict):\n                validate(v, type=r.get(\"range\"), keywords=keywords)\n            elif isinstance(v, list):\n                for it in v:\n                    _check_keywords(k, it)\n            elif r.range != \"rdfs:Literal\" and not re.match(MATCH_IRI, v):\n                raise ValidateError(f\"value of '{k}' is an invalid IRI: '{v}'\")\n        elif k not in context:\n            raise ValidateError(f\"unknown keyword: '{k}'\")\n\n    # Check the keyword-value pairs in `dct`\n    for k, v in dct.items():\n        if k.startswith(\"@\"):\n            continue\n        _check_keywords(k, v)\n\n    if type:\n        typename = keywords.typename(type)\n\n        for k in dct:\n            if not k.startswith(\"@\"):\n                if not check_keyword(k, typename):\n                    logger.info(\n                        f\"unexpected keyword '{k}' provided for type: '{type}'\"\n                    )\n\n        for kr, vr in resources[typename].items():\n            if \"conformance\" in vr and vr.conformance == \"mandatory\":\n                if kr not in dct:\n                    raise ValidateError(\n                        f\"missing mandatory keyword '{kr}' for type: '{type}'\"\n                    )\n</code></pre>"},{"location":"api_reference/datadoc/errors/","title":"errors","text":"<p>Exceptions and warnings for the tripper.datadoc package.</p>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.DatadocValueError","title":"<code> DatadocValueError            (InvalidDatadocError, ValueError)         </code>","text":"<p>Invalid/inconsistent value (of correct type).</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class DatadocValueError(InvalidDatadocError, ValueError):\n    \"\"\"Invalid/inconsistent value (of correct type).\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.IRIExistsError","title":"<code> IRIExistsError            (TripperError)         </code>","text":"<p>The IRI already exists in the triplestore.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class IRIExistsError(TripperError):\n    \"\"\"The IRI already exists in the triplestore.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.IRIExistsWarning","title":"<code> IRIExistsWarning            (TripperWarning)         </code>","text":"<p>The IRI already exists in the triplestore.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class IRIExistsWarning(TripperWarning):\n    \"\"\"The IRI already exists in the triplestore.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.InvalidContextError","title":"<code> InvalidContextError            (TripperError)         </code>","text":"<p>Context is invalid.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class InvalidContextError(TripperError):\n    \"\"\"Context is invalid.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.InvalidDatadocError","title":"<code> InvalidDatadocError            (TripperError)         </code>","text":"<p>Invalid data documentation dict (or list).</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class InvalidDatadocError(TripperError):\n    \"\"\"Invalid data documentation dict (or list).\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.InvalidKeywordError","title":"<code> InvalidKeywordError            (InvalidDatadocError, KeyError)         </code>","text":"<p>Keyword is not defined.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class InvalidKeywordError(InvalidDatadocError, KeyError):  # remove?\n    \"\"\"Keyword is not defined.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.MissingKeyError","title":"<code> MissingKeyError            (InvalidDatadocError, KeyError)         </code>","text":"<p>Missing required key in description of a resource or keyword.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class MissingKeyError(InvalidDatadocError, KeyError):\n    \"\"\"Missing required key in description of a resource or keyword.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.NoSuchTypeError","title":"<code> NoSuchTypeError            (TripperError, KeyError)         </code>","text":"<p>There are no pre-defined type defined with the given name.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class NoSuchTypeError(TripperError, KeyError):\n    \"\"\"There are no pre-defined type defined with the given name.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.ParseError","title":"<code> ParseError            (TripperError)         </code>","text":"<p>Error when parsing a file.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class ParseError(TripperError):\n    \"\"\"Error when parsing a file.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.PrefixMismatchError","title":"<code> PrefixMismatchError            (TripperError)         </code>","text":"<p>Prefix mismatch between two sources.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class PrefixMismatchError(TripperError):\n    \"\"\"Prefix mismatch between two sources.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.RedefineError","title":"<code> RedefineError            (TripperWarning)         </code>","text":"<p>Trying to redefine an existing concept or keyword.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class RedefineError(TripperWarning):\n    \"\"\"Trying to redefine an existing concept or keyword.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.RedefineKeywordWarning","title":"<code> RedefineKeywordWarning            (TripperWarning)         </code>","text":"<p>Redefine an existing keyword (by mapping it to a new IRI).</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class RedefineKeywordWarning(TripperWarning):\n    \"\"\"Redefine an existing keyword (by mapping it to a new IRI).\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.SkipRedefineKeywordWarning","title":"<code> SkipRedefineKeywordWarning            (TripperWarning)         </code>","text":"<p>Skip redefining an existing keyword in a user-defined keyword definition (by mapping it to a new IRI).</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class SkipRedefineKeywordWarning(TripperWarning):\n    \"\"\"Skip redefining an existing keyword in a user-defined keyword\n    definition (by mapping it to a new IRI).\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.UnknownKeywordWarning","title":"<code> UnknownKeywordWarning            (TripperWarning)         </code>","text":"<p>Unknown keyword in data documentation.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class UnknownKeywordWarning(TripperWarning):\n    \"\"\"Unknown keyword in data documentation.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/errors/#tripper.datadoc.errors.ValidateError","title":"<code> ValidateError            (TripperError)         </code>","text":"<p>Error validating data documentation dict.</p> Source code in <code>tripper/datadoc/errors.py</code> <pre><code>class ValidateError(TripperError):\n    \"\"\"Error validating data documentation dict.\"\"\"\n</code></pre>"},{"location":"api_reference/datadoc/keywords/","title":"keywords","text":"<p>Parse keywords definition and generate context.</p>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords","title":"<code> Keywords        </code>","text":"<p>A class representing all keywords within a theme.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>class Keywords:\n    \"\"\"A class representing all keywords within a theme.\"\"\"\n\n    # pylint: disable=too-many-public-methods\n\n    rootdir = Path(__file__).absolute().parent.parent.parent.resolve()\n\n    def __init__(\n        self,\n        theme: \"Optional[Union[str, Sequence[str]]]\" = \"ddoc:datadoc\",\n        yamlfile: \"Optional[FileLoc]\" = None,\n        timeout: float = 3,\n    ) -&gt; None:\n        \"\"\"Initialises keywords object.\n\n        Arguments:\n            theme: IRI of one of more themes to load keywords for.\n            yamlfile: A YAML file with keyword definitions to parse.  May also\n                be an URI in which case it will be accessed via HTTP GET.\n                Deprecated. Use the `load_yaml()` or `add()` methods instead.\n            timeout: Timeout in case `yamlfile` is a URI.\n\n        Attributes:\n            data: The dict loaded from the keyword yamlfile.\n            keywords: A dict mapping keywords (name/prefixed/iri) to dicts\n                describing the keywords.\n            theme: IRI of a theme or scientic domain that the keywords\n                belong to.\n        \"\"\"\n        default_prefixes = AttrDict(ddoc=str(DDOC))\n        self.theme = None  # theme for this object\n        self.data = AttrDict(prefixes=default_prefixes, resources=AttrDict())\n\n        # A \"view\" into `self.data`. A dict mapping short, prefixed\n        # and expanded keyword names to corresponding value dicts in\n        # self.data.\n        self.keywords = AttrDict()\n\n        # Themes and files that has been parsed\n        self.parsed: \"set\" = set()\n\n        # Used for parsing dicts. Maps any of the elements in the\n        # value to the key (with highest precedence first).\n        self.input_mappings = {\n            \"label\": [\"skos:prefLabel\", \"rdfs:label\"],\n            \"description\": [\n                # TODO: Uncomment when EMMO has changed annotations to\n                # human readable IRIs\n                # \"emmo:elucidation\",\n                \"skos:definition\",\n                \"dcterms:description\",\n                \"rdfs:comment\",\n            ],\n            \"usageNote\": [\"vann:usageNote\", \"skos:scopeNote\"],\n            \"unit\": [\"ddoc:unitSymbol\"],\n            \"subPropertyOf\": [\"rdfs:subPropertyOf\"],\n            \"inverseOf\": [\"owl:inverseOf\"],\n        }\n\n        if theme:\n            self.add_theme(theme)\n\n        if yamlfile:\n            warnings.warn(\n                \"The `yamlfile` argument is deprecated. Use the `load_yaml()` \"\n                \"or `add()` methods instead.\",\n                DeprecationWarning,\n            )\n            if isinstance(yamlfile, (str, Path)):\n                self.load_yaml(yamlfile, timeout=timeout)\n            else:\n                for path in yamlfile:\n                    self.load_yaml(path, timeout=timeout)\n\n    def __contains__(self, item):\n        return item in self.keywords\n\n    def __getitem__(self, key):\n        return self.keywords[key]\n\n    def __iter__(self):\n        return iter(k for k in self.keywords if is_curie(k))\n\n    def __len__(self):\n        return len(list(self.__iter__()))\n\n    def __dir__(self):\n        return dir(Keywords) + [\"data\", \"keywords\", \"theme\"]\n\n    def __eq__(self, other):\n        return self.data == other.data and self.theme == other.theme\n\n    def _set_keyword(self, keywords, keyword, value, redefine=False):\n        \"\"\"Add new keyword-value pair to `keywords` dict.\"\"\"\n        # value = AttrDict(value)\n        expanded = expand_iri(value.iri, self.get_prefixes())\n        prefixed = prefix_iri(expanded, self.get_prefixes())\n        if redefine or keyword not in keywords:\n            keywords[keyword] = value\n        if redefine or prefixed not in keywords:\n            keywords[prefixed] = value\n        if redefine or expanded not in keywords:\n            keywords[expanded] = value\n\n    def _set_keywords(self, clear=True, redefine=False):\n        \"\"\"Update internal keywords attribute to data attribute.\n\n        Arguments:\n            clear: If false, only new keywords will be added, but nothing\n                removed.\n            redefine: Wheter to redefine existing keyword.\n        \"\"\"\n        if clear:\n            self.keywords.clear()\n        for clsvalue in self.data.get(\"resources\", AttrDict()).values():\n            for keyword, value in clsvalue.get(\"keywords\", AttrDict()).items():\n                self._set_keyword(\n                    self.keywords, keyword, value, redefine=redefine\n                )\n\n    def copy(self):\n        \"\"\"Returns a copy of self.\"\"\"\n        new = Keywords(theme=None)\n        new.theme = self.theme\n        new.data = deepcopy(self.data)\n        new.keywords = deepcopy(self.keywords)\n        new.parsed = self.parsed.copy()\n        return new\n\n    def add(\n        self,\n        keywords: \"Optional[KeywordsType]\",\n        format: \"Optional[Union[str, Sequence]]\" = None,\n        timeout: float = 3,\n        strict: bool = False,\n        redefine: str = \"raise\",\n    ) -&gt; None:\n        \"\"\"Add `keywords` to this Keywords object.\n\n        Arguments:\n            keywords: Keywords definitions to add to this Keyword object.\n                May be another Keyword object, path to a file, theme or a\n                sequence of these.\n            format: Format if `keywords`. Recognised formats include:\n                yaml, csv, tsv, turtle, xml, json-ld, rdfa, ...\n            timeout: Timeout when accessing remote files.\n            strict: Whether to raise an `InvalidKeywordError` exception if `d`\n                contains an unknown key.\n            redefine: Determine how to handle redefinition of existing\n                keywords.  Should be one of the following strings:\n                  - \"allow\": Allow redefining a keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"skip\": Don't redefine existing keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"raise\": Raise an RedefineError (default).\n\n        \"\"\"\n        if not isinstance(keywords, str) and isinstance(keywords, Sequence):\n            if isinstance(format, str):\n                format = [format] * len(keywords)\n            elif format and len(format) != len(keywords):\n                raise TypeError(\n                    \"If given, `format` must have the same length as \"\n                    \"`keywords`\"\n                )\n\n        def _add(kw, fmt):\n            if kw is None:\n                pass\n            elif isinstance(kw, Keywords):\n                self.theme = merge(self.theme, kw.theme)\n                recursive_update(self.data, kw.data, cls=AttrDict)\n                self._set_keywords(clear=False)\n            elif isinstance(kw, dict):\n                self._load_yaml(kw, strict=strict, redefine=redefine)\n            elif not isinstance(kw, str) and isinstance(kw, Sequence):\n                for i, e in enumerate(kw):\n                    _add(e, fmt[i] if fmt else None)\n            elif isinstance(kw, (str, Path, IOBase)):\n                if (\n                    isinstance(kw, str)\n                    and \":\" in kw\n                    and not (\n                        kw.startswith(\"/\") or kw.startswith(\"./\") or is_uri(kw)\n                    )\n                ):\n                    self.add_theme(\n                        kw,\n                        timeout=timeout,\n                        strict=strict,\n                        redefine=redefine,\n                    )\n                else:\n                    if not fmt:\n                        name = kw.name if hasattr(kw, \"name\") else kw\n                        fmt = Path(name).suffix\n                    fmt = fmt.lstrip(\".\").lower()\n                    # pylint:disable=consider-using-get\n                    if fmt in RDFLIB_SUFFIX_FORMAT_MAP:\n                        fmt = RDFLIB_SUFFIX_FORMAT_MAP[fmt]\n\n                    if fmt in (\"yaml\", \"yml\"):\n                        self.load_yaml(\n                            kw,\n                            timeout=timeout,\n                            strict=strict,\n                            redefine=redefine,\n                        )\n                    elif fmt in (\"csv\", \"tsv\", \"xlsx\", \"excel\"):\n                        self.load_table(kw, format=fmt)\n                    else:\n                        self.load_rdffile(\n                            kw,\n                            format=fmt,\n                            timeout=timeout,\n                            strict=strict,\n                            redefine=redefine,\n                        )\n            else:\n                raise TypeError(\n                    \"`keywords` must be a KeywordsType object (Keywords \"\n                    \"instance, dict, IO, Path, string or sequence). \"\n                    f\"Got: {type(kw)}\"\n                )\n\n        _add(keywords, format)\n\n    def add_theme(\n        self,\n        theme: \"Union[str, Sequence[str]]\",\n        timeout: float = 3,\n        strict: bool = False,\n        redefine: str = \"raise\",\n    ) -&gt; None:\n        \"\"\"Add keywords for `theme`, where `theme` is the IRI of a\n        theme or scientific domain or a list of such IRIs.\n\n        Arguments:\n            theme: IRI (or list of IRIs) of a theme/scientific domain to load.\n            timeout: Timeout when accessing remote files.\n            strict: Whether to raise an `InvalidKeywordError` exception if the\n                theme contains an unknown key.\n            redefine: Determine how to handle redefinition of existing\n                keywords.  Should be one of the following strings:\n                  - \"allow\": Allow redefining a keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"skip\": Don't redefine existing keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"raise\": Raise an RedefineError (default).\n\n        \"\"\"\n        if isinstance(theme, str):\n            theme = [theme]\n\n        parsedkey = (tuple(theme), strict, redefine)\n        if parsedkey in self.parsed:\n            return\n\n        for name in theme:  # type: ignore\n            expanded = expand_iri(name, self.get_prefixes())\n            prefixed = prefix_iri(name, self.get_prefixes())\n            add(\n                self.data,\n                \"theme\",\n                prefixed,\n            )\n            for ep in get_entry_points(\"tripper.keywords\"):\n                if expand_iri(ep.value, self.get_prefixes()) == expanded:\n                    package_name, path = ep.name.split(\"/\", 1)\n                    package = import_module(package_name)\n                    fullpath = (\n                        Path(package.__file__).parent / path  # type: ignore\n                    )\n                    self.add(\n                        fullpath,\n                        timeout=timeout,\n                        strict=strict,\n                        redefine=redefine,\n                    )\n                    break\n            else:\n                # Fallback in case the entry point is not installed\n                if expanded == DDOC.datadoc:\n                    self.load_yaml(\n                        self.rootdir\n                        / \"tripper\"\n                        / \"context\"\n                        / \"0.3\"\n                        / \"keywords.yaml\",\n                        timeout=timeout,\n                        strict=strict,\n                        redefine=redefine,\n                    )\n                else:\n                    raise TypeError(f\"Unknown theme: {name}\")\n\n        self.parsed.add(parsedkey)\n\n    def load_yaml(\n        self,\n        yamlfile: \"Union[Path, str]\",\n        timeout: float = 3,\n        strict: bool = True,\n        redefine: str = \"raise\",\n    ) -&gt; None:\n        \"\"\"Load YAML file with keyword definitions.\n\n        Arguments:\n            yamlfile: Path of URL to a YAML file to load.\n            timeout: Timeout when accessing remote files.\n            strict: Whether to raise an `InvalidKeywordError` exception if `d`\n                contains an unknown key.\n            redefine: Determine how to handle redefinition of existing\n                keywords.  Should be one of the following strings:\n                  - \"allow\": Allow redefining a keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"skip\": Don't redefine existing keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"raise\": Raise an RedefineError (default).\n\n        \"\"\"\n        parsedkey = (yamlfile, strict, redefine)\n        if parsedkey in self.parsed:\n            return\n\n        with openfile(yamlfile, timeout=timeout, mode=\"rt\") as f:\n            d = yaml.safe_load(f)\n        try:\n            self._load_yaml(d, strict=strict, redefine=redefine)\n        except Exception as exc:\n            raise ParseError(f\"error parsing '{yamlfile}'\") from exc\n\n        self.parsed.add(parsedkey)\n\n    def _load_yaml(\n        self,\n        d: dict,\n        strict: bool = True,\n        redefine: str = \"raise\",\n    ) -&gt; None:\n        \"\"\"Parse a dict with keyword definitions following the format of\n        the YAML file.\n\n        Arguments:\n            d: Dict defining a keyword following the YAML file format.\n            strict: Whether to raise an `InvalidKeywordError` exception if `d`\n                contains an unknown key.\n            redefine: Determine how to handle redefinition of existing\n                keywords.  Should be one of the following strings:\n                  - \"allow\": Allow redefining a keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"skip\": Don't redefine existing keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"raise\": Raise an RedefineError (default).\n\n        \"\"\"\n        # pylint: disable=too-many-nested-blocks,too-many-statements\n        # pylint: disable=too-many-locals\n        self.add(d.get(\"basedOn\"))\n\n        required_resource_keys = {\"iri\"}\n        valid_resource_keys = {\n            \"iri\",\n            \"subClassOf\",\n            \"description\",\n            \"usageNote\",\n            \"keywords\",\n        }\n        required_keywords = {\"iri\"}\n        valid_keywords = {\n            \"name\",\n            \"iri\",\n            \"type\",\n            \"subPropertyOf\",\n            \"inverseOf\",  # XXX - to be implemented\n            \"domain\",\n            \"range\",\n            \"datatype\",\n            \"inverse\",  # XXX - to be implemented (remove?)\n            \"unit\",  # XXX - to be implemented\n            \"conformance\",\n            \"description\",\n            \"usageNote\",\n            \"theme\",\n            \"default\",\n        }\n        iri_keywords = {\n            \"iri\",\n            \"type\",\n            \"subPropertyOf\",\n            \"inverseOf\",\n            \"domain\",\n            \"range\",\n            \"datatype\",\n        }\n        valid_conformances = [\"mandatory\", \"recommended\", \"optional\"]\n\n        def to_prefixed(x):\n            \"\"\"Help function that converts an IRI or list of IRIs to\n            prefixed IRIs.\"\"\"\n            if isinstance(x, str):\n                return self.prefixed(x, strict=False)\n            return [to_prefixed(e) for e in x]\n\n        # Create a deep copies that we are updating\n        prefixes = deepcopy(self.data.prefixes)\n        resources = deepcopy(self.data.resources)\n        keywords = deepcopy(self.keywords)\n\n        # Prefixes\n        for prefix, ns in d.get(\"prefixes\", AttrDict()).items():\n            if prefix in prefixes and ns != prefixes[prefix]:\n                raise PrefixMismatchError(\n                    f\"prefix '{prefix}' is already mapped to \"\n                    f\"'{prefixes[prefix]}'. Cannot redefine it to '{ns}'\"\n                )\n            prefixes[prefix] = ns\n\n        # Map keywords IRIs to keyword definitions\n        iridefs = {}\n        for defs in d.get(\"resources\", {}).values():\n            for kw, val in defs.get(\"keywords\", {}).items():\n                # Check that value has all the required keywords\n                for k in required_keywords:\n                    if k not in val:\n                        raise MissingKeyError(f\"no '{k}' in keyword '{kw}'\")\n                key = prefix_iri(val[\"iri\"], prefixes)\n                if len(val) &gt; 1 or key not in iridefs:\n                    iridefs[key] = val\n\n        # Resources\n        for cls, defs in d.get(\"resources\", AttrDict()).items():\n            resval = resources.get(cls, AttrDict())\n\n            defs = AttrDict(defs).copy()\n            for key in required_resource_keys:\n                if key not in defs:\n                    raise MissingKeyError(\n                        f\"missing required key '{key}' for resource '{cls}'\"\n                    )\n            for key in defs:\n                if strict and key not in valid_resource_keys:\n                    raise InvalidDatadocError(f\"invalid resource key: '{key}'\")\n            # TODO: Check for redefinition of existing class\n\n            resval.iri = prefix_iri(defs.iri, prefixes)\n            if \"subClassOf\" in defs:\n                resval.subClassOf = to_prefixed(defs.subClassOf)\n            if \"description\" in defs:\n                resval.description = defs.description\n            if \"usageNote\" in defs:\n                resval.usageNote = defs.usageNote\n            resval.setdefault(\"keywords\", AttrDict())\n\n            for keyword, value in defs.get(\"keywords\", AttrDict()).items():\n\n                # If a value only contain an IRI, replace it with a more\n                # elaborate definition (if it exists)\n                if len(value) == 1:\n                    value = AttrDict(iridefs[value[\"iri\"]])\n\n                # Check conformance values\n                if \"conformance\" in value:\n                    c = value[\"conformance\"]\n                    if c not in valid_conformances:\n                        raise DatadocValueError(f\"invalid conformance: {c}\")\n\n                # If strict, check that all keys are known\n                if strict:\n                    for k in value.keys():\n                        if k not in valid_keywords:\n                            raise InvalidKeywordError(\n                                f\"keyword '{keyword}' has invalid key: {k}\"\n                            )\n\n                # Normalise IRIs in values to prefixed IRIs\n                value = AttrDict(value).copy()\n                for k in iri_keywords:\n                    if k in value:\n                        value[k] = to_prefixed(value[k])\n\n                # Add extra annotations to value\n                if \"name\" not in value or \":\" in value.name:\n                    value.name = keyword\n                if \"theme\" in d:\n                    add(value, \"theme\", d[\"theme\"])\n                add(value, \"domain\", prefix_iri(defs.iri, prefixes))\n\n                # Check whether we try to redefine an existing keyword\n                skip = False\n                if keyword in keywords:\n                    for k, v in value.items():\n                        oldval = keywords[keyword].get(k)\n                        if k in (\"iri\", \"domain\") or v == oldval:\n                            continue\n                        oldiri = keywords[keyword].iri\n                        if value.iri == oldiri:\n                            if redefine != \"allow\":\n                                raise RedefineError(\n                                    \"Cannot redefine existing concept \"\n                                    f\"'{value.iri}'. Trying to change \"\n                                    f\"property '{k}' from '{oldval}' to \"\n                                    f\"'{v}'.\"\n                                )\n                            warnings.warn(\n                                \"Redefining existing concept \"\n                                f\"'{value.iri}'. Change property \"\n                                f\"'{k}' from '{oldval}' to '{v}'.\",\n                                RedefineKeywordWarning,\n                            )\n                        if redefine == \"raise\":\n                            raise RedefineError(\n                                f\"Trying to redefine keyword \"\n                                f\"'{keyword}' from '{oldiri}' \"\n                                f\"to '{value.iri}'.\"\n                            )\n                        if redefine == \"skip\":\n                            skip = True\n                            warnings.warn(\n                                f\"Skip redefinition of keyword: {keyword}\",\n                                SkipRedefineKeywordWarning,\n                            )\n                        elif redefine == \"allow\":\n                            warnings.warn(\n                                f\"Redefining keyword '{keyword}' from \"\n                                f\"'{oldiri}' to '{value.iri}'.\",\n                                RedefineKeywordWarning,\n                            )\n                        else:\n                            raise ValueError(\n                                \"Invalid value of `redefine` \"\n                                f'argument: \"{redefine}\".  Should be '\n                                'one of \"allow\", \"keep\" or \"raise\".'\n                            )\n                        break\n                if skip:\n                    continue\n\n                kw = resval.keywords\n                if keyword in kw:\n                    kw[keyword].update(value)\n                else:\n                    kw[keyword] = value\n\n                self._set_keyword(keywords, keyword, value, redefine=True)\n\n            if cls in resources:\n                resources[cls].update(resval)\n            else:\n                resources[cls] = resval\n\n        # Everything succeeded, update instance\n        self.data.prefixes.update(prefixes)\n        self.data.resources.update(resources)\n        self.keywords.update(keywords)\n\n        # Run an extra round and add keywords we have missed.\n        self._set_keywords(clear=False, redefine=False)\n\n    def save_yaml(\n        self,\n        yamlfile: \"Union[Path, str]\",\n        keywords: \"Optional[Sequence[str]]\" = None,\n        classes: \"Optional[Union[str, Sequence[str]]]\" = None,\n        themes: \"Optional[Union[str, Sequence[str]]]\" = None,\n        namespace_filter: \"Optional[Union[str, Sequence[str]]]\" = None,\n    ) -&gt; None:\n        \"\"\"Save YAML file with keyword definitions.\n\n        Arguments:\n            yamlfile: File to save keyword definitions to.\n            keywords: Sequence of keywords to include.\n            classes: Include keywords that have these classes in their domain.\n            themes: Include keywords for these themes.\n            namespace_filter: A prefix, namespace or a sequence of these.\n                If given, keep only keywords and classes from the returned\n                `keywordset` and `classet` with IRIs in one of\n                these namespaces.\n\n        \"\"\"\n        keywords, classes, themes = self._keywords_list(\n            keywords, classes, themes, namespace_filter=namespace_filter\n        )\n        resources = {}\n        for cls, clsval in self.data.resources.items():\n            if self.prefixed(cls) in classes:\n                resources[cls] = dict(clsval.copy())\n                resources[cls][\"keywords\"] = {}\n                for k, v in self.data.resources[cls].keywords.items():\n                    if self.prefixed(k) in keywords:\n                        resources[cls][\"keywords\"][k] = dict(v)\n        data = dict(self.data.copy())\n        del data[\"resources\"]\n        recursive_update(data, {}, cls=dict)\n        data[\"resources\"] = resources\n\n        with open(yamlfile, \"wt\", encoding=\"utf-8\") as f:\n            yaml.safe_dump(data, f, sort_keys=False)\n\n    def load_table(\n        self,\n        filename: \"FileLoc\",\n        format: \"Optional[str]\" = None,  # pylint: disable=unused-argument\n        prefixes: \"Optional[dict]\" = None,\n        theme: \"Optional[str]\" = None,\n        basedOn: \"Optional[Union[str, List[str]]]\" = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Load keywords from a csv file.\n\n        Arguments:\n            filename: File to load.\n            format: File format. Unused.  Only csv is currently supported.\n            prefixes: Dict with additional prefixes used in the table.\n            theme: Theme defined by the table.\n            basedOn: Theme(s) that the table is based on.\n            kwargs: Keyword arguments passed on to TableDoc.parse_csv().\n        \"\"\"\n        # pylint: disable=import-outside-toplevel\n        from tripper.datadoc.tabledoc import TableDoc\n\n        td = TableDoc.parse_csv(\n            filename, type=None, prefixes=prefixes, **kwargs\n        )\n        dicts = td.asdicts()\n        self.fromdicts(dicts, prefixes=prefixes, theme=theme, basedOn=basedOn)\n\n    def save_table(\n        self,\n        filename: \"FileLoc\",\n        format: \"Optional[str]\" = None,  # pylint: disable=unused-argument\n        names: \"Optional[Sequence]\" = None,\n        strip: bool = True,\n        keymode: str = \"name\",\n        **kwargs,\n    ) -&gt; None:\n        # pylint: disable=line-too-long\n        \"\"\"Load keywords from a csv file.\n\n        Arguments:\n            filename: File to load.\n            format: File format. Unused.  Only csv is currently supported.\n            names: A sequence of keyword or class names to save.  The\n                default is to save all keywords.\n            strip: Whether to strip leading and trailing whitespaces\n                from cells.\n            keymode: How to represent column headers.  Should be either\n                \"name\", \"prefixed\" (CURIE) or \"expanded\" (full IRI).\n            kwargs: Additional keyword arguments passed to the writer.\n                For more details, see [write_csv()].\n\n        References:\n        [write_csv()]: https://emmc-asbl.github.io/tripper/latest/api_reference/datadoc/tabledoc/#tripper.datadoc.tabledoc.TableDoc.write_csv\n        \"\"\"\n        # pylint: disable=import-outside-toplevel\n        from tripper.datadoc.tabledoc import TableDoc\n\n        dicts = self.asdicts(names, keymode=keymode)\n        td = TableDoc.fromdicts(dicts, type=None, keywords=self, strip=strip)\n        td.write_csv(filename, **kwargs)\n\n    def keywordnames(self) -&gt; \"list\":\n        \"\"\"Return a list with all keyword names defined in this instance.\"\"\"\n        return [k for k in self.keywords.keys() if \":\" not in k]\n\n    def classnames(self) -&gt; \"list\":\n        \"\"\"Return a list with all class names defined in this instance.\"\"\"\n        return list(self.data.resources.keys())\n\n    def asdicts(\n        self,\n        names: \"Optional[Sequence]\" = None,\n        keymode: str = \"prefixed\",\n    ) -&gt; \"List[dict]\":\n        \"\"\"Return the content of this Keywords object as a list of JSON-LD\n        dicts.\n\n        Arguments:\n            names: A sequence of keyword or class names.  The\n                default is to return all keywords.\n            keymode: How to represent keys.  Should be either \"name\",\n                \"prefixed\" (CURIE) or \"expanded\" (full IRI).\n\n        Returns:\n            List of JSON-LD dicts corresponding to `names`.\n        \"\"\"\n        keymodes = {\n            \"name\": iriname,\n            \"prefixed\": None,\n            \"expanded\": self.expanded,\n        }\n        # TODO: use `self.input_mappings` instead\n        maps = {\n            \"subPropertyOf\": \"rdfs:subPropertyOf\",\n            \"unit\": \"ddoc:unitSymbol\",\n            \"description\": \"dcterms:description\",\n            \"usageNote\": \"vann:usageNote\",\n            \"theme\": \"dcat:theme\",\n        }\n\n        def key(k):\n            \"\"\"Return key `k` accordig to `keymode`.\"\"\"\n            return keymodes[keymode](k) if keymodes[keymode] else k\n\n        conformance_indv = {v: k for k, v in CONFORMANCE_MAPS.items()}\n        if names is None:\n            names = self.keywordnames()\n\n        classes = []\n        dicts = []\n        for name in names:\n            if name not in self.keywords:\n                classes.append(name)\n                continue\n            d = self.keywords[name]\n            if \"range\" in d and self.expanded(d.range) != RDFS.Literal:\n                proptype = \"owl:ObjectProperty\"\n                range = d.range\n            elif (\n                \"datatype\" in d and self.expanded(d.datatype) != RDF.langString\n            ):\n                proptype = \"owl:DatatypeProperty\"\n                range = d.get(\"datatype\")\n            else:\n                proptype = \"owl:AnnotationProperty\"\n                range = d.get(\"datatype\")\n\n            dct = {\n                \"@id\": d.iri,\n                \"@type\": proptype,\n                key(\"rdfs:label\"): d.name,\n            }\n            if \"domain\" in d:\n                dct[key(\"rdfs:domain\")] = d.domain\n            if range:\n                dct[key(\"rdfs:range\")] = range\n            if \"conformance\" in d:\n                dct[key(\"ddoc:conformance\")] = conformance_indv.get(\n                    d.conformance, d.conformance\n                )\n            for k, v in d.items():\n                if k in maps:\n                    dct[key(maps[k])] = v\n            dicts.append(dct)\n\n        if classes:\n            classmaps = {}\n            for k, v in self.data.resources.items():\n                classmaps[k] = k\n                classmaps[self.expanded(k)] = k\n                classmaps[self.prefixed(k)] = k\n\n            for name in classes:\n                d = self.data.resources[classmaps[name]]\n                dct = {\"@id\": d.iri, \"@type\": OWL.Class}\n                if \"subClassOf\" in d:\n                    dct[key(\"rdfs:subClassOf\")] = d.subClassOf\n                if \"description\" in d:\n                    dct[key(\"dcterms:description\")] = d.description\n                if \"usageNote\" in d:\n                    dct[key(\"vann:usageNote\")] = d.usageNote\n                dicts.append(dct)\n\n        return dicts\n\n    def fromdicts(\n        self,\n        dicts: \"Sequence[dict]\",\n        prefixes: \"Optional[dict]\" = None,\n        theme: \"Optional[str]\" = None,\n        basedOn: \"Optional[Union[str, List[str]]]\" = None,\n        strict: bool = False,\n        redefine: str = \"raise\",\n    ) -&gt; None:\n        \"\"\"Populate this Keywords object from a sequence of dicts.\n\n        Arguments:\n            dicts: A sequence of JSON-LD dicts to populate this keywords object\n                from.  Their format should follow what is returned by\n                tripper.datadoc.acquire().\n            prefixes: Dict with additional prefixes used by `dicts`.\n            theme: Theme defined by `dicts`.\n            basedOn: Theme(s) that `dicts` are based on.\n            strict: Whether to raise an `InvalidKeywordError` exception if `d`\n                contains an unknown key.\n            redefine: Determine how to handle redefinition of existing\n                keywords.  Should be one of the following strings:\n                  - \"allow\": Allow redefining a keyword.\n                  - \"skip\": Don't redefine existing keyword.\n                  - \"raise\": Raise an RedefineError (default).\n\n        \"\"\"\n        data = self._fromdicts(\n            dicts,\n            prefixes=prefixes,\n            theme=theme,\n            basedOn=basedOn,\n        )\n        self._load_yaml(data, strict=strict, redefine=redefine)\n\n    def _fromdicts(\n        self,\n        dicts: \"Sequence[dict]\",\n        prefixes: \"Optional[dict]\" = None,\n        theme: \"Optional[str]\" = None,\n        basedOn: \"Optional[Union[str, List[str]]]\" = None,\n    ) -&gt; dict:\n        \"\"\"Help method for `fromdicts()` that returns a dict with\n        keyword definitions following the format of the YAML file.\n        \"\"\"\n        # pylint: disable=too-many-locals,too-many-statements\n\n        def to_prefixed(x, prefixes, strict=True):\n            \"\"\"Help function that converts an IRI or list of IRIs to\n            prefixed IRIs.\"\"\"\n            if isinstance(x, str):\n                return prefix_iri(x, prefixes, strict=strict)\n            return [to_prefixed(e, prefixes, strict=strict) for e in x]\n\n        # Prefixes (merged with self.data.prefixes)\n        p = self.get_prefixes().copy()\n        if prefixes:\n            for prefix, ns in prefixes.items():\n                if prefix in p and p[prefix] != ns:\n                    raise PrefixMismatchError(\n                        f\"adding prefix `{prefix}: {ns}` but it is already \"\n                        f\"defined to '{p[prefix]}'\"\n                    )\n            p.update({k: str(v) for k, v in prefixes.items()})\n\n        def isproperty(v):\n            if \"@type\" not in v:\n                return False\n            types = [v[\"@type\"]] if isinstance(v[\"@type\"], str) else v[\"@type\"]\n            for t in types:\n                exp = expand_iri(t, p, strict=True)\n                if exp in (\n                    OWL.AnnotationProperty,\n                    OWL.ObjectProperty,\n                    OWL.DatatypeProperty,\n                    RDF.Property,\n                ):\n                    return True\n            return False\n\n        entities = {expand_iri(d[\"@id\"], p): d for d in dicts}\n        properties = {k: v for k, v in entities.items() if isproperty(v)}\n        classes = {k: v for k, v in entities.items() if k not in properties}\n\n        data = AttrDict()\n        if theme:\n            data.theme = theme\n        if basedOn:\n            data.basedOn = basedOn\n        data.prefixes = p\n        data.resources = AttrDict()\n        resources = data.resources\n\n        # Expand input mappings\n        input_mappings = {}\n        for key, maps in self.input_mappings.items():\n            s = []\n            for m in maps:\n                s.append(self.expanded(m, strict=False))\n                s.append(self.prefixed(m, strict=False))\n                s.append(iriname(m))\n            input_mappings[key] = s\n\n        # Add classes\n        clslabels = {}\n        for k, v in classes.items():\n            d = AttrDict(iri=prefix_iri(k, p))\n            for key, maps in input_mappings.items():\n                for m in maps:\n                    if m in v:\n                        d[key] = v[m]\n                        break\n            if \"subClassOf\" in v and isinstance(v[\"subClassOf\"], str):\n                d[\"subClassOf\"] = to_prefixed(v[\"subClassOf\"], p, strict=True)\n            d.setdefault(\"keywords\", AttrDict())\n            # label = d.pop(\"label\") if \"label\" in d else iriname(k)\n            for key in d:\n                if key in input_mappings[\"label\"]:\n                    label = d.pop(key)\n                    break\n            else:\n                label = iriname(k)\n            resources[label] = d\n            clslabels[d.iri] = label\n\n        # Add properties\n        for propname, value in properties.items():\n            name = iriname(propname)\n            label = value[\"label\"] if \"label\" in value else name\n            d = AttrDict(iri=value[\"@id\"])\n            if \"@type\" in value:\n                d.type = to_prefixed(value[\"@type\"], p)\n            d.domain = value.get(\"domain\", RDFS.Resource)\n\n            for domain in asseq(d.domain):\n                dlabel = prefix_iri(domain, p, strict=True)\n                domainname = clslabels.get(dlabel, iriname(domain))\n                if domainname not in resources:\n                    if domainname not in self.data.resources:\n                        if domainname not in (\"Resource\",):\n                            logger.info(\n                                f\"Adding undefined domain '{domain}' for \"\n                                f\"keyword '{label}'\"\n                            )\n                        r = AttrDict(\n                            iri=prefix_iri(domain, p),\n                            keywords=AttrDict(),\n                        )\n                    else:\n                        r = self.data.resources[domainname].copy()\n                    resources[domainname] = r\n                    r.keywords[label] = d\n                else:\n                    resources[domainname].keywords[label] = d\n            if \"range\" in value:\n                _types = asseq(d.get(\"type\", OWL.AnnotationProperty))\n                types = [expand_iri(t, p) for t in _types]\n                if OWL.ObjectProperty in types:\n                    d.range = value[\"range\"]\n                else:\n                    d.range = \"rdfs:Literal\"\n                    if \"range\" in value:\n                        d.datatype = value[\"range\"]\n            else:\n                d.range = \"rdfs:Literal\"\n                # TODO: Define if we accept missing datatype for literals\n            if \"conformance\" in value:\n                d.conformance = CONFORMANCE_MAPS[value[\"conformance\"]]\n            for key, maps in input_mappings.items():\n                for m in maps:\n                    if key != \"label\" and m in value:\n                        d.setdefault(key, value[m])\n                        break\n        return data\n\n    def missing_keywords(\n        self,\n        ts: \"Triplestore\",\n        include_classes: bool = False,\n        return_existing: bool = False,\n    ) -&gt; \"Union[list, Tuple[list, list]]\":\n        \"\"\"List keywords not defined in triplestore `ts`.\n\n        Arguments:\n            ts: Triplestore object to check.\n            include_classes: Also return missing classes.\n            return_existing: If true, two lists are returned:\n                - list of keywords missing in `ts`\n                - list of keywords existing in `ts`\n\n        Returns:\n            List with the names of keywords in this instance that are\n            not defined in triplestore `ts`.\n        \"\"\"\n        expanded = {k for k in self.keywords.keys() if \"://\" in k}\n        if include_classes:\n            expanded.update(self.expanded(c) for c in self.classnames())\n\n        if not expanded:\n            return []\n\n        query = f\"\"\"\n        SELECT ?s WHERE {{\n          VALUES ?s {{ { ' '.join(f'&lt;{iri}&gt;' for iri in expanded) } }}\n          ?s a ?o\n        }}\n        \"\"\"\n        existing = {r[0] for r in ts.query(query)}\n        missing = expanded.difference(existing)\n        missing_names = [self.shortname(k) for k in missing]\n\n        if return_existing:\n            existing_names = [self.keywords[k].name for k in existing]\n            return missing_names, existing_names\n        return missing_names\n\n    def _load_rdf(\n        self, ts: \"Triplestore\", iris: \"Optional[Sequence[str]]\" = None\n    ) -&gt; \"Sequence[dict]\":\n        \"\"\"Help method for load_rdf(). Returns dicts loaded from triplestore\n        `ts`.\n\n        If `iris` is not given, all OWL properties in `ts` will be loaded.\n        \"\"\"\n        # pylint: disable=import-outside-toplevel,too-many-nested-blocks\n        # pylint: disable=too-many-locals\n        from tripper.datadoc.context import Context\n        from tripper.datadoc.dataset import acquire\n\n        context = Context(self.get_context())\n\n        if iris is None:\n            query = \"\"\"\n            PREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\n            PREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n            SELECT DISTINCT ?s WHERE {\n              VALUES ?o {\n                owl:DatatypeProperty owl:ObjectProperty owl:AnnotationProperty\n                rdf:Property\n              }\n              ?s a ?o .\n            }\n            \"\"\"\n            iris = [iri[0] for iri in ts.query(query)]\n\n        prefixes = self.data.prefixes\n        for prefix, ns in ts.namespaces.items():\n            self.add_prefix(prefix, ns)\n\n        dicts = [\n            AttrDict(acquire(ts, iri, context=context).items()) for iri in iris\n        ]\n        dct = {expand_iri(d[\"@id\"], prefixes): d for d in dicts}\n\n        # FIXME: Add domain and range to returned dicts\n        # Add domain and range to dicts\n        seen = set()\n        for d in list(dct.values()):\n            for ref in (\"domain\", \"range\"):\n                if ref in d:\n                    for domain in asseq(d[ref]):\n                        expanded = expand_iri(domain, prefixes)\n                        if expanded.startswith(str(XSD)):\n                            continue\n                        if expanded not in seen:\n                            seen.add(expanded)\n                            acquired = acquire(ts, expanded, context=context)\n                            if acquired:\n                                dct[expanded] = acquired  # type: ignore\n\n        newdicts = list(dct.values())\n        return newdicts\n\n    def save_rdf(self, ts: \"Triplestore\") -&gt; dict:\n        \"\"\"Save to triplestore.\"\"\"\n        # pylint: disable=import-outside-toplevel,cyclic-import\n        from tripper.datadoc.dataset import store\n\n        for prefix, ns in self.get_prefixes().items():\n            ts.bind(prefix, ns)\n\n        # Ensure that the schema for properties is stored\n        load_datadoc_schema(ts)\n\n        # Store all keywords that are not already in the triplestore\n        missing = self.missing_keywords(ts, include_classes=True)\n        dicts = self.asdicts(missing)\n        return store(ts, dicts)\n\n    def load_rdf(\n        self,\n        ts: \"Triplestore\",\n        iris: \"Optional[Sequence[str]]\" = None,\n        strict: bool = False,\n        redefine: str = \"raise\",\n    ) -&gt; None:\n        \"\"\"Populate this Keyword object from a triplestore.\n\n        Arguments:\n            ts: Triplestore to load keywords from.\n            iris: IRIs to load. The default is to load IRIs corresponding to all\n                properties an classes.\n            strict: Whether to raise an `InvalidKeywordError` exception if `d`\n                contains an unknown key.\n            redefine: Determine how to handle redefinition of existing\n                keywords.  Should be one of the following strings:\n                  - \"allow\": Allow redefining a keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"skip\": Don't redefine existing keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"raise\": Raise an RedefineError (default).\n\n        \"\"\"\n        dicts = self._load_rdf(ts, iris)\n        self.fromdicts(\n            dicts,\n            prefixes=ts.namespaces,\n            strict=strict,\n            redefine=redefine,\n        )\n\n    def load_rdffile(\n        self,\n        rdffile: \"FileLoc\",\n        format: \"Optional[str]\" = None,\n        timeout: float = 3,\n        iris: \"Optional[Sequence[str]]\" = None,\n        strict: bool = False,\n        redefine: str = \"raise\",\n    ) -&gt; None:\n        \"\"\"Load RDF from file or URL.\n\n        Arguments:\n            rdffile: File to load.\n            format: Any format supported by rdflib.Graph.parse().\n            timeout: Timeout in case `yamlfile` is a URI.\n            iris: IRIs to load. The default is to load IRIs corresponding to\n                all properties an classes.\n            strict: Whether to raise an `InvalidKeywordError` exception if `d`\n                contains an unknown key.\n            redefine: Determine how to handle redefinition of existing\n                keywords.  Should be one of the following strings:\n                  - \"allow\": Allow redefining a keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"skip\": Don't redefine existing keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"raise\": Raise an RedefineError (default).\n\n        \"\"\"\n        if format is None:\n            format = guess_rdf_format(rdffile)\n\n        ts = Triplestore(\"rdflib\")\n        with openfile(rdffile, timeout=timeout, mode=\"rt\") as f:\n            ts.parse(f, format=format)\n        self.load_rdf(ts, iris=iris, strict=strict, redefine=redefine)\n\n    def isnested(self, keyword: str) -&gt; bool:\n        \"\"\"Returns whether the keyword corresponds to an object property.\"\"\"\n        d = self.keywords[keyword]\n        if \"datatype\" in d or d.range == \"rdfs:Literal\":\n            return False\n        return True\n\n    def expanded(self, keyword: str, strict: bool = True) -&gt; str:\n        \"\"\"Return the keyword expanded to its full IRI.\"\"\"\n        if keyword in self.keywords:\n            iri = self.keywords[keyword].iri\n        elif \"resources\" in self.data and keyword in self.data.resources:\n            iri = self.data.resources[keyword].iri\n        elif \":\" in keyword or not strict:\n            iri = keyword\n        else:\n            raise InvalidKeywordError(keyword)\n        return expand_iri(iri, self.get_prefixes(), strict=strict)\n\n    def range(self, keyword: str) -&gt; str:\n        \"\"\"Return the range of the keyword.\"\"\"\n        return self.keywords[keyword].range\n\n    def superclasses(self, cls: str) -&gt; \"Union[str, list]\":\n        \"\"\"Return a list with `cls` and it superclasses prefixed.\n\n        Example:\n\n        &gt;&gt;&gt; keywords = Keywords()\n        &gt;&gt;&gt; keywords.superclasses(\"Dataset\")\n        ... # doctest: +NORMALIZE_WHITESPACE\n        ['dcat:Dataset',\n         'dcat:Resource',\n         'emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a']\n\n        &gt;&gt;&gt; keywords.superclasses(\"dcat:Dataset\")\n        ... # doctest: +NORMALIZE_WHITESPACE\n        ['dcat:Dataset',\n         'dcat:Resource',\n         'emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a']\n\n        \"\"\"\n        if cls in self.data.resources:\n            r = self.data.resources[cls]\n        else:\n            cls = prefix_iri(cls, self.get_prefixes())\n            rlst = [r for r in self.data.resources.values() if cls == r.iri]\n            if not rlst:\n                raise NoSuchTypeError(cls)\n            if len(rlst) &gt; 1:\n                raise RuntimeError(\n                    f\"{cls} matches more than one resource: \"\n                    f\"{', '.join(r.iri for r in rlst)}\"\n                )\n            r = rlst[0]\n\n        if \"subClassOf\" in r:\n            if isinstance(r.subClassOf, str):\n                return [r.iri, r.subClassOf]\n            return [r.iri] + r.subClassOf\n        return r.iri\n\n    def keywordname(self, keyword: str) -&gt; str:\n        \"\"\"Return the short name of `keyword`.\"\"\"\n        warnings.warn(\n            \"Keywords.keywordname() is deprecated. Use Keywords.shortname() \"\n            \"instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        if keyword not in self.keywords:\n            raise InvalidKeywordError(keyword)\n        return self.keywords[keyword].name\n\n    def shortname(self, iri: str) -&gt; str:\n        \"\"\"Return the short name of `iri`.\n\n        If `strict` is False, return last component of the expanded IRI.\n\n        Example:\n\n        &gt;&gt;&gt; keywords = Keywords()\n        &gt;&gt;&gt; keywords.shortname(\"dcterms:title\")\n        'title'\n\n        \"\"\"\n        if iri in self.keywords:\n            return self.keywords[iri].name\n        if iri in self.data.resources.keys():\n            return iri\n        expanded = self.expanded(iri)\n        for k, v in self.data.resources.items():\n            if expanded == self.expanded(v.iri):\n                return k\n            for kk, vv in v.keywords.items():\n                if expanded == self.expanded(vv.iri):\n                    return kk\n        raise InvalidKeywordError(iri)\n\n    def prefixed(self, name: str, strict: bool = True) -&gt; str:\n        \"\"\"Return prefixed name or `name`.\n\n        Example:\n\n        &gt;&gt;&gt; keywords = Keywords()\n        &gt;&gt;&gt; keywords.prefixed(\"title\")\n        'dcterms:title'\n        \"\"\"\n        if name in self.keywords:\n            return prefix_iri(self.keywords[name].iri, self.get_prefixes())\n        if name in self.data.resources:\n            return prefix_iri(\n                self.data.resources[name].iri,\n                self.get_prefixes(),\n                strict=strict,\n            )\n        if is_curie(name):\n            return name\n        return prefix_iri(name, self.get_prefixes(), strict=strict)\n\n    def typename(self, type) -&gt; str:\n        \"\"\"Return the short name of `type`.\n\n        Example:\n\n        &gt;&gt;&gt; keywords = Keywords()\n        &gt;&gt;&gt; keywords.typename(\"dcat:Dataset\")\n        'Dataset'\n\n        \"\"\"\n        if type in self.data.resources:\n            return type\n        prefixed = prefix_iri(type, self.get_prefixes())\n        for name, r in self.data.resources.items():\n            if prefixed == r.iri:\n                return name\n        raise NoSuchTypeError(type)\n\n    def get_prefixes(self) -&gt; dict:\n        \"\"\"Return prefixes dict.\"\"\"\n        return self.data.get(\"prefixes\", {})\n\n    def add_prefix(self, prefix, namespace, replace=False):\n        \"\"\"Bind `prefix` to `namespace`.\n\n        If `namespace` is None, is the prefix removed.\n\n        If `replace` is true, will existing namespace will be overridden.\n        \"\"\"\n        if namespace is None:\n            del self.data.prefixes[str(prefix)]\n        elif replace:\n            self.data.prefixes[str(prefix)] = str(namespace)\n        else:\n            self.data.prefixes.setdefault(str(prefix), str(namespace))\n\n    def get_context(self) -&gt; dict:\n        \"\"\"Return JSON-LD context as a dict.\n\n        Note: The returned dict corresponds to the value of the \"@context\"\n        keyword in a JSON-LD document.\n        \"\"\"\n        ctx = {}\n        ctx[\"@version\"] = 1.1\n\n        # Add prefixes to context\n        prefixes = self.data.get(\"prefixes\", {})\n        for prefix, ns in prefixes.items():\n            ctx[prefix] = ns\n\n        resources = self.data.get(\"resources\", {})\n\n        # Translate datatypes\n        translate = {\"rdf:JSON\": \"@json\"}\n\n        # Add keywords (properties) to context\n        for resource in resources.values():\n            for k, v in resource.get(\"keywords\", {}).items():\n                iri = v[\"iri\"]\n                if \"datatype\" in v:\n                    dt = v[\"datatype\"]\n                    if isinstance(dt, str):\n                        dt = translate.get(dt, dt)\n                    else:\n                        dt = [translate.get(t, t) for t in dt]\n\n                    d = {}\n                    if v.get(\"reverse\", \"\").lower() == \"true\":\n                        d[\"@reverse\"] = iri\n                    else:\n                        d[\"@id\"] = iri\n\n                    if dt == \"rdf:langString\" or \"language\" in v:\n                        d[\"@language\"] = v.get(\"language\", \"en\")\n                    else:\n                        d[\"@type\"] = dt\n\n                    ctx[k] = d  # type: ignore\n                elif v.get(\"range\", \"rdfs:Literal\") == \"rdfs:Literal\":\n                    ctx[k] = iri\n                else:\n                    ctx[k] = {  # type: ignore\n                        \"@id\": iri,\n                        \"@type\": \"@id\",\n                    }\n\n        # Add resources (classes) to context\n        for k, v in resources.items():\n            ctx.setdefault(\n                k,\n                {  # type: ignore\n                    \"@id\": v.iri,\n                    \"@type\": OWL.Class,\n                },\n            )\n\n        return ctx\n\n    def save_context(self, outfile: \"FileLoc\", indent: int = 2) -&gt; None:\n        \"\"\"Save JSON-LD context file.\n\n        Arguments:\n            outfile: File to save the JSON-LD context to.\n            indent: Indentation level. Defaults to two.\n        \"\"\"\n        context = {\"@context\": self.get_context()}\n        with open(outfile, \"wt\", encoding=\"utf-8\") as f:\n            json.dump(context, f, indent=indent)\n            f.write(os.linesep)\n\n    def _keywords_list(\n        self,\n        keywords: \"Optional[Sequence[str]]\" = None,\n        classes: \"Optional[Union[str, Sequence[str]]]\" = None,\n        themes: \"Optional[Union[str, Sequence[str]]]\" = None,\n        namespace_filter: \"Optional[Union[str, Sequence[str]]]\" = None,\n    ) -&gt; \"Tuple[Set[str], Set[str], Set[str]]\":\n        \"\"\"Help function returning a list of keywords corresponding to\n        arguments `keywords`, `classes` and `themes`.\n\n        Arguments:\n            keywords: Sequence of keywords to include.\n            classes: Include keywords that have these classes in their domain.\n            themes: Include keywords for these themes.\n            namespace_filter: A prefix, namespace or a sequence of these.\n                If given, keep only keywords and classes from the returned\n                `keywordset` and `classet` with IRIs in one of\n                these namespaces.\n\n        Returns:\n            keywordset: Set with all included keywords.\n            classet: Set with all included classes.\n            themeset: Set with all included themes.\n\n        SeeAlso:\n            save_markdown_table()\n\n        \"\"\"\n        keywords = (\n            set(self.prefixed(k) for k in asseq(keywords))\n            if keywords\n            else set()\n        )\n        classes = (\n            set(self.prefixed(d) for d in asseq(classes)) if classes else set()\n        )\n        themes = (\n            set(self.prefixed(t) for t in asseq(themes)) if themes else set()\n        )\n        orig_classes = classes.copy()\n        orig_themes = themes.copy()\n\n        prefixtuple = ()\n        if namespace_filter:\n            nf = (\n                [namespace_filter]\n                if isinstance(namespace_filter, str)\n                else list(namespace_filter)\n            )\n            for i, value in enumerate(nf):\n                if value not in self.data.prefixes:\n                    nf[i] = self.prefixed(value).rstrip(\":\")\n            prefixtuple = tuple(f\"{v}:\" for v in nf)  # type: ignore\n\n        if not keywords and not classes and not themes:\n            keywords.update(\n                p\n                for p in (self.prefixed(k) for k in self.keywordnames())\n                if not namespace_filter or p.startswith(prefixtuple)\n            )\n\n        for value in self.data.resources.values():\n            for k, v in value.get(\"keywords\", {}).items():\n                if not self.prefixed(k).startswith(prefixtuple):\n                    continue\n                vdomain = [\n                    self.prefixed(d) for d in asseq(v.get(\"domain\", ()))\n                ]\n                vtheme = [self.prefixed(t) for t in asseq(v.get(\"theme\", ()))]\n                if orig_classes:\n                    for domain in vdomain:\n                        prefixed = self.prefixed(domain)\n                        if prefixed in orig_classes:\n                            keywords.add(k)\n                if orig_themes:\n                    for theme in vtheme:\n                        prefixed = self.prefixed(theme)\n                        if prefixed in orig_themes:\n                            keywords.add(k)\n\n        for k in keywords:\n            v = self.keywords[k]\n            vdomain = [self.prefixed(d) for d in asseq(v.get(\"domain\", ()))]\n            vtheme = [self.prefixed(t) for t in asseq(v.get(\"theme\", ()))]\n            if vdomain and not classes.intersection(vdomain):\n                classes.add(vdomain[0])\n            if vtheme and not themes.intersection(vtheme):\n                themes.add(vtheme[0])\n\n        return keywords, classes, themes\n\n    def _keywords_table(\n        self,\n        keywords: \"Sequence[str]\",\n    ) -&gt; \"List[str]\":\n        \"\"\"Help function for save_markdown_table().\n\n        Returns a list with Markdown table documenting the provided\n        sequence of keywords.\n        \"\"\"\n        # pylint: disable=too-many-locals,too-many-branches\n        header = [\n            \"Keyword\",\n            \"Range\",\n            \"Conformance\",\n            \"Definition\",\n            \"Usage note\",\n        ]\n        order = {\"mandatory\": 1, \"recommended\": 2, \"optional\": 3}\n        refs = []\n        table = []\n        for keyword in keywords:\n            d = self.keywords[keyword]\n            rangestr = f\"[{d.range}]\" if \"range\" in d else \"\"\n            if \"datatype\" in d:\n                rangestr += (\n                    \", \" + \", \".join(d.datatype)\n                    if isinstance(d.datatype, list)\n                    else f\"&lt;br&gt;({d.datatype})\"\n                )\n            table.append(\n                [\n                    f\"[{d.name}]\",\n                    rangestr,\n                    f\"{d.conformance}\" if \"conformance\" in d else \"\",\n                    f\"{d.description}\" if \"description\" in d else \"\",\n                    f\"{d.usageNote}\" if \"usageNote\" in d else \"\",\n                ]\n            )\n            refs.append(f\"[{d.name}]: {self.expanded(d.iri)}\")\n            if \"range\" in d:\n                refs.append(f\"[{d.range}]: {self.expanded(d.range)}\")\n        table.sort(key=lambda row: order.get(row[2], 10))\n\n        out = self._to_table(header, table)\n        out.append(\"\")\n        out.extend(refs)\n        out.append(\"\")\n        out.append(\"\")\n        return out\n\n    def save_markdown_table(\n        self, outfile: \"FileLoc\", keywords: \"Sequence[str]\"\n    ) -&gt; None:\n        \"\"\"Save markdown file with documentation of the keywords.\"\"\"\n        table = self._keywords_table(keywords)\n        with open(outfile, \"wt\", encoding=\"utf-8\") as f:\n            f.write(os.linesep.join(table) + os.linesep)\n\n    def save_markdown(\n        self,\n        outfile: \"FileLoc\",\n        keywords: \"Optional[Sequence[str]]\" = None,\n        classes: \"Optional[Union[str, Sequence[str]]]\" = None,\n        themes: \"Optional[Union[str, Sequence[str]]]\" = None,\n        namespace_filter: \"Optional[Union[str, Sequence[str]]]\" = None,\n        explanation: bool = False,\n        special: bool = False,\n    ) -&gt; None:\n        \"\"\"Save markdown file with documentation of the keywords.\n\n        Arguments:\n            outfile: File to save the markdown documentation to.\n            keywords: Sequence of keywords to include.\n            classes: Include keywords that have these classes in their domain.\n            themes: Include keywords for these themes.\n            namespace_filter: A prefix, namespace or a sequence of these.\n                Keep only keywords within this namespace.\n                It is important that the namespace(s) are defined with\n                prefixes in the Keywords object.\n            explanation: Whether to include explanation of columns labels.\n            special: Whether to generate documentation of special\n                JSON-LD keywords.\n\n        \"\"\"\n        # pylint: disable=too-many-locals,too-many-branches\n        keywords, classes, themes = self._keywords_list(\n            keywords, classes, themes, namespace_filter=namespace_filter\n        )\n        ts = Triplestore(\"rdflib\")\n        for prefix, ns in self.data.get(\"prefixes\", {}).items():\n            ts.bind(prefix, ns)\n\n        if namespace_filter:\n            header = \"Keywords for namespaces:\" + \", \".join(namespace_filter)\n        elif themes:\n            header = \"Keywords for theme: \" + \", \".join(themes)\n        else:\n            header = \"Keywords\"\n        out = [\n            \"&lt;!-- Do not edit! This file is generated with Tripper. --&gt;\",\n            \"\",\n            header,\n            \"\",\n        ]\n        column_explanations = [\n            \"The meaning of the columns are as follows:\",\n            \"\",\n            \"- **Keyword**: The keyword referring to a property used for \"\n            \"the data documentation.\",\n            \"- **Range**: Refer to the class for the values of the keyword.\",\n            \"- **Conformance**: Whether the keyword is mandatory, recommended \"\n            \"or optional when documenting the given type of resources.\",\n            \"- **Definition**: The definition of the keyword.\",\n            \"- **Usage note**: Notes about how to use the keyword.\",\n            \"\",\n        ]\n        special_keywords = [\n            \"## Special keywords (from JSON-LD)\",\n            \"See the [JSON-LD specification] for more details.\",\n            \"\",\n            # pylint: disable=line-too-long\n            \"| Keyword    | Range         | Conformance | Definition                                                              | Usage note |\",\n            \"|------------|---------------|-------------|-------------------------------------------------------------------------|------------|\",\n            \"| [@id]      | IRI           | mandatory   | IRI identifying the resource to document.                               |            |\",\n            \"| [@type]    | IRI           | recommended | Ontological class defining the class of a node.                         |            |\",\n            \"| [@context] | dict&amp;#124list | optional    | Context defining namespace prefixes and additional keywords.            |            |\",\n            \"| [@base]    | namespace     | optional    | Base IRI against which relative IRIs are resolved.                      |            |\",\n            \"| [@vocab]   | namespace     | optional    | Used to expand properties and values in @type with a common prefix IRI. |            |\",\n            \"| [@graph]   | list          | optional    | Used for documenting multiple resources.                                |            |\",\n            \"\",\n        ]\n        if explanation:\n            out.extend(column_explanations)\n        if special:\n            out.extend(special_keywords)\n        refs = []\n\n        for cls in sorted(classes):\n            name = self.prefixed(cls)\n            shortname = iriname(name)\n            if shortname in self.data.resources:\n                resource = self.data.resources[shortname]\n            else:\n                for rname, resource in self.data.resources.items():\n                    if self.prefixed(resource.iri) == name:\n                        shortname = rname\n                        break\n                else:\n                    raise MissingKeyError(cls)\n\n            out.append(\"\")\n            out.append(f\"## Properties on [{shortname}]\")\n            if \"description\" in resource:\n                out.append(resource.description)\n            if \"subClassOf\" in resource:\n                out.append(\"\")\n                subcl = (\n                    [resource.subClassOf]\n                    if isinstance(resource.subClassOf, str)\n                    else resource.subClassOf\n                )\n                out.append(\n                    f\"- subClassOf: {', '.join(f'[{sc}]' for sc in subcl)}\"\n                )\n                for sc in subcl:\n                    refs.append(f\"[{sc}]: {ts.expand_iri(sc)}\")\n            if \"iri\" in resource:\n                refs.append(f\"[{shortname}]: {ts.expand_iri(resource.iri)}\")\n            included_keywords = [\n                k\n                for k, v in self.keywords.items()\n                if name in v.domain and is_curie(k)\n            ]\n            out.extend(\n                self._keywords_table(keywords=sorted(included_keywords))\n            )\n            out.append(\"\")\n\n        # References\n        extra_refs = [\n            # pylint: disable=line-too-long\n            \"[@id]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n            \"[@type]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n            \"[@context]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n            \"[@base]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n            \"[@vocab]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n            \"[@graph]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n        ]\n        refs.extend(extra_refs)\n        out.append(\"\")\n        out.append(\"\")\n        out.append(\"\")\n        out.extend(refs)\n        with open(outfile, \"wt\", encoding=\"utf-8\") as f:\n            f.write(\"\\n\".join(out) + \"\\n\")\n\n    def save_markdown_prefixes(self, outfile: \"FileLoc\") -&gt; None:\n        \"\"\"Save markdown file with documentation of the prefixes.\"\"\"\n        out = [\n            \"# Predefined prefixes\",\n            (\n                \"All namespace prefixes listed on this page are defined in \"\n                \"the [default JSON-LD context].\"\n            ),\n            (\n                \"See [User-defined prefixes] for how to extend this list \"\n                \"with additional namespace prefixes.\"\n            ),\n        ]\n        rows = [\n            [prefix, ns]\n            for prefix, ns in self.data.get(\"prefixes\", {}).items()\n        ]\n        out.extend(self._to_table([\"Prefix\", \"Namespace\"], rows))\n        out.append(\"\")\n        out.append(\"\")\n        out.extend(\n            [\n                # pylint: disable=line-too-long\n                \"[default JSON-LD context]: https://raw.githubusercontent.com/EMMC-ASBL/tripper/refs/heads/master/tripper/context/0.3/context.json\",\n                \"[User-defined prefixes]: customisation.md/#user-defined-prefixes\",\n            ]\n        )\n        with open(outfile, \"wt\", encoding=\"utf-8\") as f:\n            f.write(\"\\n\".join(out) + \"\\n\")\n\n    def _to_table(self, header: \"Sequence\", rows: \"Iterable\") -&gt; list:\n        \"\"\"Return header and rows as a .\"\"\"\n\n        widths = [len(h) for h in header]\n        for row in rows:\n            for i, col in enumerate(row):\n                n = len(col)\n                if n &gt; widths[i]:\n                    widths[i] = n\n\n        lines = []\n        empty = \"\"\n        if rows:\n            lines.append(\"\")\n            lines.append(\n                \"| \"\n                + \" | \".join(\n                    f\"{head:{widths[i]}}\" for i, head in enumerate(header)\n                )\n                + \" |\"\n            )\n            lines.append(\n                \"| \"\n                + \" | \".join(\n                    f\"{empty:-&lt;{widths[i]}}\" for i in range(len(header))\n                )\n                + \" |\"\n            )\n            for row in rows:\n                lines.append(\n                    \"| \"\n                    + \" | \".join(\n                        f\"{col:{widths[i]}}\" for i, col in enumerate(row)\n                    )\n                    + \" |\"\n                )\n\n        return lines\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.__init__","title":"<code>__init__(self, theme='ddoc:datadoc', yamlfile=None, timeout=3)</code>  <code>special</code>","text":"<p>Initialises keywords object.</p> <p>Parameters:</p> Name Type Description Default <code>theme</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>IRI of one of more themes to load keywords for.</p> <code>'ddoc:datadoc'</code> <code>yamlfile</code> <code>Optional[FileLoc]</code> <p>A YAML file with keyword definitions to parse.  May also be an URI in which case it will be accessed via HTTP GET. Deprecated. Use the <code>load_yaml()</code> or <code>add()</code> methods instead.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Timeout in case <code>yamlfile</code> is a URI.</p> <code>3</code> <p>Attributes:</p> Name Type Description <code>data</code> <p>The dict loaded from the keyword yamlfile.</p> <code>keywords</code> <p>A dict mapping keywords (name/prefixed/iri) to dicts describing the keywords.</p> <code>theme</code> <p>IRI of a theme or scientic domain that the keywords belong to.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def __init__(\n    self,\n    theme: \"Optional[Union[str, Sequence[str]]]\" = \"ddoc:datadoc\",\n    yamlfile: \"Optional[FileLoc]\" = None,\n    timeout: float = 3,\n) -&gt; None:\n    \"\"\"Initialises keywords object.\n\n    Arguments:\n        theme: IRI of one of more themes to load keywords for.\n        yamlfile: A YAML file with keyword definitions to parse.  May also\n            be an URI in which case it will be accessed via HTTP GET.\n            Deprecated. Use the `load_yaml()` or `add()` methods instead.\n        timeout: Timeout in case `yamlfile` is a URI.\n\n    Attributes:\n        data: The dict loaded from the keyword yamlfile.\n        keywords: A dict mapping keywords (name/prefixed/iri) to dicts\n            describing the keywords.\n        theme: IRI of a theme or scientic domain that the keywords\n            belong to.\n    \"\"\"\n    default_prefixes = AttrDict(ddoc=str(DDOC))\n    self.theme = None  # theme for this object\n    self.data = AttrDict(prefixes=default_prefixes, resources=AttrDict())\n\n    # A \"view\" into `self.data`. A dict mapping short, prefixed\n    # and expanded keyword names to corresponding value dicts in\n    # self.data.\n    self.keywords = AttrDict()\n\n    # Themes and files that has been parsed\n    self.parsed: \"set\" = set()\n\n    # Used for parsing dicts. Maps any of the elements in the\n    # value to the key (with highest precedence first).\n    self.input_mappings = {\n        \"label\": [\"skos:prefLabel\", \"rdfs:label\"],\n        \"description\": [\n            # TODO: Uncomment when EMMO has changed annotations to\n            # human readable IRIs\n            # \"emmo:elucidation\",\n            \"skos:definition\",\n            \"dcterms:description\",\n            \"rdfs:comment\",\n        ],\n        \"usageNote\": [\"vann:usageNote\", \"skos:scopeNote\"],\n        \"unit\": [\"ddoc:unitSymbol\"],\n        \"subPropertyOf\": [\"rdfs:subPropertyOf\"],\n        \"inverseOf\": [\"owl:inverseOf\"],\n    }\n\n    if theme:\n        self.add_theme(theme)\n\n    if yamlfile:\n        warnings.warn(\n            \"The `yamlfile` argument is deprecated. Use the `load_yaml()` \"\n            \"or `add()` methods instead.\",\n            DeprecationWarning,\n        )\n        if isinstance(yamlfile, (str, Path)):\n            self.load_yaml(yamlfile, timeout=timeout)\n        else:\n            for path in yamlfile:\n                self.load_yaml(path, timeout=timeout)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.add","title":"<code>add(self, keywords, format=None, timeout=3, strict=False, redefine='raise')</code>","text":"<p>Add <code>keywords</code> to this Keywords object.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>Optional[KeywordsType]</code> <p>Keywords definitions to add to this Keyword object. May be another Keyword object, path to a file, theme or a sequence of these.</p> required <code>format</code> <code>Optional[Union[str, Sequence]]</code> <p>Format if <code>keywords</code>. Recognised formats include: yaml, csv, tsv, turtle, xml, json-ld, rdfa, ...</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Timeout when accessing remote files.</p> <code>3</code> <code>strict</code> <code>bool</code> <p>Whether to raise an <code>InvalidKeywordError</code> exception if <code>d</code> contains an unknown key.</p> <code>False</code> <code>redefine</code> <code>str</code> <p>Determine how to handle redefinition of existing keywords.  Should be one of the following strings:   - \"allow\": Allow redefining a keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"skip\": Don't redefine existing keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"raise\": Raise an RedefineError (default).</p> <code>'raise'</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def add(\n    self,\n    keywords: \"Optional[KeywordsType]\",\n    format: \"Optional[Union[str, Sequence]]\" = None,\n    timeout: float = 3,\n    strict: bool = False,\n    redefine: str = \"raise\",\n) -&gt; None:\n    \"\"\"Add `keywords` to this Keywords object.\n\n    Arguments:\n        keywords: Keywords definitions to add to this Keyword object.\n            May be another Keyword object, path to a file, theme or a\n            sequence of these.\n        format: Format if `keywords`. Recognised formats include:\n            yaml, csv, tsv, turtle, xml, json-ld, rdfa, ...\n        timeout: Timeout when accessing remote files.\n        strict: Whether to raise an `InvalidKeywordError` exception if `d`\n            contains an unknown key.\n        redefine: Determine how to handle redefinition of existing\n            keywords.  Should be one of the following strings:\n              - \"allow\": Allow redefining a keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"skip\": Don't redefine existing keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"raise\": Raise an RedefineError (default).\n\n    \"\"\"\n    if not isinstance(keywords, str) and isinstance(keywords, Sequence):\n        if isinstance(format, str):\n            format = [format] * len(keywords)\n        elif format and len(format) != len(keywords):\n            raise TypeError(\n                \"If given, `format` must have the same length as \"\n                \"`keywords`\"\n            )\n\n    def _add(kw, fmt):\n        if kw is None:\n            pass\n        elif isinstance(kw, Keywords):\n            self.theme = merge(self.theme, kw.theme)\n            recursive_update(self.data, kw.data, cls=AttrDict)\n            self._set_keywords(clear=False)\n        elif isinstance(kw, dict):\n            self._load_yaml(kw, strict=strict, redefine=redefine)\n        elif not isinstance(kw, str) and isinstance(kw, Sequence):\n            for i, e in enumerate(kw):\n                _add(e, fmt[i] if fmt else None)\n        elif isinstance(kw, (str, Path, IOBase)):\n            if (\n                isinstance(kw, str)\n                and \":\" in kw\n                and not (\n                    kw.startswith(\"/\") or kw.startswith(\"./\") or is_uri(kw)\n                )\n            ):\n                self.add_theme(\n                    kw,\n                    timeout=timeout,\n                    strict=strict,\n                    redefine=redefine,\n                )\n            else:\n                if not fmt:\n                    name = kw.name if hasattr(kw, \"name\") else kw\n                    fmt = Path(name).suffix\n                fmt = fmt.lstrip(\".\").lower()\n                # pylint:disable=consider-using-get\n                if fmt in RDFLIB_SUFFIX_FORMAT_MAP:\n                    fmt = RDFLIB_SUFFIX_FORMAT_MAP[fmt]\n\n                if fmt in (\"yaml\", \"yml\"):\n                    self.load_yaml(\n                        kw,\n                        timeout=timeout,\n                        strict=strict,\n                        redefine=redefine,\n                    )\n                elif fmt in (\"csv\", \"tsv\", \"xlsx\", \"excel\"):\n                    self.load_table(kw, format=fmt)\n                else:\n                    self.load_rdffile(\n                        kw,\n                        format=fmt,\n                        timeout=timeout,\n                        strict=strict,\n                        redefine=redefine,\n                    )\n        else:\n            raise TypeError(\n                \"`keywords` must be a KeywordsType object (Keywords \"\n                \"instance, dict, IO, Path, string or sequence). \"\n                f\"Got: {type(kw)}\"\n            )\n\n    _add(keywords, format)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.add_prefix","title":"<code>add_prefix(self, prefix, namespace, replace=False)</code>","text":"<p>Bind <code>prefix</code> to <code>namespace</code>.</p> <p>If <code>namespace</code> is None, is the prefix removed.</p> <p>If <code>replace</code> is true, will existing namespace will be overridden.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def add_prefix(self, prefix, namespace, replace=False):\n    \"\"\"Bind `prefix` to `namespace`.\n\n    If `namespace` is None, is the prefix removed.\n\n    If `replace` is true, will existing namespace will be overridden.\n    \"\"\"\n    if namespace is None:\n        del self.data.prefixes[str(prefix)]\n    elif replace:\n        self.data.prefixes[str(prefix)] = str(namespace)\n    else:\n        self.data.prefixes.setdefault(str(prefix), str(namespace))\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.add_theme","title":"<code>add_theme(self, theme, timeout=3, strict=False, redefine='raise')</code>","text":"<p>Add keywords for <code>theme</code>, where <code>theme</code> is the IRI of a theme or scientific domain or a list of such IRIs.</p> <p>Parameters:</p> Name Type Description Default <code>theme</code> <code>Union[str, Sequence[str]]</code> <p>IRI (or list of IRIs) of a theme/scientific domain to load.</p> required <code>timeout</code> <code>float</code> <p>Timeout when accessing remote files.</p> <code>3</code> <code>strict</code> <code>bool</code> <p>Whether to raise an <code>InvalidKeywordError</code> exception if the theme contains an unknown key.</p> <code>False</code> <code>redefine</code> <code>str</code> <p>Determine how to handle redefinition of existing keywords.  Should be one of the following strings:   - \"allow\": Allow redefining a keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"skip\": Don't redefine existing keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"raise\": Raise an RedefineError (default).</p> <code>'raise'</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def add_theme(\n    self,\n    theme: \"Union[str, Sequence[str]]\",\n    timeout: float = 3,\n    strict: bool = False,\n    redefine: str = \"raise\",\n) -&gt; None:\n    \"\"\"Add keywords for `theme`, where `theme` is the IRI of a\n    theme or scientific domain or a list of such IRIs.\n\n    Arguments:\n        theme: IRI (or list of IRIs) of a theme/scientific domain to load.\n        timeout: Timeout when accessing remote files.\n        strict: Whether to raise an `InvalidKeywordError` exception if the\n            theme contains an unknown key.\n        redefine: Determine how to handle redefinition of existing\n            keywords.  Should be one of the following strings:\n              - \"allow\": Allow redefining a keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"skip\": Don't redefine existing keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"raise\": Raise an RedefineError (default).\n\n    \"\"\"\n    if isinstance(theme, str):\n        theme = [theme]\n\n    parsedkey = (tuple(theme), strict, redefine)\n    if parsedkey in self.parsed:\n        return\n\n    for name in theme:  # type: ignore\n        expanded = expand_iri(name, self.get_prefixes())\n        prefixed = prefix_iri(name, self.get_prefixes())\n        add(\n            self.data,\n            \"theme\",\n            prefixed,\n        )\n        for ep in get_entry_points(\"tripper.keywords\"):\n            if expand_iri(ep.value, self.get_prefixes()) == expanded:\n                package_name, path = ep.name.split(\"/\", 1)\n                package = import_module(package_name)\n                fullpath = (\n                    Path(package.__file__).parent / path  # type: ignore\n                )\n                self.add(\n                    fullpath,\n                    timeout=timeout,\n                    strict=strict,\n                    redefine=redefine,\n                )\n                break\n        else:\n            # Fallback in case the entry point is not installed\n            if expanded == DDOC.datadoc:\n                self.load_yaml(\n                    self.rootdir\n                    / \"tripper\"\n                    / \"context\"\n                    / \"0.3\"\n                    / \"keywords.yaml\",\n                    timeout=timeout,\n                    strict=strict,\n                    redefine=redefine,\n                )\n            else:\n                raise TypeError(f\"Unknown theme: {name}\")\n\n    self.parsed.add(parsedkey)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.asdicts","title":"<code>asdicts(self, names=None, keymode='prefixed')</code>","text":"<p>Return the content of this Keywords object as a list of JSON-LD dicts.</p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>Optional[Sequence]</code> <p>A sequence of keyword or class names.  The default is to return all keywords.</p> <code>None</code> <code>keymode</code> <code>str</code> <p>How to represent keys.  Should be either \"name\", \"prefixed\" (CURIE) or \"expanded\" (full IRI).</p> <code>'prefixed'</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List of JSON-LD dicts corresponding to <code>names</code>.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def asdicts(\n    self,\n    names: \"Optional[Sequence]\" = None,\n    keymode: str = \"prefixed\",\n) -&gt; \"List[dict]\":\n    \"\"\"Return the content of this Keywords object as a list of JSON-LD\n    dicts.\n\n    Arguments:\n        names: A sequence of keyword or class names.  The\n            default is to return all keywords.\n        keymode: How to represent keys.  Should be either \"name\",\n            \"prefixed\" (CURIE) or \"expanded\" (full IRI).\n\n    Returns:\n        List of JSON-LD dicts corresponding to `names`.\n    \"\"\"\n    keymodes = {\n        \"name\": iriname,\n        \"prefixed\": None,\n        \"expanded\": self.expanded,\n    }\n    # TODO: use `self.input_mappings` instead\n    maps = {\n        \"subPropertyOf\": \"rdfs:subPropertyOf\",\n        \"unit\": \"ddoc:unitSymbol\",\n        \"description\": \"dcterms:description\",\n        \"usageNote\": \"vann:usageNote\",\n        \"theme\": \"dcat:theme\",\n    }\n\n    def key(k):\n        \"\"\"Return key `k` accordig to `keymode`.\"\"\"\n        return keymodes[keymode](k) if keymodes[keymode] else k\n\n    conformance_indv = {v: k for k, v in CONFORMANCE_MAPS.items()}\n    if names is None:\n        names = self.keywordnames()\n\n    classes = []\n    dicts = []\n    for name in names:\n        if name not in self.keywords:\n            classes.append(name)\n            continue\n        d = self.keywords[name]\n        if \"range\" in d and self.expanded(d.range) != RDFS.Literal:\n            proptype = \"owl:ObjectProperty\"\n            range = d.range\n        elif (\n            \"datatype\" in d and self.expanded(d.datatype) != RDF.langString\n        ):\n            proptype = \"owl:DatatypeProperty\"\n            range = d.get(\"datatype\")\n        else:\n            proptype = \"owl:AnnotationProperty\"\n            range = d.get(\"datatype\")\n\n        dct = {\n            \"@id\": d.iri,\n            \"@type\": proptype,\n            key(\"rdfs:label\"): d.name,\n        }\n        if \"domain\" in d:\n            dct[key(\"rdfs:domain\")] = d.domain\n        if range:\n            dct[key(\"rdfs:range\")] = range\n        if \"conformance\" in d:\n            dct[key(\"ddoc:conformance\")] = conformance_indv.get(\n                d.conformance, d.conformance\n            )\n        for k, v in d.items():\n            if k in maps:\n                dct[key(maps[k])] = v\n        dicts.append(dct)\n\n    if classes:\n        classmaps = {}\n        for k, v in self.data.resources.items():\n            classmaps[k] = k\n            classmaps[self.expanded(k)] = k\n            classmaps[self.prefixed(k)] = k\n\n        for name in classes:\n            d = self.data.resources[classmaps[name]]\n            dct = {\"@id\": d.iri, \"@type\": OWL.Class}\n            if \"subClassOf\" in d:\n                dct[key(\"rdfs:subClassOf\")] = d.subClassOf\n            if \"description\" in d:\n                dct[key(\"dcterms:description\")] = d.description\n            if \"usageNote\" in d:\n                dct[key(\"vann:usageNote\")] = d.usageNote\n            dicts.append(dct)\n\n    return dicts\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.classnames","title":"<code>classnames(self)</code>","text":"<p>Return a list with all class names defined in this instance.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def classnames(self) -&gt; \"list\":\n    \"\"\"Return a list with all class names defined in this instance.\"\"\"\n    return list(self.data.resources.keys())\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.copy","title":"<code>copy(self)</code>","text":"<p>Returns a copy of self.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def copy(self):\n    \"\"\"Returns a copy of self.\"\"\"\n    new = Keywords(theme=None)\n    new.theme = self.theme\n    new.data = deepcopy(self.data)\n    new.keywords = deepcopy(self.keywords)\n    new.parsed = self.parsed.copy()\n    return new\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.expanded","title":"<code>expanded(self, keyword, strict=True)</code>","text":"<p>Return the keyword expanded to its full IRI.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def expanded(self, keyword: str, strict: bool = True) -&gt; str:\n    \"\"\"Return the keyword expanded to its full IRI.\"\"\"\n    if keyword in self.keywords:\n        iri = self.keywords[keyword].iri\n    elif \"resources\" in self.data and keyword in self.data.resources:\n        iri = self.data.resources[keyword].iri\n    elif \":\" in keyword or not strict:\n        iri = keyword\n    else:\n        raise InvalidKeywordError(keyword)\n    return expand_iri(iri, self.get_prefixes(), strict=strict)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.fromdicts","title":"<code>fromdicts(self, dicts, prefixes=None, theme=None, basedOn=None, strict=False, redefine='raise')</code>","text":"<p>Populate this Keywords object from a sequence of dicts.</p> <p>Parameters:</p> Name Type Description Default <code>dicts</code> <code>Sequence[dict]</code> <p>A sequence of JSON-LD dicts to populate this keywords object from.  Their format should follow what is returned by tripper.datadoc.acquire().</p> required <code>prefixes</code> <code>Optional[dict]</code> <p>Dict with additional prefixes used by <code>dicts</code>.</p> <code>None</code> <code>theme</code> <code>Optional[str]</code> <p>Theme defined by <code>dicts</code>.</p> <code>None</code> <code>basedOn</code> <code>Optional[Union[str, List[str]]]</code> <p>Theme(s) that <code>dicts</code> are based on.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to raise an <code>InvalidKeywordError</code> exception if <code>d</code> contains an unknown key.</p> <code>False</code> <code>redefine</code> <code>str</code> <p>Determine how to handle redefinition of existing keywords.  Should be one of the following strings:   - \"allow\": Allow redefining a keyword.   - \"skip\": Don't redefine existing keyword.   - \"raise\": Raise an RedefineError (default).</p> <code>'raise'</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def fromdicts(\n    self,\n    dicts: \"Sequence[dict]\",\n    prefixes: \"Optional[dict]\" = None,\n    theme: \"Optional[str]\" = None,\n    basedOn: \"Optional[Union[str, List[str]]]\" = None,\n    strict: bool = False,\n    redefine: str = \"raise\",\n) -&gt; None:\n    \"\"\"Populate this Keywords object from a sequence of dicts.\n\n    Arguments:\n        dicts: A sequence of JSON-LD dicts to populate this keywords object\n            from.  Their format should follow what is returned by\n            tripper.datadoc.acquire().\n        prefixes: Dict with additional prefixes used by `dicts`.\n        theme: Theme defined by `dicts`.\n        basedOn: Theme(s) that `dicts` are based on.\n        strict: Whether to raise an `InvalidKeywordError` exception if `d`\n            contains an unknown key.\n        redefine: Determine how to handle redefinition of existing\n            keywords.  Should be one of the following strings:\n              - \"allow\": Allow redefining a keyword.\n              - \"skip\": Don't redefine existing keyword.\n              - \"raise\": Raise an RedefineError (default).\n\n    \"\"\"\n    data = self._fromdicts(\n        dicts,\n        prefixes=prefixes,\n        theme=theme,\n        basedOn=basedOn,\n    )\n    self._load_yaml(data, strict=strict, redefine=redefine)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.get_context","title":"<code>get_context(self)</code>","text":"<p>Return JSON-LD context as a dict.</p> <p>Note: The returned dict corresponds to the value of the \"@context\" keyword in a JSON-LD document.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def get_context(self) -&gt; dict:\n    \"\"\"Return JSON-LD context as a dict.\n\n    Note: The returned dict corresponds to the value of the \"@context\"\n    keyword in a JSON-LD document.\n    \"\"\"\n    ctx = {}\n    ctx[\"@version\"] = 1.1\n\n    # Add prefixes to context\n    prefixes = self.data.get(\"prefixes\", {})\n    for prefix, ns in prefixes.items():\n        ctx[prefix] = ns\n\n    resources = self.data.get(\"resources\", {})\n\n    # Translate datatypes\n    translate = {\"rdf:JSON\": \"@json\"}\n\n    # Add keywords (properties) to context\n    for resource in resources.values():\n        for k, v in resource.get(\"keywords\", {}).items():\n            iri = v[\"iri\"]\n            if \"datatype\" in v:\n                dt = v[\"datatype\"]\n                if isinstance(dt, str):\n                    dt = translate.get(dt, dt)\n                else:\n                    dt = [translate.get(t, t) for t in dt]\n\n                d = {}\n                if v.get(\"reverse\", \"\").lower() == \"true\":\n                    d[\"@reverse\"] = iri\n                else:\n                    d[\"@id\"] = iri\n\n                if dt == \"rdf:langString\" or \"language\" in v:\n                    d[\"@language\"] = v.get(\"language\", \"en\")\n                else:\n                    d[\"@type\"] = dt\n\n                ctx[k] = d  # type: ignore\n            elif v.get(\"range\", \"rdfs:Literal\") == \"rdfs:Literal\":\n                ctx[k] = iri\n            else:\n                ctx[k] = {  # type: ignore\n                    \"@id\": iri,\n                    \"@type\": \"@id\",\n                }\n\n    # Add resources (classes) to context\n    for k, v in resources.items():\n        ctx.setdefault(\n            k,\n            {  # type: ignore\n                \"@id\": v.iri,\n                \"@type\": OWL.Class,\n            },\n        )\n\n    return ctx\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.get_prefixes","title":"<code>get_prefixes(self)</code>","text":"<p>Return prefixes dict.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def get_prefixes(self) -&gt; dict:\n    \"\"\"Return prefixes dict.\"\"\"\n    return self.data.get(\"prefixes\", {})\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.isnested","title":"<code>isnested(self, keyword)</code>","text":"<p>Returns whether the keyword corresponds to an object property.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def isnested(self, keyword: str) -&gt; bool:\n    \"\"\"Returns whether the keyword corresponds to an object property.\"\"\"\n    d = self.keywords[keyword]\n    if \"datatype\" in d or d.range == \"rdfs:Literal\":\n        return False\n    return True\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.keywordname","title":"<code>keywordname(self, keyword)</code>","text":"<p>Return the short name of <code>keyword</code>.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def keywordname(self, keyword: str) -&gt; str:\n    \"\"\"Return the short name of `keyword`.\"\"\"\n    warnings.warn(\n        \"Keywords.keywordname() is deprecated. Use Keywords.shortname() \"\n        \"instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    if keyword not in self.keywords:\n        raise InvalidKeywordError(keyword)\n    return self.keywords[keyword].name\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.keywordnames","title":"<code>keywordnames(self)</code>","text":"<p>Return a list with all keyword names defined in this instance.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def keywordnames(self) -&gt; \"list\":\n    \"\"\"Return a list with all keyword names defined in this instance.\"\"\"\n    return [k for k in self.keywords.keys() if \":\" not in k]\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.load_rdf","title":"<code>load_rdf(self, ts, iris=None, strict=False, redefine='raise')</code>","text":"<p>Populate this Keyword object from a triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to load keywords from.</p> required <code>iris</code> <code>Optional[Sequence[str]]</code> <p>IRIs to load. The default is to load IRIs corresponding to all properties an classes.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to raise an <code>InvalidKeywordError</code> exception if <code>d</code> contains an unknown key.</p> <code>False</code> <code>redefine</code> <code>str</code> <p>Determine how to handle redefinition of existing keywords.  Should be one of the following strings:   - \"allow\": Allow redefining a keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"skip\": Don't redefine existing keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"raise\": Raise an RedefineError (default).</p> <code>'raise'</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def load_rdf(\n    self,\n    ts: \"Triplestore\",\n    iris: \"Optional[Sequence[str]]\" = None,\n    strict: bool = False,\n    redefine: str = \"raise\",\n) -&gt; None:\n    \"\"\"Populate this Keyword object from a triplestore.\n\n    Arguments:\n        ts: Triplestore to load keywords from.\n        iris: IRIs to load. The default is to load IRIs corresponding to all\n            properties an classes.\n        strict: Whether to raise an `InvalidKeywordError` exception if `d`\n            contains an unknown key.\n        redefine: Determine how to handle redefinition of existing\n            keywords.  Should be one of the following strings:\n              - \"allow\": Allow redefining a keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"skip\": Don't redefine existing keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"raise\": Raise an RedefineError (default).\n\n    \"\"\"\n    dicts = self._load_rdf(ts, iris)\n    self.fromdicts(\n        dicts,\n        prefixes=ts.namespaces,\n        strict=strict,\n        redefine=redefine,\n    )\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.load_rdffile","title":"<code>load_rdffile(self, rdffile, format=None, timeout=3, iris=None, strict=False, redefine='raise')</code>","text":"<p>Load RDF from file or URL.</p> <p>Parameters:</p> Name Type Description Default <code>rdffile</code> <code>FileLoc</code> <p>File to load.</p> required <code>format</code> <code>Optional[str]</code> <p>Any format supported by rdflib.Graph.parse().</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Timeout in case <code>yamlfile</code> is a URI.</p> <code>3</code> <code>iris</code> <code>Optional[Sequence[str]]</code> <p>IRIs to load. The default is to load IRIs corresponding to all properties an classes.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to raise an <code>InvalidKeywordError</code> exception if <code>d</code> contains an unknown key.</p> <code>False</code> <code>redefine</code> <code>str</code> <p>Determine how to handle redefinition of existing keywords.  Should be one of the following strings:   - \"allow\": Allow redefining a keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"skip\": Don't redefine existing keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"raise\": Raise an RedefineError (default).</p> <code>'raise'</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def load_rdffile(\n    self,\n    rdffile: \"FileLoc\",\n    format: \"Optional[str]\" = None,\n    timeout: float = 3,\n    iris: \"Optional[Sequence[str]]\" = None,\n    strict: bool = False,\n    redefine: str = \"raise\",\n) -&gt; None:\n    \"\"\"Load RDF from file or URL.\n\n    Arguments:\n        rdffile: File to load.\n        format: Any format supported by rdflib.Graph.parse().\n        timeout: Timeout in case `yamlfile` is a URI.\n        iris: IRIs to load. The default is to load IRIs corresponding to\n            all properties an classes.\n        strict: Whether to raise an `InvalidKeywordError` exception if `d`\n            contains an unknown key.\n        redefine: Determine how to handle redefinition of existing\n            keywords.  Should be one of the following strings:\n              - \"allow\": Allow redefining a keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"skip\": Don't redefine existing keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"raise\": Raise an RedefineError (default).\n\n    \"\"\"\n    if format is None:\n        format = guess_rdf_format(rdffile)\n\n    ts = Triplestore(\"rdflib\")\n    with openfile(rdffile, timeout=timeout, mode=\"rt\") as f:\n        ts.parse(f, format=format)\n    self.load_rdf(ts, iris=iris, strict=strict, redefine=redefine)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.load_table","title":"<code>load_table(self, filename, format=None, prefixes=None, theme=None, basedOn=None, **kwargs)</code>","text":"<p>Load keywords from a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>FileLoc</code> <p>File to load.</p> required <code>format</code> <code>Optional[str]</code> <p>File format. Unused.  Only csv is currently supported.</p> <code>None</code> <code>prefixes</code> <code>Optional[dict]</code> <p>Dict with additional prefixes used in the table.</p> <code>None</code> <code>theme</code> <code>Optional[str]</code> <p>Theme defined by the table.</p> <code>None</code> <code>basedOn</code> <code>Optional[Union[str, List[str]]]</code> <p>Theme(s) that the table is based on.</p> <code>None</code> <code>kwargs</code> <p>Keyword arguments passed on to TableDoc.parse_csv().</p> <code>{}</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def load_table(\n    self,\n    filename: \"FileLoc\",\n    format: \"Optional[str]\" = None,  # pylint: disable=unused-argument\n    prefixes: \"Optional[dict]\" = None,\n    theme: \"Optional[str]\" = None,\n    basedOn: \"Optional[Union[str, List[str]]]\" = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Load keywords from a csv file.\n\n    Arguments:\n        filename: File to load.\n        format: File format. Unused.  Only csv is currently supported.\n        prefixes: Dict with additional prefixes used in the table.\n        theme: Theme defined by the table.\n        basedOn: Theme(s) that the table is based on.\n        kwargs: Keyword arguments passed on to TableDoc.parse_csv().\n    \"\"\"\n    # pylint: disable=import-outside-toplevel\n    from tripper.datadoc.tabledoc import TableDoc\n\n    td = TableDoc.parse_csv(\n        filename, type=None, prefixes=prefixes, **kwargs\n    )\n    dicts = td.asdicts()\n    self.fromdicts(dicts, prefixes=prefixes, theme=theme, basedOn=basedOn)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.load_yaml","title":"<code>load_yaml(self, yamlfile, timeout=3, strict=True, redefine='raise')</code>","text":"<p>Load YAML file with keyword definitions.</p> <p>Parameters:</p> Name Type Description Default <code>yamlfile</code> <code>Union[Path, str]</code> <p>Path of URL to a YAML file to load.</p> required <code>timeout</code> <code>float</code> <p>Timeout when accessing remote files.</p> <code>3</code> <code>strict</code> <code>bool</code> <p>Whether to raise an <code>InvalidKeywordError</code> exception if <code>d</code> contains an unknown key.</p> <code>True</code> <code>redefine</code> <code>str</code> <p>Determine how to handle redefinition of existing keywords.  Should be one of the following strings:   - \"allow\": Allow redefining a keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"skip\": Don't redefine existing keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"raise\": Raise an RedefineError (default).</p> <code>'raise'</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def load_yaml(\n    self,\n    yamlfile: \"Union[Path, str]\",\n    timeout: float = 3,\n    strict: bool = True,\n    redefine: str = \"raise\",\n) -&gt; None:\n    \"\"\"Load YAML file with keyword definitions.\n\n    Arguments:\n        yamlfile: Path of URL to a YAML file to load.\n        timeout: Timeout when accessing remote files.\n        strict: Whether to raise an `InvalidKeywordError` exception if `d`\n            contains an unknown key.\n        redefine: Determine how to handle redefinition of existing\n            keywords.  Should be one of the following strings:\n              - \"allow\": Allow redefining a keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"skip\": Don't redefine existing keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"raise\": Raise an RedefineError (default).\n\n    \"\"\"\n    parsedkey = (yamlfile, strict, redefine)\n    if parsedkey in self.parsed:\n        return\n\n    with openfile(yamlfile, timeout=timeout, mode=\"rt\") as f:\n        d = yaml.safe_load(f)\n    try:\n        self._load_yaml(d, strict=strict, redefine=redefine)\n    except Exception as exc:\n        raise ParseError(f\"error parsing '{yamlfile}'\") from exc\n\n    self.parsed.add(parsedkey)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.missing_keywords","title":"<code>missing_keywords(self, ts, include_classes=False, return_existing=False)</code>","text":"<p>List keywords not defined in triplestore <code>ts</code>.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore object to check.</p> required <code>include_classes</code> <code>bool</code> <p>Also return missing classes.</p> <code>False</code> <code>return_existing</code> <code>bool</code> <p>If true, two lists are returned: - list of keywords missing in <code>ts</code> - list of keywords existing in <code>ts</code></p> <code>False</code> <p>Returns:</p> Type Description <code>Union[list, Tuple[list, list]]</code> <p>List with the names of keywords in this instance that are not defined in triplestore <code>ts</code>.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def missing_keywords(\n    self,\n    ts: \"Triplestore\",\n    include_classes: bool = False,\n    return_existing: bool = False,\n) -&gt; \"Union[list, Tuple[list, list]]\":\n    \"\"\"List keywords not defined in triplestore `ts`.\n\n    Arguments:\n        ts: Triplestore object to check.\n        include_classes: Also return missing classes.\n        return_existing: If true, two lists are returned:\n            - list of keywords missing in `ts`\n            - list of keywords existing in `ts`\n\n    Returns:\n        List with the names of keywords in this instance that are\n        not defined in triplestore `ts`.\n    \"\"\"\n    expanded = {k for k in self.keywords.keys() if \"://\" in k}\n    if include_classes:\n        expanded.update(self.expanded(c) for c in self.classnames())\n\n    if not expanded:\n        return []\n\n    query = f\"\"\"\n    SELECT ?s WHERE {{\n      VALUES ?s {{ { ' '.join(f'&lt;{iri}&gt;' for iri in expanded) } }}\n      ?s a ?o\n    }}\n    \"\"\"\n    existing = {r[0] for r in ts.query(query)}\n    missing = expanded.difference(existing)\n    missing_names = [self.shortname(k) for k in missing]\n\n    if return_existing:\n        existing_names = [self.keywords[k].name for k in existing]\n        return missing_names, existing_names\n    return missing_names\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.prefixed","title":"<code>prefixed(self, name, strict=True)</code>","text":"<p>Return prefixed name or <code>name</code>.</p> <p>Examples:</p> <p>keywords = Keywords() keywords.prefixed(\"title\") 'dcterms:title'</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def prefixed(self, name: str, strict: bool = True) -&gt; str:\n    \"\"\"Return prefixed name or `name`.\n\n    Example:\n\n    &gt;&gt;&gt; keywords = Keywords()\n    &gt;&gt;&gt; keywords.prefixed(\"title\")\n    'dcterms:title'\n    \"\"\"\n    if name in self.keywords:\n        return prefix_iri(self.keywords[name].iri, self.get_prefixes())\n    if name in self.data.resources:\n        return prefix_iri(\n            self.data.resources[name].iri,\n            self.get_prefixes(),\n            strict=strict,\n        )\n    if is_curie(name):\n        return name\n    return prefix_iri(name, self.get_prefixes(), strict=strict)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.range","title":"<code>range(self, keyword)</code>","text":"<p>Return the range of the keyword.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def range(self, keyword: str) -&gt; str:\n    \"\"\"Return the range of the keyword.\"\"\"\n    return self.keywords[keyword].range\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.save_context","title":"<code>save_context(self, outfile, indent=2)</code>","text":"<p>Save JSON-LD context file.</p> <p>Parameters:</p> Name Type Description Default <code>outfile</code> <code>FileLoc</code> <p>File to save the JSON-LD context to.</p> required <code>indent</code> <code>int</code> <p>Indentation level. Defaults to two.</p> <code>2</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def save_context(self, outfile: \"FileLoc\", indent: int = 2) -&gt; None:\n    \"\"\"Save JSON-LD context file.\n\n    Arguments:\n        outfile: File to save the JSON-LD context to.\n        indent: Indentation level. Defaults to two.\n    \"\"\"\n    context = {\"@context\": self.get_context()}\n    with open(outfile, \"wt\", encoding=\"utf-8\") as f:\n        json.dump(context, f, indent=indent)\n        f.write(os.linesep)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.save_markdown","title":"<code>save_markdown(self, outfile, keywords=None, classes=None, themes=None, namespace_filter=None, explanation=False, special=False)</code>","text":"<p>Save markdown file with documentation of the keywords.</p> <p>Parameters:</p> Name Type Description Default <code>outfile</code> <code>FileLoc</code> <p>File to save the markdown documentation to.</p> required <code>keywords</code> <code>Optional[Sequence[str]]</code> <p>Sequence of keywords to include.</p> <code>None</code> <code>classes</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>Include keywords that have these classes in their domain.</p> <code>None</code> <code>themes</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>Include keywords for these themes.</p> <code>None</code> <code>namespace_filter</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>A prefix, namespace or a sequence of these. Keep only keywords within this namespace. It is important that the namespace(s) are defined with prefixes in the Keywords object.</p> <code>None</code> <code>explanation</code> <code>bool</code> <p>Whether to include explanation of columns labels.</p> <code>False</code> <code>special</code> <code>bool</code> <p>Whether to generate documentation of special JSON-LD keywords.</p> <code>False</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def save_markdown(\n    self,\n    outfile: \"FileLoc\",\n    keywords: \"Optional[Sequence[str]]\" = None,\n    classes: \"Optional[Union[str, Sequence[str]]]\" = None,\n    themes: \"Optional[Union[str, Sequence[str]]]\" = None,\n    namespace_filter: \"Optional[Union[str, Sequence[str]]]\" = None,\n    explanation: bool = False,\n    special: bool = False,\n) -&gt; None:\n    \"\"\"Save markdown file with documentation of the keywords.\n\n    Arguments:\n        outfile: File to save the markdown documentation to.\n        keywords: Sequence of keywords to include.\n        classes: Include keywords that have these classes in their domain.\n        themes: Include keywords for these themes.\n        namespace_filter: A prefix, namespace or a sequence of these.\n            Keep only keywords within this namespace.\n            It is important that the namespace(s) are defined with\n            prefixes in the Keywords object.\n        explanation: Whether to include explanation of columns labels.\n        special: Whether to generate documentation of special\n            JSON-LD keywords.\n\n    \"\"\"\n    # pylint: disable=too-many-locals,too-many-branches\n    keywords, classes, themes = self._keywords_list(\n        keywords, classes, themes, namespace_filter=namespace_filter\n    )\n    ts = Triplestore(\"rdflib\")\n    for prefix, ns in self.data.get(\"prefixes\", {}).items():\n        ts.bind(prefix, ns)\n\n    if namespace_filter:\n        header = \"Keywords for namespaces:\" + \", \".join(namespace_filter)\n    elif themes:\n        header = \"Keywords for theme: \" + \", \".join(themes)\n    else:\n        header = \"Keywords\"\n    out = [\n        \"&lt;!-- Do not edit! This file is generated with Tripper. --&gt;\",\n        \"\",\n        header,\n        \"\",\n    ]\n    column_explanations = [\n        \"The meaning of the columns are as follows:\",\n        \"\",\n        \"- **Keyword**: The keyword referring to a property used for \"\n        \"the data documentation.\",\n        \"- **Range**: Refer to the class for the values of the keyword.\",\n        \"- **Conformance**: Whether the keyword is mandatory, recommended \"\n        \"or optional when documenting the given type of resources.\",\n        \"- **Definition**: The definition of the keyword.\",\n        \"- **Usage note**: Notes about how to use the keyword.\",\n        \"\",\n    ]\n    special_keywords = [\n        \"## Special keywords (from JSON-LD)\",\n        \"See the [JSON-LD specification] for more details.\",\n        \"\",\n        # pylint: disable=line-too-long\n        \"| Keyword    | Range         | Conformance | Definition                                                              | Usage note |\",\n        \"|------------|---------------|-------------|-------------------------------------------------------------------------|------------|\",\n        \"| [@id]      | IRI           | mandatory   | IRI identifying the resource to document.                               |            |\",\n        \"| [@type]    | IRI           | recommended | Ontological class defining the class of a node.                         |            |\",\n        \"| [@context] | dict&amp;#124list | optional    | Context defining namespace prefixes and additional keywords.            |            |\",\n        \"| [@base]    | namespace     | optional    | Base IRI against which relative IRIs are resolved.                      |            |\",\n        \"| [@vocab]   | namespace     | optional    | Used to expand properties and values in @type with a common prefix IRI. |            |\",\n        \"| [@graph]   | list          | optional    | Used for documenting multiple resources.                                |            |\",\n        \"\",\n    ]\n    if explanation:\n        out.extend(column_explanations)\n    if special:\n        out.extend(special_keywords)\n    refs = []\n\n    for cls in sorted(classes):\n        name = self.prefixed(cls)\n        shortname = iriname(name)\n        if shortname in self.data.resources:\n            resource = self.data.resources[shortname]\n        else:\n            for rname, resource in self.data.resources.items():\n                if self.prefixed(resource.iri) == name:\n                    shortname = rname\n                    break\n            else:\n                raise MissingKeyError(cls)\n\n        out.append(\"\")\n        out.append(f\"## Properties on [{shortname}]\")\n        if \"description\" in resource:\n            out.append(resource.description)\n        if \"subClassOf\" in resource:\n            out.append(\"\")\n            subcl = (\n                [resource.subClassOf]\n                if isinstance(resource.subClassOf, str)\n                else resource.subClassOf\n            )\n            out.append(\n                f\"- subClassOf: {', '.join(f'[{sc}]' for sc in subcl)}\"\n            )\n            for sc in subcl:\n                refs.append(f\"[{sc}]: {ts.expand_iri(sc)}\")\n        if \"iri\" in resource:\n            refs.append(f\"[{shortname}]: {ts.expand_iri(resource.iri)}\")\n        included_keywords = [\n            k\n            for k, v in self.keywords.items()\n            if name in v.domain and is_curie(k)\n        ]\n        out.extend(\n            self._keywords_table(keywords=sorted(included_keywords))\n        )\n        out.append(\"\")\n\n    # References\n    extra_refs = [\n        # pylint: disable=line-too-long\n        \"[@id]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n        \"[@type]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n        \"[@context]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n        \"[@base]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n        \"[@vocab]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n        \"[@graph]: https://www.w3.org/TR/json-ld11/#syntax-tokens-and-keywords\",\n    ]\n    refs.extend(extra_refs)\n    out.append(\"\")\n    out.append(\"\")\n    out.append(\"\")\n    out.extend(refs)\n    with open(outfile, \"wt\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(out) + \"\\n\")\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.save_markdown_prefixes","title":"<code>save_markdown_prefixes(self, outfile)</code>","text":"<p>Save markdown file with documentation of the prefixes.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def save_markdown_prefixes(self, outfile: \"FileLoc\") -&gt; None:\n    \"\"\"Save markdown file with documentation of the prefixes.\"\"\"\n    out = [\n        \"# Predefined prefixes\",\n        (\n            \"All namespace prefixes listed on this page are defined in \"\n            \"the [default JSON-LD context].\"\n        ),\n        (\n            \"See [User-defined prefixes] for how to extend this list \"\n            \"with additional namespace prefixes.\"\n        ),\n    ]\n    rows = [\n        [prefix, ns]\n        for prefix, ns in self.data.get(\"prefixes\", {}).items()\n    ]\n    out.extend(self._to_table([\"Prefix\", \"Namespace\"], rows))\n    out.append(\"\")\n    out.append(\"\")\n    out.extend(\n        [\n            # pylint: disable=line-too-long\n            \"[default JSON-LD context]: https://raw.githubusercontent.com/EMMC-ASBL/tripper/refs/heads/master/tripper/context/0.3/context.json\",\n            \"[User-defined prefixes]: customisation.md/#user-defined-prefixes\",\n        ]\n    )\n    with open(outfile, \"wt\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(out) + \"\\n\")\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.save_markdown_table","title":"<code>save_markdown_table(self, outfile, keywords)</code>","text":"<p>Save markdown file with documentation of the keywords.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def save_markdown_table(\n    self, outfile: \"FileLoc\", keywords: \"Sequence[str]\"\n) -&gt; None:\n    \"\"\"Save markdown file with documentation of the keywords.\"\"\"\n    table = self._keywords_table(keywords)\n    with open(outfile, \"wt\", encoding=\"utf-8\") as f:\n        f.write(os.linesep.join(table) + os.linesep)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.save_rdf","title":"<code>save_rdf(self, ts)</code>","text":"<p>Save to triplestore.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def save_rdf(self, ts: \"Triplestore\") -&gt; dict:\n    \"\"\"Save to triplestore.\"\"\"\n    # pylint: disable=import-outside-toplevel,cyclic-import\n    from tripper.datadoc.dataset import store\n\n    for prefix, ns in self.get_prefixes().items():\n        ts.bind(prefix, ns)\n\n    # Ensure that the schema for properties is stored\n    load_datadoc_schema(ts)\n\n    # Store all keywords that are not already in the triplestore\n    missing = self.missing_keywords(ts, include_classes=True)\n    dicts = self.asdicts(missing)\n    return store(ts, dicts)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.save_table","title":"<code>save_table(self, filename, format=None, names=None, strip=True, keymode='name', **kwargs)</code>","text":"<p>Load keywords from a csv file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>FileLoc</code> <p>File to load.</p> required <code>format</code> <code>Optional[str]</code> <p>File format. Unused.  Only csv is currently supported.</p> <code>None</code> <code>names</code> <code>Optional[Sequence]</code> <p>A sequence of keyword or class names to save.  The default is to save all keywords.</p> <code>None</code> <code>strip</code> <code>bool</code> <p>Whether to strip leading and trailing whitespaces from cells.</p> <code>True</code> <code>keymode</code> <code>str</code> <p>How to represent column headers.  Should be either \"name\", \"prefixed\" (CURIE) or \"expanded\" (full IRI).</p> <code>'name'</code> <code>kwargs</code> <p>Additional keyword arguments passed to the writer. For more details, see [write_csv()].</p> <code>{}</code> <p>References:</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def save_table(\n    self,\n    filename: \"FileLoc\",\n    format: \"Optional[str]\" = None,  # pylint: disable=unused-argument\n    names: \"Optional[Sequence]\" = None,\n    strip: bool = True,\n    keymode: str = \"name\",\n    **kwargs,\n) -&gt; None:\n    # pylint: disable=line-too-long\n    \"\"\"Load keywords from a csv file.\n\n    Arguments:\n        filename: File to load.\n        format: File format. Unused.  Only csv is currently supported.\n        names: A sequence of keyword or class names to save.  The\n            default is to save all keywords.\n        strip: Whether to strip leading and trailing whitespaces\n            from cells.\n        keymode: How to represent column headers.  Should be either\n            \"name\", \"prefixed\" (CURIE) or \"expanded\" (full IRI).\n        kwargs: Additional keyword arguments passed to the writer.\n            For more details, see [write_csv()].\n\n    References:\n    [write_csv()]: https://emmc-asbl.github.io/tripper/latest/api_reference/datadoc/tabledoc/#tripper.datadoc.tabledoc.TableDoc.write_csv\n    \"\"\"\n    # pylint: disable=import-outside-toplevel\n    from tripper.datadoc.tabledoc import TableDoc\n\n    dicts = self.asdicts(names, keymode=keymode)\n    td = TableDoc.fromdicts(dicts, type=None, keywords=self, strip=strip)\n    td.write_csv(filename, **kwargs)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.save_yaml","title":"<code>save_yaml(self, yamlfile, keywords=None, classes=None, themes=None, namespace_filter=None)</code>","text":"<p>Save YAML file with keyword definitions.</p> <p>Parameters:</p> Name Type Description Default <code>yamlfile</code> <code>Union[Path, str]</code> <p>File to save keyword definitions to.</p> required <code>keywords</code> <code>Optional[Sequence[str]]</code> <p>Sequence of keywords to include.</p> <code>None</code> <code>classes</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>Include keywords that have these classes in their domain.</p> <code>None</code> <code>themes</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>Include keywords for these themes.</p> <code>None</code> <code>namespace_filter</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>A prefix, namespace or a sequence of these. If given, keep only keywords and classes from the returned <code>keywordset</code> and <code>classet</code> with IRIs in one of these namespaces.</p> <code>None</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def save_yaml(\n    self,\n    yamlfile: \"Union[Path, str]\",\n    keywords: \"Optional[Sequence[str]]\" = None,\n    classes: \"Optional[Union[str, Sequence[str]]]\" = None,\n    themes: \"Optional[Union[str, Sequence[str]]]\" = None,\n    namespace_filter: \"Optional[Union[str, Sequence[str]]]\" = None,\n) -&gt; None:\n    \"\"\"Save YAML file with keyword definitions.\n\n    Arguments:\n        yamlfile: File to save keyword definitions to.\n        keywords: Sequence of keywords to include.\n        classes: Include keywords that have these classes in their domain.\n        themes: Include keywords for these themes.\n        namespace_filter: A prefix, namespace or a sequence of these.\n            If given, keep only keywords and classes from the returned\n            `keywordset` and `classet` with IRIs in one of\n            these namespaces.\n\n    \"\"\"\n    keywords, classes, themes = self._keywords_list(\n        keywords, classes, themes, namespace_filter=namespace_filter\n    )\n    resources = {}\n    for cls, clsval in self.data.resources.items():\n        if self.prefixed(cls) in classes:\n            resources[cls] = dict(clsval.copy())\n            resources[cls][\"keywords\"] = {}\n            for k, v in self.data.resources[cls].keywords.items():\n                if self.prefixed(k) in keywords:\n                    resources[cls][\"keywords\"][k] = dict(v)\n    data = dict(self.data.copy())\n    del data[\"resources\"]\n    recursive_update(data, {}, cls=dict)\n    data[\"resources\"] = resources\n\n    with open(yamlfile, \"wt\", encoding=\"utf-8\") as f:\n        yaml.safe_dump(data, f, sort_keys=False)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.shortname","title":"<code>shortname(self, iri)</code>","text":"<p>Return the short name of <code>iri</code>.</p> <p>If <code>strict</code> is False, return last component of the expanded IRI.</p> <p>Examples:</p> <p>keywords = Keywords() keywords.shortname(\"dcterms:title\") 'title'</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def shortname(self, iri: str) -&gt; str:\n    \"\"\"Return the short name of `iri`.\n\n    If `strict` is False, return last component of the expanded IRI.\n\n    Example:\n\n    &gt;&gt;&gt; keywords = Keywords()\n    &gt;&gt;&gt; keywords.shortname(\"dcterms:title\")\n    'title'\n\n    \"\"\"\n    if iri in self.keywords:\n        return self.keywords[iri].name\n    if iri in self.data.resources.keys():\n        return iri\n    expanded = self.expanded(iri)\n    for k, v in self.data.resources.items():\n        if expanded == self.expanded(v.iri):\n            return k\n        for kk, vv in v.keywords.items():\n            if expanded == self.expanded(vv.iri):\n                return kk\n    raise InvalidKeywordError(iri)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.superclasses","title":"<code>superclasses(self, cls)</code>","text":"<p>Return a list with <code>cls</code> and it superclasses prefixed.</p> <p>Examples:</p> <p>keywords = Keywords() keywords.superclasses(\"Dataset\") ... # doctest: +NORMALIZE_WHITESPACE ['dcat:Dataset',  'dcat:Resource',  'emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a']</p> <p>keywords.superclasses(\"dcat:Dataset\") ... # doctest: +NORMALIZE_WHITESPACE ['dcat:Dataset',  'dcat:Resource',  'emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a']</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def superclasses(self, cls: str) -&gt; \"Union[str, list]\":\n    \"\"\"Return a list with `cls` and it superclasses prefixed.\n\n    Example:\n\n    &gt;&gt;&gt; keywords = Keywords()\n    &gt;&gt;&gt; keywords.superclasses(\"Dataset\")\n    ... # doctest: +NORMALIZE_WHITESPACE\n    ['dcat:Dataset',\n     'dcat:Resource',\n     'emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a']\n\n    &gt;&gt;&gt; keywords.superclasses(\"dcat:Dataset\")\n    ... # doctest: +NORMALIZE_WHITESPACE\n    ['dcat:Dataset',\n     'dcat:Resource',\n     'emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a']\n\n    \"\"\"\n    if cls in self.data.resources:\n        r = self.data.resources[cls]\n    else:\n        cls = prefix_iri(cls, self.get_prefixes())\n        rlst = [r for r in self.data.resources.values() if cls == r.iri]\n        if not rlst:\n            raise NoSuchTypeError(cls)\n        if len(rlst) &gt; 1:\n            raise RuntimeError(\n                f\"{cls} matches more than one resource: \"\n                f\"{', '.join(r.iri for r in rlst)}\"\n            )\n        r = rlst[0]\n\n    if \"subClassOf\" in r:\n        if isinstance(r.subClassOf, str):\n            return [r.iri, r.subClassOf]\n        return [r.iri] + r.subClassOf\n    return r.iri\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.Keywords.typename","title":"<code>typename(self, type)</code>","text":"<p>Return the short name of <code>type</code>.</p> <p>Examples:</p> <p>keywords = Keywords() keywords.typename(\"dcat:Dataset\") 'Dataset'</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def typename(self, type) -&gt; str:\n    \"\"\"Return the short name of `type`.\n\n    Example:\n\n    &gt;&gt;&gt; keywords = Keywords()\n    &gt;&gt;&gt; keywords.typename(\"dcat:Dataset\")\n    'Dataset'\n\n    \"\"\"\n    if type in self.data.resources:\n        return type\n    prefixed = prefix_iri(type, self.get_prefixes())\n    for name, r in self.data.resources.items():\n        if prefixed == r.iri:\n            return name\n    raise NoSuchTypeError(type)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.get_keywords","title":"<code>get_keywords(keywords=None, format=None, theme='ddoc:datadoc', yamlfile=None, timeout=3, strict=False, redefine='raise')</code>","text":"<p>A convenient function that returns a Context instance.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>Optional[KeywordsType]</code> <p>Optional existing keywords object.</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Format of input if <code>keywords</code> refer to a file that can be loaded.</p> <code>None</code> <code>theme</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>IRI of one of more themes to load keywords for.</p> <code>'ddoc:datadoc'</code> <code>yamlfile</code> <code>Optional[FileLoc]</code> <p>YAML file with keyword definitions to parse.  May also be an URI in which case it will be accessed via HTTP GET. Deprecated. Use the <code>load_yaml()</code> or <code>add()</code> methods instead.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Timeout in case <code>yamlfile</code> is a URI.</p> <code>3</code> <code>strict</code> <code>bool</code> <p>Whether to raise an <code>InvalidKeywordError</code> exception if <code>d</code> contains an unknown key.</p> <code>False</code> <code>redefine</code> <code>str</code> <p>Determine how to handle redefinition of existing keywords.  Should be one of the following strings:   - \"allow\": Allow redefining a keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"skip\": Don't redefine existing keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"raise\": Raise an RedefineError (default).</p> <code>'raise'</code> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def get_keywords(\n    keywords: \"Optional[KeywordsType]\" = None,\n    format: \"Optional[str]\" = None,\n    theme: \"Optional[Union[str, Sequence[str]]]\" = \"ddoc:datadoc\",\n    yamlfile: \"Optional[FileLoc]\" = None,\n    timeout: float = 3,\n    strict: bool = False,\n    redefine: str = \"raise\",\n) -&gt; \"Keywords\":\n    \"\"\"A convenient function that returns a Context instance.\n\n    Arguments:\n        keywords: Optional existing keywords object.\n        format: Format of input if `keywords` refer to a file that can be\n            loaded.\n        theme: IRI of one of more themes to load keywords for.\n        yamlfile: YAML file with keyword definitions to parse.  May also\n            be an URI in which case it will be accessed via HTTP GET.\n            Deprecated. Use the `load_yaml()` or `add()` methods instead.\n        timeout: Timeout in case `yamlfile` is a URI.\n        strict: Whether to raise an `InvalidKeywordError` exception if `d`\n            contains an unknown key.\n        redefine: Determine how to handle redefinition of existing\n            keywords.  Should be one of the following strings:\n              - \"allow\": Allow redefining a keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"skip\": Don't redefine existing keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"raise\": Raise an RedefineError (default).\n    \"\"\"\n    if isinstance(keywords, Keywords):\n        kw = keywords\n        if theme:\n            kw.add_theme(\n                theme, timeout=timeout, strict=strict, redefine=redefine\n            )\n    else:\n        kw = Keywords(theme=theme)\n        if keywords:\n            kw.add(\n                keywords,\n                format=format,\n                timeout=timeout,\n                strict=strict,\n                redefine=redefine,\n            )\n\n    if yamlfile:\n        warnings.warn(\n            \"The `yamlfile` argument is deprecated. Use the `load_yaml()` or \"\n            \"`add()` methods instead.\",\n            DeprecationWarning,\n        )\n        kw.load_yaml(\n            yamlfile, timeout=timeout, strict=strict, redefine=redefine\n        )\n\n    return kw\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.load_datadoc_schema","title":"<code>load_datadoc_schema(ts)</code>","text":"<p>Load schema for data documentation to triplestore <code>ts</code>.</p> <p>It is safe to call this function more than once.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def load_datadoc_schema(ts: \"Triplestore\") -&gt; None:\n    \"\"\"Load schema for data documentation to triplestore `ts`.\n\n    It is safe to call this function more than once.\n    \"\"\"\n    if not ts.query(f\"ASK WHERE {{ &lt;{-DDOC}&gt; a &lt;{OWL.Ontology}&gt; }}\"):\n        ts.bind(\"ddoc\", DDOC)\n        path = Path(tripper.__file__).parent / \"context\" / \"datadoc.ttl\"\n        ts.parse(path)\n</code></pre>"},{"location":"api_reference/datadoc/keywords/#tripper.datadoc.keywords.main","title":"<code>main(argv=None)</code>","text":"<p>Main function providing CLI access to keywords.</p> Source code in <code>tripper/datadoc/keywords.py</code> <pre><code>def main(argv=None):  # pylint: disable=too-many-statements\n    \"\"\"Main function providing CLI access to keywords.\"\"\"\n    import argparse  # pylint: disable=import-outside-toplevel\n\n    parser = argparse.ArgumentParser(\n        description=(\n            \"Tool for generation of JSON-LD context and documentation from \"\n            \"keyword definitions.\"\n        )\n    )\n    parser.add_argument(\n        \"--input\",\n        \"-i\",\n        metavar=\"FILENAME\",\n        default=[],\n        action=\"append\",\n        help=\"Load keywords from this file. May be given multiple times.\",\n    )\n    parser.add_argument(\n        \"--format\",\n        \"-f\",\n        metavar=\"FORMAT\",\n        nargs=\"?\",\n        action=\"append\",\n        help=(\n            \"Formats of --input. Default format is inferred from the file \"\n            \"name extension.  If given, this option must be provided the \"\n            \"same number of times as --input.\"\n        ),\n    )\n    parser.add_argument(\n        \"--theme\",\n        \"-t\",\n        metavar=\"NAME\",\n        nargs=\"?\",\n        default=[],\n        action=\"append\",\n        help=\"Load keywords from this theme.\",\n    )\n    parser.add_argument(\n        \"--strict\",\n        action=\"store_true\",\n        help=\"Whether to raise an exception of input contains an unknown key.\",\n    )\n    parser.add_argument(  # pylint: disable=duplicate-code\n        \"--redefine\",\n        default=\"raise\",\n        choices=[\"raise\", \"allow\", \"skip\"],\n        help=\"How to handle redifinition of existing keywords.\",\n    )\n    parser.add_argument(\n        \"--write-context\",\n        \"--write-json\",\n        \"-c\",\n        metavar=\"FILENAME\",\n        help=\"Generate JSON-LD context file.\",\n    )\n    parser.add_argument(\n        \"--write-kw-md\",\n        \"-k\",\n        metavar=\"FILENAME\",\n        help=\"Generate keywords Markdown documentation.\",\n    )\n    parser.add_argument(\n        \"--write-kw-yaml\",\n        \"-y\",\n        metavar=\"FILENAME\",\n        help=\"Generate keywords YAML file.\",\n    )\n    parser.add_argument(\n        \"--explanation\",\n        \"-e\",\n        action=\"store_true\",\n        help=\"Whether to include explanation in generated documentation.\",\n    )\n    parser.add_argument(\n        \"--special-keywords\",\n        \"-s\",\n        action=\"store_true\",\n        help=\"Whether to include special keywords in generated documentation.\",\n    )\n    parser.add_argument(\n        \"--namespace-filter\",\n        \"--nf\",\n        metavar=\"NAMESPACE\",\n        action=\"append\",\n        help=(\n            \"Keep only keywords with IRIs in \"\n            \"the namespace \"\n            \"provided by this option.  Can be provided more that once. \"\n            \"To be used with the --write-kw-md option. \"\n            \"A namespace can be specified by its full IRI or by a pre-defined \"\n            \"prefix.\"\n            \"Note that the namespace must be defined with a prefix. \"\n            \"If missing,\"\n            \"it can be added with --prefix.\"\n        ),\n    )\n    parser.add_argument(\n        \"--kw\",\n        metavar=\"KW1,KW2,...\",\n        help=(\n            \"Comma-separated list of keywords to include in generated table. \"\n            \"Implies --write-kw-md.\"\n        ),\n    )\n    parser.add_argument(\n        \"--classes\",\n        metavar=\"C1,C2,...\",\n        help=(\n            \"Generate keywords Markdown documentation for any keywords who's \"\n            \"domain is in the comma-separated list CLASSES. \"\n            \"Implies --write-kw-md.\"\n        ),\n    )\n    parser.add_argument(\n        \"--themes\",\n        metavar=\"T1,T2,...\",\n        help=(\n            \"Generate keywords Markdown documentation for any keywords that \"\n            \"belong to one of the themes in the comma-separated list THEMES. \"\n            \"Implies --write-kw-md, --write-kw-json or --write-kw-yaml.\"\n        ),\n    )\n    parser.add_argument(\n        \"--write-prefixes\",\n        \"-w\",\n        metavar=\"FILENAME\",\n        help=\"Write prefixes Markdown file.\",\n    )\n    parser.add_argument(\n        \"--prefix\",\n        \"-p\",\n        metavar=\"PREFIX:NAMESPACE\",\n        default=[],\n        action=\"append\",\n        help=\"Add the prefix given as tuple PREFIX=NAMESPACE, \"\n        \"can be used multiple times.\",\n    )\n\n    parser.add_argument(\n        \"--list-themes\",\n        action=\"store_true\",\n        help=\"List installed themes and exit.\",\n    )\n    parser.add_argument(\n        \"--set-prefix\",\n        \"-P\",\n        metavar=\"PREFIX:NAMESPACE\",\n        action=\"append\",\n        help=(\n            \"Set perfix. This option may be given more than once. \"\n            \"It can be used as a workaround for PrefixMismatchError.\"\n        ),\n    )\n\n    args = parser.parse_args(argv)\n\n    if args.list_themes:\n        themes = [ep.value for ep in get_entry_points(\"tripper.keywords\")]\n        parser.exit(message=os.linesep.join(themes) + os.linesep)\n\n    if args.format and len(args.format) != len(args.input):\n        parser.error(\n            \"The number of --format options must match the number \"\n            \"of --input options.\"\n        )\n\n    if args.theme:\n        default_theme = None if None in args.theme else args.theme[0]\n    else:\n        default_theme = \"ddoc:datadoc\"\n\n    kw = Keywords(theme=default_theme)\n\n    if args.prefix:\n        for p in asseq(args.prefix):\n            if \":\" not in p:\n                parser.error(\n                    f\"Invalid prefix definition '{p}'. Must be of the \"\n                    \"form PREFIX:NAMESPACE.\"\n                )\n            prefix, namespace = p.split(\":\", 1)\n            kw.add_prefix(prefix, namespace, replace=True)\n\n    for theme in args.theme[1:]:\n        if theme:\n            kw.add_theme(theme, strict=args.strict, redefine=args.redefine)\n\n    if args.set_prefix:\n        for s in args.set_prefix:\n            prefix, ns = s.split(\":\", 1)\n            kw.data.prefixes[prefix] = ns\n\n    kw.add(args.input, args.format, strict=args.strict, redefine=args.redefine)\n\n    if args.write_context:\n        kw.save_context(args.write_context)\n\n    if args.write_kw_md or args.kw or args.classes or args.themes:\n        kw.save_markdown(\n            args.write_kw_md,\n            keywords=args.kw.split(\",\") if args.kw else None,\n            classes=args.classes.split(\",\") if args.classes else None,\n            themes=args.themes.split(\",\") if args.themes else None,\n            explanation=args.explanation,\n            special=args.special_keywords,\n            namespace_filter=args.namespace_filter,\n        )\n\n    if args.write_prefixes:\n        kw.save_markdown_prefixes(args.write_prefixes)\n\n    if args.write_kw_yaml:\n        kw.save_yaml(\n            args.write_kw_yaml, namespace_filter=args.namespace_filter\n        )\n</code></pre>"},{"location":"api_reference/datadoc/prefixes/","title":"prefixes","text":"<p>Utilities for storing and loading namespace prefixes in the triplestore.</p>"},{"location":"api_reference/datadoc/prefixes/#tripper.datadoc.prefixes.bnode","title":"<code>bnode()</code>","text":"<p>Returns a new unique blank node.</p> Source code in <code>tripper/datadoc/prefixes.py</code> <pre><code>def bnode() -&gt; str:\n    \"\"\"Returns a new unique blank node.\"\"\"\n    global _bnode_counter  # pylint: disable=global-statement\n    _bnode_counter += 1\n    return f\"_:b{_bnode_counter}\"\n</code></pre>"},{"location":"api_reference/datadoc/prefixes/#tripper.datadoc.prefixes.load_prefixes","title":"<code>load_prefixes(ts, prefix=None, namespace=None)</code>","text":"<p>Returns an list of all matching prefix-namespace pairs defined in the triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore instance to load from.</p> required <code>prefix</code> <p>prefix to search for</p> <code>None</code> <code>namespace</code> <p>namespace to search for (URI)</p> <code>None</code> <p>Returns:</p> Type Description <code>list</code> <p>List of <code>(prefix, namespace)</code> tuples.</p> Source code in <code>tripper/datadoc/prefixes.py</code> <pre><code>def load_prefixes(ts: \"Triplestore\", prefix=None, namespace=None) -&gt; list:\n    \"\"\"Returns an list of all matching prefix-namespace\n    pairs defined in the triplestore.\n\n    Arguments:\n        ts: Triplestore instance to load from.\n        prefix: prefix to search for\n        namespace: namespace to search for (URI)\n\n    Returns:\n        List of `(prefix, namespace)` tuples.\n    \"\"\"\n    bind = []\n    if prefix is not None:\n        bind.append(f\"BIND({Literal(prefix).n3()} AS ?prefix)\")\n    if namespace is not None:\n        bind.append(\n            f\"BIND({Literal(namespace, datatype=XSD.anyURI).n3()} AS ?ns)\"\n        )\n    binds = \"\\n  \".join(bind)\n    query = f\"\"\"\n    SELECT ?prefix ?ns WHERE {{\n      {binds}\n      ?s &lt;{VANN.preferredNamespacePrefix}&gt; ?prefix ;\n         &lt;{VANN.preferredNamespaceUri}&gt; ?ns .\n    }}\n    \"\"\"\n    return ts.query(query)\n</code></pre>"},{"location":"api_reference/datadoc/prefixes/#tripper.datadoc.prefixes.save_prefixes","title":"<code>save_prefixes(ts, prefixes)</code>","text":"<p>Save prefixes to the triplestore.</p> <p>Prefix-namespace pairs already in the knowledge base will not be     duplicated.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore instance to store the prefixes to.</p> required <code>prefixes</code> <code>dict</code> <p>Dict to store. It should map prefixes to namespaces.</p> required Source code in <code>tripper/datadoc/prefixes.py</code> <pre><code>def save_prefixes(ts: \"Triplestore\", prefixes: dict) -&gt; None:\n    \"\"\"Save prefixes to the triplestore.\n\n    Prefix-namespace pairs already in the knowledge base will not be\n        duplicated.\n\n    Arguments:\n        ts: Triplestore instance to store the prefixes to.\n        prefixes: Dict to store. It should map prefixes to namespaces.\n\n    \"\"\"\n    existing = set(load_prefixes(ts))\n    triples = []\n    for k, v in prefixes.items():\n        if (k, v) not in existing:\n            b = bnode()\n            ns = Literal(v, datatype=XSD.anyURI)\n            triples.extend(\n                [\n                    (b, RDF.type, OWL.Ontology),\n                    (b, VANN.preferredNamespacePrefix, Literal(k)),\n                    (b, VANN.preferredNamespaceUri, ns),\n                ]\n            )\n    ts.add_triples(triples)\n</code></pre>"},{"location":"api_reference/datadoc/tabledoc/","title":"tabledoc","text":"<p>Basic interface for tabular documentation of datasets.</p>"},{"location":"api_reference/datadoc/tabledoc/#tripper.datadoc.tabledoc.TableDoc","title":"<code> TableDoc        </code>","text":"<p>Representation of tabular documentation of datasets.</p> <p>Parameters:</p> Name Type Description Default <code>header</code> <code>Sequence[str]</code> <p>Sequence of column header labels.  Nested data can be represented by dot-separated label strings (e.g. \"distribution.downloadURL\")</p> required <code>data</code> <code>Sequence[Sequence[str]]</code> <p>Sequence of rows of data. Each row documents an entry.</p> required <code>type</code> <code>Optional[str]</code> <p>Type of data to save (applies to all rows).  Should either be one of the pre-defined names: \"dataset\", \"distribution\", \"accessService\", \"parser\" and \"generator\" or an IRI to a class in an ontology.</p> <code>None</code> <code>theme</code> <code>Optional[Union[str, Sequence[str]]]</code> <p>Name of one of more themes to load keywords for.</p> <code>'ddoc:datadoc'</code> <code>keywords</code> <code>Optional[KeywordsType]</code> <p>Keywords object with additional keywords definitions. If not provided, only default keywords are considered.</p> <code>None</code> <code>context</code> <code>Optional[ContextType]</code> <p>Additional user-defined context that should be returned on top of the default context.  It may be a string with an URL to the user-defined context, a dict with the user-defined context or a sequence of strings and dicts.</p> <code>None</code> <code>prefixes</code> <code>Optional[dict]</code> <p>Dict with prefixes in addition to those included in the JSON-LD context.  Should map namespace prefixes to IRIs.</p> <code>None</code> <code>strip</code> <code>bool</code> <p>Whether to strip leading and trailing whitespaces from cells.</p> <code>True</code> <code>strict</code> <code>bool</code> <p>Whether to raise an <code>InvalidKeywordError</code> exception if <code>d</code> contains an unknown key.</p> <code>False</code> <code>redefine</code> <code>str</code> <p>Determine how to handle redefinition of existing keywords.  Should be one of the following strings:   - \"allow\": Allow redefining a keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"skip\": Don't redefine existing keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"raise\": Raise an RedefineError (default).</p> <code>'raise'</code> Source code in <code>tripper/datadoc/tabledoc.py</code> <pre><code>class TableDoc:\n    \"\"\"Representation of tabular documentation of datasets.\n\n    Arguments:\n        header: Sequence of column header labels.  Nested data can\n            be represented by dot-separated label strings (e.g.\n            \"distribution.downloadURL\")\n        data: Sequence of rows of data. Each row documents an entry.\n        type: Type of data to save (applies to all rows).  Should\n            either be one of the pre-defined names: \"dataset\",\n            \"distribution\", \"accessService\", \"parser\" and \"generator\"\n            or an IRI to a class in an ontology.\n        theme: Name of one of more themes to load keywords for.\n        keywords: Keywords object with additional keywords definitions.\n            If not provided, only default keywords are considered.\n        context: Additional user-defined context that should be\n            returned on top of the default context.  It may be a\n            string with an URL to the user-defined context, a dict\n            with the user-defined context or a sequence of strings and\n            dicts.\n        prefixes: Dict with prefixes in addition to those included in the\n            JSON-LD context.  Should map namespace prefixes to IRIs.\n        strip: Whether to strip leading and trailing whitespaces from cells.\n        strict: Whether to raise an `InvalidKeywordError` exception if `d`\n            contains an unknown key.\n        redefine: Determine how to handle redefinition of existing\n            keywords.  Should be one of the following strings:\n              - \"allow\": Allow redefining a keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"skip\": Don't redefine existing keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"raise\": Raise an RedefineError (default).\n\n    \"\"\"\n\n    # pylint: disable=redefined-builtin,too-few-public-methods\n    # pylint: disable=too-many-arguments\n\n    def __init__(\n        self,\n        header: \"Sequence[str]\",\n        data: \"Sequence[Sequence[str]]\",\n        type: \"Optional[str]\" = None,\n        theme: \"Optional[Union[str, Sequence[str]]]\" = \"ddoc:datadoc\",\n        keywords: \"Optional[KeywordsType]\" = None,\n        context: \"Optional[ContextType]\" = None,\n        prefixes: \"Optional[dict]\" = None,\n        strip: bool = True,\n        strict: bool = False,\n        redefine: str = \"raise\",\n    ) -&gt; None:\n        self.header = list(header)\n        self.data = [list(row) for row in data]\n        self.type = type\n        self.keywords = get_keywords(\n            keywords=keywords,\n            theme=theme,\n            strict=strict,\n            redefine=redefine,\n        )\n        self.context = get_context(\n            context=context,\n            keywords=self.keywords,\n            prefixes=prefixes,\n        )\n        self.strip = strip\n\n    def save(self, ts: Triplestore) -&gt; None:\n        \"\"\"Save tabular datadocumentation to triplestore.\"\"\"\n        self.context.add_context(\n            {prefix: str(ns) for prefix, ns in ts.namespaces.items()}\n        )\n\n        for prefix, ns in self.context.get_prefixes().items():\n            ts.bind(prefix, ns)\n\n        store(ts, self.asdicts(), type=self.type, context=self.context)\n\n    def asdicts(self) -&gt; \"List[dict]\":\n        \"\"\"Return the table as a list of dicts.\"\"\"\n        results = []\n        for row in self.data:\n            d = AttrDict()\n            for i, colname in enumerate(self.header):\n                cell = row[i].strip() if row[i] and self.strip else row[i]\n                if cell:\n\n                    # Convert cell value to correct Python type\n                    if not colname.startswith(\"@\"):\n                        leafname = colname.split(\".\")[-1]\n                        df = self.context.getdef(leafname.split(\"[\")[0])\n                        if \"@type\" in df and df[\"@type\"] != \"@id\":\n                            cell = Literal(cell, datatype=df[\"@type\"]).value\n\n                    addnested(\n                        d, colname.strip() if self.strip else colname, cell\n                    )\n            results.append(stripnested(d))\n        ld = told(\n            results,\n            type=self.type,\n            prefixes=self.context.get_prefixes(),\n            context=self.context,\n        )\n        dicts = ld[\"@graph\"]\n        return dicts\n\n    @staticmethod\n    def fromdicts(\n        dicts: \"Sequence[dict]\",\n        type: \"Optional[str]\" = None,\n        keywords: \"Optional[KeywordsType]\" = None,\n        context: \"Optional[ContextType]\" = None,\n        prefixes: \"Optional[dict]\" = None,\n        strip: bool = True,\n    ) -&gt; \"TableDoc\":\n        \"\"\"Create new TableDoc instance from a sequence of dicts.\n\n        Arguments:\n            dicts: Sequence of single-resource dicts.\n            type: Type of data to save (applies to all rows).  Should\n                either be one of the pre-defined names: \"Dataset\",\n                \"Distribution\", \"AccessService\", \"Parser\" and\n                \"Generator\" or an IRI to a class in an ontology.\n            keywords: Keywords object with additional keywords definitions.\n                If not provided, only default keywords are considered.\n            context: Additional user-defined context that should be\n                returned on top of the default context.  It may be a\n                string with an URL to the user-defined context, a dict\n                with the user-defined context or a sequence of strings\n                and dicts.\n            prefixes: Dict with prefixes in addition to those included\n                in the JSON-LD context.  Should map namespace prefixes\n                to IRIs.\n            strip: Whether to strip leading and trailing whitespaces\n                from cells.\n\n        Returns:\n            New TableDoc instance.\n\n        \"\"\"\n        # Store the header as keys in a dict to keep ordering\n        headdict = {\"@id\": True}\n\n        def addheaddict(d, prefix=\"\"):\n            \"\"\"Add keys in `d` to headdict.\n\n            Nested dicts will result in dot-separated keys.\n            \"\"\"\n            for k, v in d.items():\n                if k == \"@context\":\n                    pass\n                elif isinstance(v, dict):\n                    d = v.copy()\n                    d.pop(\"@type\", None)\n                    addheaddict(d, k + \".\")\n                else:\n                    headdict[prefix + k] = True\n\n        def tolist(v, mult, pad=None):\n            \"\"\"Return `v` as a list of length `mult` with given padding.\"\"\"\n            if isinstance(v, list):\n                return v + [pad] * (mult - len(v))\n            return [v] + [pad] * (mult - 1)\n\n        # Assign the headdict\n        for d in dicts:\n            addheaddict(d)\n\n        header = list(headdict)\n\n        # Calculate multiplicity of each header label\n        mult = [1] * len(header)\n        for dct in dicts:\n            for i, head in enumerate(header):\n                if head in dct and isinstance(dct[head], list):\n                    mult[i] = max(mult[i], len(dct[head]))\n\n        # Assign table data. Multiplicity and nested dicts are accounted for\n        data = []\n        for dct in dicts:\n            row = []\n            for head, m in zip(header, mult):\n                if head in dct:\n                    row.extend(tolist(dct[head], m))\n                else:\n                    d = dct\n                    for key in head.split(\".\"):\n                        d = d.get(key, {})\n                    row.extend(tolist(d if d != {} else None, m))\n            data.append(row)\n\n        # New multiplied header\n        newheader = []\n        for head, m in zip(header, mult):\n            newheader.extend(tolist(head, m, head))\n\n        return TableDoc(\n            header=newheader,\n            data=data,\n            type=type,\n            keywords=keywords,\n            context=context,\n            prefixes=prefixes,\n            strip=strip,\n        )\n\n    @staticmethod\n    def parse_csv(\n        csvfile: \"Union[Iterable[str], Path, str]\",\n        type: \"Optional[str]\" = None,\n        keywords: \"Optional[KeywordsType]\" = None,\n        context: \"Optional[ContextType]\" = None,\n        prefixes: \"Optional[dict]\" = None,\n        encoding: str = \"utf-8\",\n        dialect: \"Optional[Union[csv.Dialect, str]]\" = None,\n        strict: bool = False,\n        redefine: str = \"raise\",\n        **kwargs,\n    ) -&gt; \"TableDoc\":\n        # pylint: disable=line-too-long\n        \"\"\"Parse a csv file using the standard library csv module.\n\n        Arguments:\n            csvfile: Name of CSV file to parse or an iterable of strings.\n            type: Type of data to save (applies to all rows).  Should\n                either be one of the pre-defined names: \"Dataset\",\n                \"Distribution\", \"AccessService\", \"Parser\" and \"Generator\"\n                or an IRI to a class in an ontology.\n            keywords: Keywords object with additional keywords definitions.\n                If not provided, only default keywords are considered.\n            context: Dict with user-defined JSON-LD context.\n            prefixes: Dict with prefixes in addition to those included in the\n                JSON-LD context.  Should map namespace prefixes to IRIs.\n            encoding: The encoding of the csv file.  Note that Excel may\n                encode as \"ISO-8859\" (which was commonly used in the 1990th).\n            dialect: A subclass of csv.Dialect, or the name of the dialect,\n                specifying how the `csvfile` is formatted.  For more details,\n                see [Dialects and Formatting Parameters].\n            strict: Whether to raise an `InvalidKeywordError` exception if `d`\n                contains an unknown key.\n            redefine: Determine how to handle redefinition of existing\n                keywords.  Should be one of the following strings:\n                  - \"allow\": Allow redefining a keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"skip\": Don't redefine existing keyword. Emits a\n                    `RedefineKeywordWarning`.\n                  - \"raise\": Raise an RedefineError (default).\n            kwargs: Additional keyword arguments overriding individual\n                formatting parameters.  For more details, see\n                [Dialects and Formatting Parameters].\n\n        Returns:\n            New TableDoc instance.\n\n        References:\n        [Dialects and Formatting Parameters]: https://docs.python.org/3/library/csv.html#dialects-and-formatting-parameters\n        \"\"\"\n\n        def read(f, dialect):\n            \"\"\"Return csv reader from file-like object `f`.\"\"\"\n            if dialect is None and not kwargs:\n                sample = f.read(1024)\n                try:\n                    dialect = csv.Sniffer().sniff(sample, delimiters=\",;\\t \")\n                except csv.Error:\n                    # The build-in sniffer not always work well with\n                    # non-numerical csv files. Try our simple sniffer\n                    dialect = csvsniff(sample)\n                finally:\n                    f.seek(0)\n            reader = csv.reader(f, dialect=dialect, **kwargs)\n            header = next(reader)\n            data = list(reader)\n            return header, data\n\n        if isinstance(csvfile, (str, Path)):\n            with openfile(csvfile, mode=\"rt\", encoding=encoding) as f:\n                header, data = read(f, dialect)\n        else:\n            header, data = read(csvfile, dialect)\n\n        return TableDoc(\n            header=header,\n            data=data,\n            type=type,\n            keywords=keywords,\n            context=context,\n            prefixes=prefixes,\n            strict=strict,\n            redefine=redefine,\n        )\n\n    def write_csv(\n        self,\n        csvfile: \"Union[Path, str, Writer]\",\n        encoding: str = \"utf-8\",\n        dialect: \"Union[csv.Dialect, str]\" = \"excel\",\n        prefixes: \"Optional[dict]\" = None,\n        **kwargs,\n    ) -&gt; None:\n        # pylint: disable=line-too-long\n        \"\"\"Write the table to a csv file using the standard library csv module.\n\n        Arguments:\n            csvfile: File-like object or name of CSV file to write.\n            encoding: The encoding of the csv file.\n            dialect: A subclass of csv.Dialect, or the name of the dialect,\n                specifying how the `csvfile` is formatted.  For more details,\n                see [Dialects and Formatting Parameters].\n            prefixes: Prefixes used to compact the header.\n            kwargs: Additional keyword arguments overriding individual\n                formatting parameters.  For more details, see\n                [Dialects and Formatting Parameters].\n\n        References:\n        [Dialects and Formatting Parameters]: https://docs.python.org/3/library/csv.html#dialects-and-formatting-parameters\n        \"\"\"\n\n        def write(f):\n            writer = csv.writer(f, dialect=dialect, **kwargs)\n\n            # TODO: use self.context and compact to shortnames\n            if prefixes:\n                header = []\n                for h in self.header:\n                    for prefix, ns in prefixes.items():\n                        if h.startswith(str(ns)):\n                            header.append(f\"{prefix}:{h[len(str(ns)):]}\")\n                            break\n                    else:\n                        header.append(h)\n                writer.writerow(header)\n            else:\n                writer.writerow(self.header)\n\n            for row in self.data:\n                writer.writerow(row)\n\n        if isinstance(csvfile, (str, Path)):\n            with open(csvfile, mode=\"wt\", encoding=encoding) as f:\n                write(f)\n        else:\n            write(csvfile)\n\n    def unique_header(self):\n        \"\"\"Return the header with brackets appended to duplicated labels\n        to make them unique.\n\n        For example, the header\n\n            [\"@id\", \"@type\", \"@type\", \"part.name\", \"part.name\"]\n\n        \"distribution.downloadURL\",\n\n        would be renamed to\n\n            [\"@id\", \"@type[1]\", \"@type[2]\", \"part[1].name\", \"part[2].name\"]\n\n        \"\"\"\n        new = []\n        seen = {}\n        for h in self.header:\n            if self.header.count(h) == 1:\n                new.append(h)\n            else:\n                head, tail = h.split(\".\", 1) if \".\" in h else (h, None)\n                n = seen[head] + 1 if head in seen else 1\n                seen[head] = n\n                new.append(f\"{head}[{n}].{tail}\" if tail else f\"{head}[{n}]\")\n        return new\n</code></pre>"},{"location":"api_reference/datadoc/tabledoc/#tripper.datadoc.tabledoc.TableDoc.asdicts","title":"<code>asdicts(self)</code>","text":"<p>Return the table as a list of dicts.</p> Source code in <code>tripper/datadoc/tabledoc.py</code> <pre><code>def asdicts(self) -&gt; \"List[dict]\":\n    \"\"\"Return the table as a list of dicts.\"\"\"\n    results = []\n    for row in self.data:\n        d = AttrDict()\n        for i, colname in enumerate(self.header):\n            cell = row[i].strip() if row[i] and self.strip else row[i]\n            if cell:\n\n                # Convert cell value to correct Python type\n                if not colname.startswith(\"@\"):\n                    leafname = colname.split(\".\")[-1]\n                    df = self.context.getdef(leafname.split(\"[\")[0])\n                    if \"@type\" in df and df[\"@type\"] != \"@id\":\n                        cell = Literal(cell, datatype=df[\"@type\"]).value\n\n                addnested(\n                    d, colname.strip() if self.strip else colname, cell\n                )\n        results.append(stripnested(d))\n    ld = told(\n        results,\n        type=self.type,\n        prefixes=self.context.get_prefixes(),\n        context=self.context,\n    )\n    dicts = ld[\"@graph\"]\n    return dicts\n</code></pre>"},{"location":"api_reference/datadoc/tabledoc/#tripper.datadoc.tabledoc.TableDoc.fromdicts","title":"<code>fromdicts(dicts, type=None, keywords=None, context=None, prefixes=None, strip=True)</code>  <code>staticmethod</code>","text":"<p>Create new TableDoc instance from a sequence of dicts.</p> <p>Parameters:</p> Name Type Description Default <code>dicts</code> <code>Sequence[dict]</code> <p>Sequence of single-resource dicts.</p> required <code>type</code> <code>Optional[str]</code> <p>Type of data to save (applies to all rows).  Should either be one of the pre-defined names: \"Dataset\", \"Distribution\", \"AccessService\", \"Parser\" and \"Generator\" or an IRI to a class in an ontology.</p> <code>None</code> <code>keywords</code> <code>Optional[KeywordsType]</code> <p>Keywords object with additional keywords definitions. If not provided, only default keywords are considered.</p> <code>None</code> <code>context</code> <code>Optional[ContextType]</code> <p>Additional user-defined context that should be returned on top of the default context.  It may be a string with an URL to the user-defined context, a dict with the user-defined context or a sequence of strings and dicts.</p> <code>None</code> <code>prefixes</code> <code>Optional[dict]</code> <p>Dict with prefixes in addition to those included in the JSON-LD context.  Should map namespace prefixes to IRIs.</p> <code>None</code> <code>strip</code> <code>bool</code> <p>Whether to strip leading and trailing whitespaces from cells.</p> <code>True</code> <p>Returns:</p> Type Description <code>TableDoc</code> <p>New TableDoc instance.</p> Source code in <code>tripper/datadoc/tabledoc.py</code> <pre><code>@staticmethod\ndef fromdicts(\n    dicts: \"Sequence[dict]\",\n    type: \"Optional[str]\" = None,\n    keywords: \"Optional[KeywordsType]\" = None,\n    context: \"Optional[ContextType]\" = None,\n    prefixes: \"Optional[dict]\" = None,\n    strip: bool = True,\n) -&gt; \"TableDoc\":\n    \"\"\"Create new TableDoc instance from a sequence of dicts.\n\n    Arguments:\n        dicts: Sequence of single-resource dicts.\n        type: Type of data to save (applies to all rows).  Should\n            either be one of the pre-defined names: \"Dataset\",\n            \"Distribution\", \"AccessService\", \"Parser\" and\n            \"Generator\" or an IRI to a class in an ontology.\n        keywords: Keywords object with additional keywords definitions.\n            If not provided, only default keywords are considered.\n        context: Additional user-defined context that should be\n            returned on top of the default context.  It may be a\n            string with an URL to the user-defined context, a dict\n            with the user-defined context or a sequence of strings\n            and dicts.\n        prefixes: Dict with prefixes in addition to those included\n            in the JSON-LD context.  Should map namespace prefixes\n            to IRIs.\n        strip: Whether to strip leading and trailing whitespaces\n            from cells.\n\n    Returns:\n        New TableDoc instance.\n\n    \"\"\"\n    # Store the header as keys in a dict to keep ordering\n    headdict = {\"@id\": True}\n\n    def addheaddict(d, prefix=\"\"):\n        \"\"\"Add keys in `d` to headdict.\n\n        Nested dicts will result in dot-separated keys.\n        \"\"\"\n        for k, v in d.items():\n            if k == \"@context\":\n                pass\n            elif isinstance(v, dict):\n                d = v.copy()\n                d.pop(\"@type\", None)\n                addheaddict(d, k + \".\")\n            else:\n                headdict[prefix + k] = True\n\n    def tolist(v, mult, pad=None):\n        \"\"\"Return `v` as a list of length `mult` with given padding.\"\"\"\n        if isinstance(v, list):\n            return v + [pad] * (mult - len(v))\n        return [v] + [pad] * (mult - 1)\n\n    # Assign the headdict\n    for d in dicts:\n        addheaddict(d)\n\n    header = list(headdict)\n\n    # Calculate multiplicity of each header label\n    mult = [1] * len(header)\n    for dct in dicts:\n        for i, head in enumerate(header):\n            if head in dct and isinstance(dct[head], list):\n                mult[i] = max(mult[i], len(dct[head]))\n\n    # Assign table data. Multiplicity and nested dicts are accounted for\n    data = []\n    for dct in dicts:\n        row = []\n        for head, m in zip(header, mult):\n            if head in dct:\n                row.extend(tolist(dct[head], m))\n            else:\n                d = dct\n                for key in head.split(\".\"):\n                    d = d.get(key, {})\n                row.extend(tolist(d if d != {} else None, m))\n        data.append(row)\n\n    # New multiplied header\n    newheader = []\n    for head, m in zip(header, mult):\n        newheader.extend(tolist(head, m, head))\n\n    return TableDoc(\n        header=newheader,\n        data=data,\n        type=type,\n        keywords=keywords,\n        context=context,\n        prefixes=prefixes,\n        strip=strip,\n    )\n</code></pre>"},{"location":"api_reference/datadoc/tabledoc/#tripper.datadoc.tabledoc.TableDoc.parse_csv","title":"<code>parse_csv(csvfile, type=None, keywords=None, context=None, prefixes=None, encoding='utf-8', dialect=None, strict=False, redefine='raise', **kwargs)</code>  <code>staticmethod</code>","text":"<p>Parse a csv file using the standard library csv module.</p> <p>Parameters:</p> Name Type Description Default <code>csvfile</code> <code>Union[Iterable[str], Path, str]</code> <p>Name of CSV file to parse or an iterable of strings.</p> required <code>type</code> <code>Optional[str]</code> <p>Type of data to save (applies to all rows).  Should either be one of the pre-defined names: \"Dataset\", \"Distribution\", \"AccessService\", \"Parser\" and \"Generator\" or an IRI to a class in an ontology.</p> <code>None</code> <code>keywords</code> <code>Optional[KeywordsType]</code> <p>Keywords object with additional keywords definitions. If not provided, only default keywords are considered.</p> <code>None</code> <code>context</code> <code>Optional[ContextType]</code> <p>Dict with user-defined JSON-LD context.</p> <code>None</code> <code>prefixes</code> <code>Optional[dict]</code> <p>Dict with prefixes in addition to those included in the JSON-LD context.  Should map namespace prefixes to IRIs.</p> <code>None</code> <code>encoding</code> <code>str</code> <p>The encoding of the csv file.  Note that Excel may encode as \"ISO-8859\" (which was commonly used in the 1990th).</p> <code>'utf-8'</code> <code>dialect</code> <code>Optional[Union[csv.Dialect, str]]</code> <p>A subclass of csv.Dialect, or the name of the dialect, specifying how the <code>csvfile</code> is formatted.  For more details, see [Dialects and Formatting Parameters].</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to raise an <code>InvalidKeywordError</code> exception if <code>d</code> contains an unknown key.</p> <code>False</code> <code>redefine</code> <code>str</code> <p>Determine how to handle redefinition of existing keywords.  Should be one of the following strings:   - \"allow\": Allow redefining a keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"skip\": Don't redefine existing keyword. Emits a     <code>RedefineKeywordWarning</code>.   - \"raise\": Raise an RedefineError (default).</p> <code>'raise'</code> <code>kwargs</code> <p>Additional keyword arguments overriding individual formatting parameters.  For more details, see [Dialects and Formatting Parameters].</p> <code>{}</code> <p>Returns:</p> Type Description <code>TableDoc</code> <p>New TableDoc instance.</p> <p>References:</p> Source code in <code>tripper/datadoc/tabledoc.py</code> <pre><code>@staticmethod\ndef parse_csv(\n    csvfile: \"Union[Iterable[str], Path, str]\",\n    type: \"Optional[str]\" = None,\n    keywords: \"Optional[KeywordsType]\" = None,\n    context: \"Optional[ContextType]\" = None,\n    prefixes: \"Optional[dict]\" = None,\n    encoding: str = \"utf-8\",\n    dialect: \"Optional[Union[csv.Dialect, str]]\" = None,\n    strict: bool = False,\n    redefine: str = \"raise\",\n    **kwargs,\n) -&gt; \"TableDoc\":\n    # pylint: disable=line-too-long\n    \"\"\"Parse a csv file using the standard library csv module.\n\n    Arguments:\n        csvfile: Name of CSV file to parse or an iterable of strings.\n        type: Type of data to save (applies to all rows).  Should\n            either be one of the pre-defined names: \"Dataset\",\n            \"Distribution\", \"AccessService\", \"Parser\" and \"Generator\"\n            or an IRI to a class in an ontology.\n        keywords: Keywords object with additional keywords definitions.\n            If not provided, only default keywords are considered.\n        context: Dict with user-defined JSON-LD context.\n        prefixes: Dict with prefixes in addition to those included in the\n            JSON-LD context.  Should map namespace prefixes to IRIs.\n        encoding: The encoding of the csv file.  Note that Excel may\n            encode as \"ISO-8859\" (which was commonly used in the 1990th).\n        dialect: A subclass of csv.Dialect, or the name of the dialect,\n            specifying how the `csvfile` is formatted.  For more details,\n            see [Dialects and Formatting Parameters].\n        strict: Whether to raise an `InvalidKeywordError` exception if `d`\n            contains an unknown key.\n        redefine: Determine how to handle redefinition of existing\n            keywords.  Should be one of the following strings:\n              - \"allow\": Allow redefining a keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"skip\": Don't redefine existing keyword. Emits a\n                `RedefineKeywordWarning`.\n              - \"raise\": Raise an RedefineError (default).\n        kwargs: Additional keyword arguments overriding individual\n            formatting parameters.  For more details, see\n            [Dialects and Formatting Parameters].\n\n    Returns:\n        New TableDoc instance.\n\n    References:\n    [Dialects and Formatting Parameters]: https://docs.python.org/3/library/csv.html#dialects-and-formatting-parameters\n    \"\"\"\n\n    def read(f, dialect):\n        \"\"\"Return csv reader from file-like object `f`.\"\"\"\n        if dialect is None and not kwargs:\n            sample = f.read(1024)\n            try:\n                dialect = csv.Sniffer().sniff(sample, delimiters=\",;\\t \")\n            except csv.Error:\n                # The build-in sniffer not always work well with\n                # non-numerical csv files. Try our simple sniffer\n                dialect = csvsniff(sample)\n            finally:\n                f.seek(0)\n        reader = csv.reader(f, dialect=dialect, **kwargs)\n        header = next(reader)\n        data = list(reader)\n        return header, data\n\n    if isinstance(csvfile, (str, Path)):\n        with openfile(csvfile, mode=\"rt\", encoding=encoding) as f:\n            header, data = read(f, dialect)\n    else:\n        header, data = read(csvfile, dialect)\n\n    return TableDoc(\n        header=header,\n        data=data,\n        type=type,\n        keywords=keywords,\n        context=context,\n        prefixes=prefixes,\n        strict=strict,\n        redefine=redefine,\n    )\n</code></pre>"},{"location":"api_reference/datadoc/tabledoc/#tripper.datadoc.tabledoc.TableDoc.save","title":"<code>save(self, ts)</code>","text":"<p>Save tabular datadocumentation to triplestore.</p> Source code in <code>tripper/datadoc/tabledoc.py</code> <pre><code>def save(self, ts: Triplestore) -&gt; None:\n    \"\"\"Save tabular datadocumentation to triplestore.\"\"\"\n    self.context.add_context(\n        {prefix: str(ns) for prefix, ns in ts.namespaces.items()}\n    )\n\n    for prefix, ns in self.context.get_prefixes().items():\n        ts.bind(prefix, ns)\n\n    store(ts, self.asdicts(), type=self.type, context=self.context)\n</code></pre>"},{"location":"api_reference/datadoc/tabledoc/#tripper.datadoc.tabledoc.TableDoc.unique_header","title":"<code>unique_header(self)</code>","text":"<p>Return the header with brackets appended to duplicated labels to make them unique.</p> <p>For example, the header</p> <pre><code>[\"@id\", \"@type\", \"@type\", \"part.name\", \"part.name\"]\n</code></pre> <p>\"distribution.downloadURL\",</p> <p>would be renamed to</p> <pre><code>[\"@id\", \"@type[1]\", \"@type[2]\", \"part[1].name\", \"part[2].name\"]\n</code></pre> Source code in <code>tripper/datadoc/tabledoc.py</code> <pre><code>def unique_header(self):\n    \"\"\"Return the header with brackets appended to duplicated labels\n    to make them unique.\n\n    For example, the header\n\n        [\"@id\", \"@type\", \"@type\", \"part.name\", \"part.name\"]\n\n    \"distribution.downloadURL\",\n\n    would be renamed to\n\n        [\"@id\", \"@type[1]\", \"@type[2]\", \"part[1].name\", \"part[2].name\"]\n\n    \"\"\"\n    new = []\n    seen = {}\n    for h in self.header:\n        if self.header.count(h) == 1:\n            new.append(h)\n        else:\n            head, tail = h.split(\".\", 1) if \".\" in h else (h, None)\n            n = seen[head] + 1 if head in seen else 1\n            seen[head] = n\n            new.append(f\"{head}[{n}].{tail}\" if tail else f\"{head}[{n}]\")\n    return new\n</code></pre>"},{"location":"api_reference/datadoc/tabledoc/#tripper.datadoc.tabledoc.TableDoc.write_csv","title":"<code>write_csv(self, csvfile, encoding='utf-8', dialect='excel', prefixes=None, **kwargs)</code>","text":"<p>Write the table to a csv file using the standard library csv module.</p> <p>Parameters:</p> Name Type Description Default <code>csvfile</code> <code>Union[Path, str, Writer]</code> <p>File-like object or name of CSV file to write.</p> required <code>encoding</code> <code>str</code> <p>The encoding of the csv file.</p> <code>'utf-8'</code> <code>dialect</code> <code>Union[csv.Dialect, str]</code> <p>A subclass of csv.Dialect, or the name of the dialect, specifying how the <code>csvfile</code> is formatted.  For more details, see [Dialects and Formatting Parameters].</p> <code>'excel'</code> <code>prefixes</code> <code>Optional[dict]</code> <p>Prefixes used to compact the header.</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments overriding individual formatting parameters.  For more details, see [Dialects and Formatting Parameters].</p> <code>{}</code> <p>References:</p> Source code in <code>tripper/datadoc/tabledoc.py</code> <pre><code>def write_csv(\n    self,\n    csvfile: \"Union[Path, str, Writer]\",\n    encoding: str = \"utf-8\",\n    dialect: \"Union[csv.Dialect, str]\" = \"excel\",\n    prefixes: \"Optional[dict]\" = None,\n    **kwargs,\n) -&gt; None:\n    # pylint: disable=line-too-long\n    \"\"\"Write the table to a csv file using the standard library csv module.\n\n    Arguments:\n        csvfile: File-like object or name of CSV file to write.\n        encoding: The encoding of the csv file.\n        dialect: A subclass of csv.Dialect, or the name of the dialect,\n            specifying how the `csvfile` is formatted.  For more details,\n            see [Dialects and Formatting Parameters].\n        prefixes: Prefixes used to compact the header.\n        kwargs: Additional keyword arguments overriding individual\n            formatting parameters.  For more details, see\n            [Dialects and Formatting Parameters].\n\n    References:\n    [Dialects and Formatting Parameters]: https://docs.python.org/3/library/csv.html#dialects-and-formatting-parameters\n    \"\"\"\n\n    def write(f):\n        writer = csv.writer(f, dialect=dialect, **kwargs)\n\n        # TODO: use self.context and compact to shortnames\n        if prefixes:\n            header = []\n            for h in self.header:\n                for prefix, ns in prefixes.items():\n                    if h.startswith(str(ns)):\n                        header.append(f\"{prefix}:{h[len(str(ns)):]}\")\n                        break\n                else:\n                    header.append(h)\n            writer.writerow(header)\n        else:\n            writer.writerow(self.header)\n\n        for row in self.data:\n            writer.writerow(row)\n\n    if isinstance(csvfile, (str, Path)):\n        with open(csvfile, mode=\"wt\", encoding=encoding) as f:\n            write(f)\n    else:\n        write(csvfile)\n</code></pre>"},{"location":"api_reference/datadoc/tabledoc/#tripper.datadoc.tabledoc.csvsniff","title":"<code>csvsniff(sample)</code>","text":"<p>Custom csv sniffer.</p> <p>Analyse csv sample and returns a csv.Dialect instance.</p> Source code in <code>tripper/datadoc/tabledoc.py</code> <pre><code>def csvsniff(sample):\n    \"\"\"Custom csv sniffer.\n\n    Analyse csv sample and returns a csv.Dialect instance.\n    \"\"\"\n    # Determine line terminator\n    if \"\\r\\n\" in sample:\n        linesep = \"\\r\\n\"\n    else:\n        counts = {s: sample.count(s) for s in \"\\n\\r\"}\n        linesep = max(counts, key=lambda k: counts[k])\n\n    lines = sample.split(linesep)\n    del lines[-1]  # skip last line since it might be truncated\n    if not lines:\n        raise csv.Error(\n            \"too long csv header. No line terminator within sample\"\n        )\n    header = lines[0]\n\n    # Possible delimiters and quote chars to check\n    delims = [d for d in \",;\\t :\" if header.count(d)]\n    quotes = [q for q in \"\\\"'\" if sample.count(q)]\n    if not quotes:\n        quotes = ['\"']\n\n    # For each (quote, delim)-pair, count the number of tokens per line\n    # Only pairs for which all lines has the same number of tokens are added\n    # to ntokens\n    ntokens = {}  # map (quote, delim) to number of tokens per line\n    for q in quotes:\n        for d in delims:\n            ntok = []\n            for ln in lines:\n                # Remove quoted tokens\n                ln = re.sub(f\"(^{q}[^{q}]*{q}{d})|({d}{q}[^{q}]*{q}$)\", d, ln)\n                ln = re.sub(f\"{d}{q}[^{q}]*{q}{d}\", d * 2, ln)\n                ntok.append(len(ln.split(d)))\n\n            if ntok and max(ntok) == min(ntok):\n                ntokens[(q, d)] = ntok[0]\n\n    # From ntokens, select (quote, delim) pair that results in the highest\n    # number of tokens per line\n    if not ntokens:\n        raise csv.Error(\"not able to determine delimiter\")\n    quote, delim = max(ntokens, key=lambda k: ntokens[k])\n\n    class dialect(csv.Dialect):\n        \"\"\"Custom dialect.\"\"\"\n\n        # pylint: disable=too-few-public-methods\n        _name = \"sniffed\"\n        delimiter = delim\n        doublequote = True  # quote chars inside quotes are duplicated\n        # escapechar = \"\\\\\"  # unused\n        lineterminator = linesep\n        quotechar = quote\n        quoting = csv.QUOTE_MINIMAL\n        skipinitialspace = False  # don't ignore spaces before a delimiter\n        strict = False  # be permissive on malformed csv input\n\n    return dialect\n</code></pre>"},{"location":"api_reference/datadoc/utils/","title":"utils","text":"<p>Utilities for manipulating dicts and lists.</p>"},{"location":"api_reference/datadoc/utils/#tripper.datadoc.utils.add","title":"<code>add(d, key, value)</code>","text":"<p>Append key-value pair to dict <code>d</code>.</p> <p>If <code>key</code> already exists in <code>d</code>, its value is converted to a list and <code>value</code> is appended to it.  <code>value</code> may also be a list. Values are not duplicated.</p> Source code in <code>tripper/datadoc/utils.py</code> <pre><code>def add(d: dict, key: str, value: \"Any\") -&gt; None:\n    \"\"\"Append key-value pair to dict `d`.\n\n    If `key` already exists in `d`, its value is converted to a list\n    and `value` is appended to it.  `value` may also be a list. Values\n    are not duplicated.\n\n    \"\"\"\n    if key not in d:\n        d[key] = value\n    else:\n        klst = d[key] if isinstance(d[key], list) else [d[key]]\n        if isinstance(value, dict):\n            v = klst if value in klst else klst + [value]\n        else:\n            vlst = value if isinstance(value, list) else [value]\n            try:\n                v = list(set(klst).union(vlst))\n            except TypeError:  # klst contains unhashable dicts\n                v = klst + [x for x in vlst if x not in klst]\n        d[key] = (\n            v[0]\n            if len(v) == 1\n            else sorted(\n                # Sort dicts at end, by representing them with a huge\n                # unicode character\n                v,\n                key=lambda x: \"\\uffff\" if isinstance(x, dict) else str(x),\n            )\n        )\n</code></pre>"},{"location":"api_reference/datadoc/utils/#tripper.datadoc.utils.addnested","title":"<code>addnested(d, key, value, cls=None)</code>","text":"<p>Like add(), but allows <code>key</code> to be a dot-separated list of sub-keys. Returns the updated <code>d</code>.</p> <p>Each sub-key will be added to <code>d</code> as a corresponding sub-dict.</p> <p>Subdicts will be of type <code>cls</code>. If <code>cls</code> is None, subdicts will default to the same type as <code>d</code> if <code>d</code> is a mapping, or to a dict otherwise.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; d = {}\n&gt;&gt;&gt; addnested(d, \"a.b.c\", \"val\")\n{'a': {'b': {'c': 'val'}}}\n</code></pre> Source code in <code>tripper/datadoc/utils.py</code> <pre><code>def addnested(\n    d: \"Union[dict, list]\",\n    key: str,\n    value: \"Any\",\n    cls: \"Optional[type]\" = None,\n) -&gt; \"Union[dict, list]\":\n    \"\"\"Like add(), but allows `key` to be a dot-separated list of sub-keys.\n    Returns the updated `d`.\n\n    Each sub-key will be added to `d` as a corresponding sub-dict.\n\n    Subdicts will be of type `cls`. If `cls` is None, subdicts will default\n    to the same type as `d` if `d` is a mapping, or to a dict otherwise.\n\n    Example:\n\n        &gt;&gt;&gt; d = {}\n        &gt;&gt;&gt; addnested(d, \"a.b.c\", \"val\")\n        {'a': {'b': {'c': 'val'}}}\n\n    \"\"\"\n    # pylint: disable=too-many-branches\n    if cls is None:\n        cls = type(d) if isinstance(d, Mapping) else dict\n\n    if \".\" in key:\n        first, rest = key.split(\".\", 1)\n        if isinstance(d, list):\n            for ele in d:\n                if isinstance(ele, dict):\n                    addnested(ele, key, value)\n                    break\n            else:\n                d.append(addnested(cls(), key, value))\n        elif first in d and isinstance(d[first], (dict, list)):\n            addnested(d[first], rest, value)\n        else:\n            addnested(d, first, addnested(cls(), rest, value))\n    elif isinstance(d, list):\n        for ele in d:\n            if isinstance(ele, dict):\n                add(ele, key, value)\n                break\n        else:\n            d.append({key: value})\n    else:\n        add(d, key, value)\n    return d\n</code></pre>"},{"location":"api_reference/datadoc/utils/#tripper.datadoc.utils.asseq","title":"<code>asseq(value)</code>","text":"<p>Returns a string or sequence as an iterable.</p> Source code in <code>tripper/datadoc/utils.py</code> <pre><code>def asseq(value: \"Union[str, Sequence]\") -&gt; \"Sequence\":\n    \"\"\"Returns a string or sequence as an iterable.\"\"\"\n    return [value] if isinstance(value, str) else value\n</code></pre>"},{"location":"api_reference/datadoc/utils/#tripper.datadoc.utils.get","title":"<code>get(d, key, default=None, aslist=True)</code>","text":"<p>Like <code>d.get(key, default)</code> but returns the value as a list if <code>aslist</code> is True and value is not already a list.</p> <p>An empty list is returned in the special case that <code>key</code> is not in <code>d</code> and <code>default</code> is None.</p> Source code in <code>tripper/datadoc/utils.py</code> <pre><code>def get(\n    d: dict, key: str, default: \"Any\" = None, aslist: bool = True\n) -&gt; \"Any\":\n    \"\"\"Like `d.get(key, default)` but returns the value as a list if\n    `aslist` is True and value is not already a list.\n\n    An empty list is returned in the special case that `key` is not in\n    `d` and `default` is None.\n\n    \"\"\"\n    value = d.get(key, default)\n    if aslist:\n        return (\n            value\n            if isinstance(value, list)\n            else [] if value is None else [value]\n        )\n    return value\n</code></pre>"},{"location":"api_reference/datadoc/utils/#tripper.datadoc.utils.iriname","title":"<code>iriname(value)</code>","text":"<p>Return the name part of an IRI or CURIE. If value has no \":\", it is returned as-is.</p> Source code in <code>tripper/datadoc/utils.py</code> <pre><code>def iriname(value: str) -&gt; str:\n    \"\"\"Return the name part of an IRI or CURIE.\n    If value has no \":\", it is returned as-is.\n    \"\"\"\n    if \":\" not in value:\n        return value\n    m = re.search(\"[:/#]([a-zA-Z_][a-zA-Z0-9_.+-]*)$\", value)\n    if not m or not m.groups():\n        raise ValueError(f\"Cannot infer name of IRI: {value}\")\n    return m.groups()[0]\n</code></pre>"},{"location":"api_reference/datadoc/utils/#tripper.datadoc.utils.merge","title":"<code>merge(a, b)</code>","text":"<p>Return the merged result of <code>a</code> and <code>b</code>, where <code>a</code> and <code>b</code> can be None, string or a sequence of strings.</p> <p>The result will be None if both <code>a</code> and <code>b</code> are None and a string if one is None and the other is a string or both are the same string.  Otherwise, the result will be a list with the unique strings from <code>a</code> and <code>b</code>.</p> <p>Examples:</p> <p>merge(None, None)</p> <p>merge(\"a\", None) 'a'</p> <p>merge(None, \"b\") 'b'</p> <p>merge(\"a\", \"b\") ['a', 'b']</p> <p>merge(\"a\", [\"c\", \"b\", \"a\"]) ['a', 'c', 'b']</p> <p>merge([\"a\", \"d\"], [\"c\", \"b\", \"a\"]) ['a', 'd', 'c', 'b']</p> Source code in <code>tripper/datadoc/utils.py</code> <pre><code>def merge(a: \"MergeType\", b: \"MergeType\") -&gt; \"MergeType\":\n    \"\"\"Return the merged result of `a` and `b`, where `a` and `b` can be\n    None, string or a sequence of strings.\n\n    The result will be None if both `a` and `b` are None and a string if one\n    is None and the other is a string or both are the same string.  Otherwise,\n    the result will be a list with the unique strings from `a` and `b`.\n\n    Examples:\n    &gt;&gt;&gt; merge(None, None)\n\n    &gt;&gt;&gt; merge(\"a\", None)\n    'a'\n\n    &gt;&gt;&gt; merge(None, \"b\")\n    'b'\n\n    &gt;&gt;&gt; merge(\"a\", \"b\")\n    ['a', 'b']\n\n    &gt;&gt;&gt; merge(\"a\", [\"c\", \"b\", \"a\"])\n    ['a', 'c', 'b']\n\n    &gt;&gt;&gt; merge([\"a\", \"d\"], [\"c\", \"b\", \"a\"])\n    ['a', 'd', 'c', 'b']\n\n    \"\"\"\n    # pylint: disable=too-many-return-statements\n    if a is None and b is None:\n        return None\n    if a is None:\n        return b\n    if b is None:\n        return a\n    if isinstance(a, str) and isinstance(b, str):\n        return a if b == a else [a, b]\n    if isinstance(a, str) and isinstance(b, Sequence):\n        return [a] + [x for x in b if x != a]\n    if isinstance(a, Sequence) and isinstance(b, str):\n        return a if b in a else list(a) + [b]\n    if isinstance(a, Sequence) and isinstance(b, Sequence):\n        return list(a) + [x for x in b if x not in a]\n    raise TypeError(\"input must be None, string or a sequence\")\n</code></pre>"},{"location":"api_reference/datadoc/utils/#tripper.datadoc.utils.stripnested","title":"<code>stripnested(d)</code>","text":"<p>Strip off brackets from keys in nested dicts.</p> <p>This function is intended for post-processing the result of a series of calls to addnested().</p> <p>Examples:</p> <p>d = {\"a[1]\": {\"x\": 1, \"y\": 2}, \"a[2]\": {\"x\": 3}} stripnested(d) {'a': [{'x': 1, 'y': 2}, {'x': 3}]}</p> Source code in <code>tripper/datadoc/utils.py</code> <pre><code>def stripnested(d):\n    \"\"\"Strip off brackets from keys in nested dicts.\n\n    This function is intended for post-processing the result of a\n    series of calls to addnested().\n\n    Example:\n\n    &gt;&gt;&gt; d = {\"a[1]\": {\"x\": 1, \"y\": 2}, \"a[2]\": {\"x\": 3}}\n    &gt;&gt;&gt; stripnested(d)\n    {'a': [{'x': 1, 'y': 2}, {'x': 3}]}\n\n    \"\"\"\n    if isinstance(d, list):\n        new = type(d)()\n        for e in d:\n            new.append(stripnested(e))\n    elif isinstance(d, dict):\n        new = type(d)()\n        for k, v in d.items():\n            add(new, k.split(\"[\")[0], v)\n    else:\n        new = d\n    return new\n</code></pre>"},{"location":"api_reference/mappings/mappings/","title":"mappings","text":"<p>Implements mappings between entities.</p> <p>Units are currently handled with pint.Quantity.  The benefit of this compared to explicit unit conversions, is that units will be handled transparently by mapping functions, without any need to specify units of input and output parameters.</p> <p>Shapes are automatically handled by expressing non-scalar quantities with numpy.</p>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.AmbiguousMappingError","title":"<code> AmbiguousMappingError            (MappingError)         </code>","text":"<p>A property maps to more than one value.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>class AmbiguousMappingError(MappingError):\n    \"\"\"A property maps to more than one value.\"\"\"\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.InconsistentDimensionError","title":"<code> InconsistentDimensionError            (MappingError)         </code>","text":"<p>The size of a dimension is assigned to more than one value.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>class InconsistentDimensionError(MappingError):\n    \"\"\"The size of a dimension is assigned to more than one value.\"\"\"\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.InconsistentTriplesError","title":"<code> InconsistentTriplesError            (MappingError)         </code>","text":"<p>Inconsistcy in RDF triples.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>class InconsistentTriplesError(MappingError):\n    \"\"\"Inconsistcy in RDF triples.\"\"\"\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.InsufficientMappingError","title":"<code> InsufficientMappingError            (MappingError)         </code>","text":"<p>There are properties or dimensions that are not mapped.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>class InsufficientMappingError(MappingError):\n    \"\"\"There are properties or dimensions that are not mapped.\"\"\"\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingError","title":"<code> MappingError            (Exception)         </code>","text":"<p>Base class for mapping errors.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>class MappingError(Exception):\n    \"\"\"Base class for mapping errors.\"\"\"\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep","title":"<code> MappingStep        </code>","text":"<p>A step in a mapping route from a target to one or more sources.</p> <p>A mapping step corresponds to one or more RDF triples.  In the simple case of a <code>mo:mapsTo</code> or <code>rdfs:isSubclassOf</code> relation, it is only one triple.  For transformations that has several input and output, a set of triples are expected.</p> <p>Parameters:</p> Name Type Description Default <code>output_iri</code> <code>str</code> <p>IRI of the output concept.</p> required <code>steptype</code> <code>'StepType'</code> <p>One of the step types from the StepType enum.</p> <code>&lt;StepType.UNSPECIFIED: 0&gt;</code> <code>function</code> <code>'Optional[Callable]'</code> <p>Callable that evaluates the output from the input.</p> <code>None</code> <code>cost</code> <code>'Union[float, Callable]'</code> <p>The cost related to this mapping step.  Should be either a float or a callable taking three arguments (<code>triplestore</code>, <code>input_iris</code> and <code>output_iri</code>) and return the cost as a float.</p> <code>1.0</code> <code>output_unit</code> <code>'Optional[str]'</code> <p>Output unit.</p> <code>None</code> <code>triplestore</code> <code>'Optional[Triplestore]'</code> <p>Triplestore instance containing the knowledge base that this mapping step was created from.</p> <code>None</code> <p>The arguments can also be assigned as attributes.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>class MappingStep:\n    \"\"\"A step in a mapping route from a target to one or more sources.\n\n    A mapping step corresponds to one or more RDF triples.  In the\n    simple case of a `mo:mapsTo` or `rdfs:isSubclassOf` relation, it is\n    only one triple.  For transformations that has several input and\n    output, a set of triples are expected.\n\n    Arguments:\n        output_iri: IRI of the output concept.\n        steptype: One of the step types from the StepType enum.\n        function: Callable that evaluates the output from the input.\n        cost: The cost related to this mapping step.  Should be either a\n            float or a callable taking three arguments (`triplestore`,\n            `input_iris` and `output_iri`) and return the cost as a float.\n        output_unit: Output unit.\n        triplestore: Triplestore instance containing the knowledge base\n            that this mapping step was created from.\n\n    The arguments can also be assigned as attributes.\n    \"\"\"\n\n    # pylint: disable=too-many-instance-attributes\n\n    def __init__(\n        self,\n        output_iri: str,\n        steptype: \"StepType\" = StepType.UNSPECIFIED,\n        function: \"Optional[Callable]\" = None,\n        cost: \"Union[float, Callable]\" = 1.0,\n        output_unit: \"Optional[str]\" = None,\n        triplestore: \"Optional[Triplestore]\" = None,\n    ) -&gt; None:\n        self.output_iri = output_iri\n        self.steptype = steptype\n        self.function = function\n        self.cost = cost\n        self.triplestore = triplestore\n        self.output_unit = output_unit\n        self.input_routes: \"List[dict]\" = []  # list of inputs dicts\n        self.join_mode = False  # whether to join upcoming input\n        self.joined_input: \"Inputs\" = {}\n\n    def add_inputs(self, inputs: \"Inputs\") -&gt; None:\n        \"\"\"Add input Mapping (e.g., dict) for an input route.\"\"\"\n        assert isinstance(inputs, Mapping)  # nosec B101\n        self.input_routes.append(inputs)\n\n    def add_input(self, input: \"Input\", name: \"Optional[str]\" = None) -&gt; None:\n        \"\"\"Add an input (MappingStep or Value), where `name` is the name\n        assigned to the argument.\n\n        If the `join_mode` attribute is false, a new route is created with\n        only one input.\n\n        If the `join_mode` attribute is true, the input is remembered, but\n        first added when `join_input()` is called.\n\n        Arguments:\n            input: A mapping step or a value.\n            name: Name assigned to the argument.\n        \"\"\"\n        assert isinstance(input, (MappingStep, Value))  # nosec B101\n        argname = name if name else f\"arg{len(self.joined_input)+1}\"\n        if self.join_mode:\n            self.joined_input[argname] = input\n        else:\n            self.add_inputs({argname: input})\n\n    def join_input(self) -&gt; None:\n        \"\"\"Join all input added with add_input() since `join_mode` was set\n        true.  Resets `join_mode` to false.\"\"\"\n        if not self.join_mode:\n            raise MappingError(\"Calling join_input() when join_mode is false.\")\n        self.join_mode = False\n        self.add_inputs(self.joined_input)\n        self.joined_input = {}\n\n    def eval(\n        self,\n        routeno: \"Optional[int]\" = None,\n        unit: \"Optional[str]\" = None,\n        magnitude: bool = False,\n        quantity: \"Optional[Type[Quantity]]\" = None,\n    ) -&gt; \"Any\":\n        \"\"\"Returns the evaluated value of given input route number.\n\n        Arguments:\n            routeno: The route number to evaluate.  If None (default)\n                the route with the lowest cost is evalueated.\n            unit: return the result in the given unit.\n                Implies `magnitude=True`.\n            magnitude: Whether to only return the magnitude of the evaluated\n                value (with no unit).\n            quantity: Quantity class to use for evaluation.  Defaults to pint.\n\n        Returns:\n            Evaluation result.\n        \"\"\"\n        if not self.number_of_routes():\n            raise MissingRelationError(\n                f\"no route to evaluate '{self.output_iri}'\"\n            )\n        if quantity is None:\n            quantity = Quantity\n        if routeno is None:\n            ((_, routeno),) = self.lowest_costs(nresults=1)\n        inputs, idx = self.get_inputs(routeno)\n        values = get_values(inputs, idx, quantity=quantity)\n\n        if self.function:\n            value = self.function(**values)\n        elif len(values) == 1:\n            (value,) = values.values()\n        else:\n            raise TypeError(\n                f\"Expected inputs to be a single argument: {values}\"\n            )\n\n        if isinstance(value, Quantity) and unit:\n            return value.m_as(unit)\n        if isinstance(value, Quantity) and magnitude:\n            return value.m\n        if isinstance(value, Value):\n            return value.get_value(\n                unit=unit, magnitude=magnitude, quantity=quantity\n            )\n        return value\n\n    def get_inputs(self, routeno: int) -&gt; \"Tuple[Inputs, int]\":\n        \"\"\"Returns input and input index `(inputs, idx)` for route number\n        `routeno`.\n\n        Arguments:\n            routeno: The route number to return inputs for.\n\n        Returns:\n            Inputs and difference between route number and number of routes for\n            an input dictioary.\n        \"\"\"\n        n = 0\n        for inputs in self.input_routes:\n            n0 = n\n            n += get_nroutes(inputs)\n            if n &gt; routeno:\n                return inputs, routeno - n0\n        raise ValueError(f\"routeno={routeno} exceeds number of routes\")\n\n    def get_input_iris(self, routeno: int) -&gt; \"Dict[str, Optional[str]]\":\n        \"\"\"Returns a dict mapping input names to iris for the given route\n        number.\n\n        Arguments:\n            routeno: The route number to return a mapping for.\n\n        Returns:\n            Mapping of input names to IRIs.\n\n        \"\"\"\n        inputs, _ = self.get_inputs(routeno)\n        return {\n            k: v.output_iri if isinstance(v, MappingStep) else v.output_iri\n            for k, v in inputs.items()\n        }\n\n    def number_of_routes(self) -&gt; int:\n        \"\"\"Total number of routes to this mapping step.\n\n        Returns:\n            Total number of routes to this mapping step.\n        \"\"\"\n        n = 0\n        for inputs in self.input_routes:\n            n += get_nroutes(inputs)\n        return n\n\n    def lowest_costs(self, nresults: int = 5) -&gt; \"List[Tuple[float, int]]\":\n        \"\"\"Returns a list of `(cost, routeno)` tuples with up to the `nresult`\n        lowest costs and their corresponding route numbers.\n\n        Arguments:\n            nresults: Number of results to return.\n\n        Returns:\n            A list of `(cost, routeno)` tuples.\n        \"\"\"\n        try:\n            import numpy as np  # pylint: disable=import-outside-toplevel\n        except ImportError as exc:\n            raise RuntimeError(\n                \"Mappings.lowest_costs() requires numpy.\\n\"\n                \"Install it with\\n\\n\"\n                \"    pip install numpy\"\n            ) from exc\n\n        result = []\n        n = 0  # total number of routes\n\n        # Loop over all toplevel routes leading into this mapping step\n        for inputs in self.input_routes:\n            # For each route, loop over all input arguments of this step\n            # The number of permutations we must consider is the product\n            # of the total number of routes to each input argument.\n            #\n            # We (potentially drastic) limit the possibilities by only\n            # considering the `nresults` routes with lowest costs into\n            # each argument.  This gives at maximum\n            #\n            #     nresults * number_of_input_arguments\n            #\n            # possibilities. We calculate the costs for all of them and\n            # store them in an array with two columns: `cost` and `routeno`.\n            # The `results` list is extended with the cost array\n            # for each toplevel route leading into this step.\n            base = np.rec.fromrecords(\n                [(0.0, 0)], names=\"cost,routeno\", formats=\"f8,i8\"\n            )\n            m = 1\n            for input in inputs.values():\n                if isinstance(input, MappingStep):\n                    nroutes = input.number_of_routes()\n                    res = np.rec.fromrecords(\n                        sorted(\n                            input.lowest_costs(nresults=nresults),\n                            key=lambda x: x[1],\n                        ),\n                        # [\n                        #     row\n                        #     for row in sorted(\n                        #         input.lowest_costs(nresults=nresults),\n                        #         key=lambda x: x[1],\n                        #     )\n                        # ],\n                        dtype=base.dtype,\n                    )\n                    res1 = res.repeat(len(base))\n                    base = np.tile(base, len(res))\n                    base.cost += res1.cost\n                    base.routeno += res1.routeno * m\n                    m *= nroutes\n                else:\n                    base.cost += input.cost\n\n            # Reduce the length of base (makes probably only sense in\n            # the case self.cost is a callable, but it doesn't hurt...)\n            base.sort()\n            base = base[:nresults]\n            base.routeno += n\n            n += m\n\n            # Add the cost for this step to `res`.  If `self.cost` is\n            # a callable, we call it with the input for each routeno\n            # as arguments.  Otherwise `self.cost` is the cost of this\n            # mapping step.\n            if callable(self.cost):\n                for i, rno in enumerate(base.routeno):\n                    inputs, _ = self.get_inputs(rno)\n                    input_iris = [\n                        input.output_iri for input in inputs.values()\n                    ]\n                    owncost = self.cost(\n                        self.triplestore, input_iris, self.output_iri\n                    )\n                    base.cost[i] += owncost\n            else:\n                owncost = self.cost\n                base.cost += owncost\n\n            result.extend(base.tolist())\n\n        # Finally sort the results according to cost and return the\n        # `nresults` rows with lowest cost.\n        return sorted(result)[:nresults]\n\n    def show(\n        self,\n        routeno: \"Optional[int]\" = None,\n        name: \"Optional[str]\" = None,\n        indent: int = 0,\n    ) -&gt; str:\n        \"\"\"Returns a string representation of the mapping routes to this step.\n\n        Arguments:\n            routeno: show given route.  The default is to show all routes.\n            name: Name of the last mapping step (mainly for internal use).\n            indent: How of blanks to prepend each line with (mainly for\n                internal use).\n\n        Returns:\n            String representation of the mapping routes.\n        \"\"\"\n        strings = []\n        ind = \" \" * indent\n        strings.append(ind + f'{name if name else \"Step\"}:')\n        strings.append(\n            ind + f\"  steptype: \"\n            f\"{self.steptype.name if self.steptype else None}\"\n        )\n        strings.append(ind + f\"  output_iri: {self.output_iri}\")\n        strings.append(ind + f\"  output_unit: {self.output_unit}\")\n        strings.append(ind + f\"  cost: {self.cost}\")\n        if routeno is None:\n            strings.append(ind + \"  routes:\")\n            for inputs in self.input_routes:\n                t = \"\\n\".join(\n                    [\n                        input_.show(name=name_, indent=indent + 6)\n                        for name_, input_ in inputs.items()\n                    ]\n                )\n                strings.append(ind + \"    - \" + t[indent + 6 :])\n        else:\n            strings.append(ind + \"  inputs:\")\n            inputs, idx = self.get_inputs(routeno)\n            t = \"\\n\".join(\n                [\n                    input_.show(routeno=idx, name=name_, indent=indent + 6)\n                    for name_, input_ in inputs.items()\n                ]\n            )\n            strings.append(ind + \"    - \" + t[indent + 6 :])\n        return \"\\n\".join(strings)\n\n    def _iri(self, iri: str) -&gt; str:\n        \"\"\"Help method that returns prefixed iri if possible, otherwise\n        `iri`.\"\"\"\n        return self.triplestore.prefix_iri(iri) if self.triplestore else iri\n\n    def _visualise(\n        self, routeno: int, next_iri: str, next_steptype: StepType\n    ) -&gt; str:\n        \"\"\"Help function for visualise().\n\n        Arguments:\n            routeno: Route number to visualise.\n            next_iri: IRI of the next mapping step (i.e. the previous mapping\n                when starting from the target).\n            next_steptype: Step type from this to next iri.\n\n        Returns:\n            Mapping route in dot (graphviz) notation.\n        \"\"\"\n        hasOutput = EMMO.EMMO_c4bace1d_4db0_4cd3_87e9_18122bae2840\n\n        # Edge labels. We invert the steptypes, since we want to visualise\n        # the workflow in forward direction, while the steptypes refer to\n        # backward direction\n        labeldict = {\n            StepType.UNSPECIFIED: \"\",\n            StepType.MAPSTO: \"inverse(mapsTo)\",\n            StepType.INV_MAPSTO: \"mapsTo\",\n            StepType.INSTANCEOF: \"instanceOf\",\n            StepType.INV_INSTANCEOF: \"inverse(instanceOf)\",\n            StepType.SUBCLASSOF: \"subClassOf\",\n            StepType.INV_SUBCLASSOF: \"inverse(subClassOf)\",\n            StepType.FUNCTION: \"function\",\n        }\n        inputs, idx = self.get_inputs(routeno)\n        strings = []\n        for _, input in inputs.items():\n            if isinstance(input, Value):\n                strings.append(\n                    f'  \"{self._iri(input.output_iri)}\" -&gt; '\n                    f'\"{self._iri(self.output_iri)}\" '\n                    f'[label=\"{labeldict[self.steptype]}\"];'\n                )\n            elif isinstance(input, MappingStep):\n                strings.append(\n                    input._visualise(  # pylint: disable=protected-access\n                        routeno=idx,\n                        next_iri=self.output_iri,\n                        next_steptype=self.steptype,\n                    )\n                )\n            else:\n                raise TypeError(\"input should be Value or MappingStep\")\n        if next_iri:\n            label = labeldict[next_steptype]\n            if next_steptype == StepType.FUNCTION and self.triplestore:\n                model_iri = self.triplestore.value(\n                    predicate=hasOutput,  # Assuming EMMO\n                    object=next_iri,\n                    default=\"function\",\n                    any=True,\n                )\n                if model_iri:\n                    label = self.triplestore.value(\n                        subject=model_iri,\n                        predicate=RDFS.label,\n                        default=self._iri(model_iri),\n                        any=True,\n                    )\n            else:\n                label = labeldict[next_steptype]\n            strings.append(\n                f'  \"{self._iri(self.output_iri)}\" -&gt; '\n                f'\"{self._iri(next_iri)}\" [label=\"{label}\"];'\n            )\n        return \"\\n\".join(strings)\n\n    def visualise(\n        self,\n        routeno: int,\n        output: \"Optional[str]\" = None,\n        format: \"Optional[str]\" = \"png\",\n        dot: str = \"dot\",\n    ) -&gt; str:\n        \"\"\"Greate a Graphviz visualisation of a given mapping route.\n\n        Arguments:\n            routeno: Number of mapping route to visualise.\n            output: If given, write the graph to this file.\n            format: File format to use with `output`.\n            dot: Path to Graphviz dot executable.\n\n        Returns:\n            String representation of the graph in dot format.\n        \"\"\"\n        strings = []\n        strings.append(\"digraph G {\")\n        strings.append(self._visualise(routeno, \"\", StepType.UNSPECIFIED))\n        strings.append(\"}\")\n        graph = \"\\n\".join(strings) + \"\\n\"\n        if output:\n            subprocess.run(\n                args=[dot, f\"-T{format}\", \"-o\", output],\n                shell=False,  # nosec: B603\n                check=True,\n                input=graph.encode(),\n            )\n        return graph\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep.add_input","title":"<code>add_input(self, input, name=None)</code>","text":"<p>Add an input (MappingStep or Value), where <code>name</code> is the name assigned to the argument.</p> <p>If the <code>join_mode</code> attribute is false, a new route is created with only one input.</p> <p>If the <code>join_mode</code> attribute is true, the input is remembered, but first added when <code>join_input()</code> is called.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>'Input'</code> <p>A mapping step or a value.</p> required <code>name</code> <code>'Optional[str]'</code> <p>Name assigned to the argument.</p> <code>None</code> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def add_input(self, input: \"Input\", name: \"Optional[str]\" = None) -&gt; None:\n    \"\"\"Add an input (MappingStep or Value), where `name` is the name\n    assigned to the argument.\n\n    If the `join_mode` attribute is false, a new route is created with\n    only one input.\n\n    If the `join_mode` attribute is true, the input is remembered, but\n    first added when `join_input()` is called.\n\n    Arguments:\n        input: A mapping step or a value.\n        name: Name assigned to the argument.\n    \"\"\"\n    assert isinstance(input, (MappingStep, Value))  # nosec B101\n    argname = name if name else f\"arg{len(self.joined_input)+1}\"\n    if self.join_mode:\n        self.joined_input[argname] = input\n    else:\n        self.add_inputs({argname: input})\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep.add_inputs","title":"<code>add_inputs(self, inputs)</code>","text":"<p>Add input Mapping (e.g., dict) for an input route.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def add_inputs(self, inputs: \"Inputs\") -&gt; None:\n    \"\"\"Add input Mapping (e.g., dict) for an input route.\"\"\"\n    assert isinstance(inputs, Mapping)  # nosec B101\n    self.input_routes.append(inputs)\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep.eval","title":"<code>eval(self, routeno=None, unit=None, magnitude=False, quantity=None)</code>","text":"<p>Returns the evaluated value of given input route number.</p> <p>Parameters:</p> Name Type Description Default <code>routeno</code> <code>'Optional[int]'</code> <p>The route number to evaluate.  If None (default) the route with the lowest cost is evalueated.</p> <code>None</code> <code>unit</code> <code>'Optional[str]'</code> <p>return the result in the given unit. Implies <code>magnitude=True</code>.</p> <code>None</code> <code>magnitude</code> <code>bool</code> <p>Whether to only return the magnitude of the evaluated value (with no unit).</p> <code>False</code> <code>quantity</code> <code>'Optional[Type[Quantity]]'</code> <p>Quantity class to use for evaluation.  Defaults to pint.</p> <code>None</code> <p>Returns:</p> Type Description <code>'Any'</code> <p>Evaluation result.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def eval(\n    self,\n    routeno: \"Optional[int]\" = None,\n    unit: \"Optional[str]\" = None,\n    magnitude: bool = False,\n    quantity: \"Optional[Type[Quantity]]\" = None,\n) -&gt; \"Any\":\n    \"\"\"Returns the evaluated value of given input route number.\n\n    Arguments:\n        routeno: The route number to evaluate.  If None (default)\n            the route with the lowest cost is evalueated.\n        unit: return the result in the given unit.\n            Implies `magnitude=True`.\n        magnitude: Whether to only return the magnitude of the evaluated\n            value (with no unit).\n        quantity: Quantity class to use for evaluation.  Defaults to pint.\n\n    Returns:\n        Evaluation result.\n    \"\"\"\n    if not self.number_of_routes():\n        raise MissingRelationError(\n            f\"no route to evaluate '{self.output_iri}'\"\n        )\n    if quantity is None:\n        quantity = Quantity\n    if routeno is None:\n        ((_, routeno),) = self.lowest_costs(nresults=1)\n    inputs, idx = self.get_inputs(routeno)\n    values = get_values(inputs, idx, quantity=quantity)\n\n    if self.function:\n        value = self.function(**values)\n    elif len(values) == 1:\n        (value,) = values.values()\n    else:\n        raise TypeError(\n            f\"Expected inputs to be a single argument: {values}\"\n        )\n\n    if isinstance(value, Quantity) and unit:\n        return value.m_as(unit)\n    if isinstance(value, Quantity) and magnitude:\n        return value.m\n    if isinstance(value, Value):\n        return value.get_value(\n            unit=unit, magnitude=magnitude, quantity=quantity\n        )\n    return value\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep.get_input_iris","title":"<code>get_input_iris(self, routeno)</code>","text":"<p>Returns a dict mapping input names to iris for the given route number.</p> <p>Parameters:</p> Name Type Description Default <code>routeno</code> <code>int</code> <p>The route number to return a mapping for.</p> required <p>Returns:</p> Type Description <code>'Dict[str, Optional[str]]'</code> <p>Mapping of input names to IRIs.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def get_input_iris(self, routeno: int) -&gt; \"Dict[str, Optional[str]]\":\n    \"\"\"Returns a dict mapping input names to iris for the given route\n    number.\n\n    Arguments:\n        routeno: The route number to return a mapping for.\n\n    Returns:\n        Mapping of input names to IRIs.\n\n    \"\"\"\n    inputs, _ = self.get_inputs(routeno)\n    return {\n        k: v.output_iri if isinstance(v, MappingStep) else v.output_iri\n        for k, v in inputs.items()\n    }\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep.get_inputs","title":"<code>get_inputs(self, routeno)</code>","text":"<p>Returns input and input index <code>(inputs, idx)</code> for route number <code>routeno</code>.</p> <p>Parameters:</p> Name Type Description Default <code>routeno</code> <code>int</code> <p>The route number to return inputs for.</p> required <p>Returns:</p> Type Description <code>'Tuple[Inputs, int]'</code> <p>Inputs and difference between route number and number of routes for an input dictioary.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def get_inputs(self, routeno: int) -&gt; \"Tuple[Inputs, int]\":\n    \"\"\"Returns input and input index `(inputs, idx)` for route number\n    `routeno`.\n\n    Arguments:\n        routeno: The route number to return inputs for.\n\n    Returns:\n        Inputs and difference between route number and number of routes for\n        an input dictioary.\n    \"\"\"\n    n = 0\n    for inputs in self.input_routes:\n        n0 = n\n        n += get_nroutes(inputs)\n        if n &gt; routeno:\n            return inputs, routeno - n0\n    raise ValueError(f\"routeno={routeno} exceeds number of routes\")\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep.join_input","title":"<code>join_input(self)</code>","text":"<p>Join all input added with add_input() since <code>join_mode</code> was set true.  Resets <code>join_mode</code> to false.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def join_input(self) -&gt; None:\n    \"\"\"Join all input added with add_input() since `join_mode` was set\n    true.  Resets `join_mode` to false.\"\"\"\n    if not self.join_mode:\n        raise MappingError(\"Calling join_input() when join_mode is false.\")\n    self.join_mode = False\n    self.add_inputs(self.joined_input)\n    self.joined_input = {}\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep.lowest_costs","title":"<code>lowest_costs(self, nresults=5)</code>","text":"<p>Returns a list of <code>(cost, routeno)</code> tuples with up to the <code>nresult</code> lowest costs and their corresponding route numbers.</p> <p>Parameters:</p> Name Type Description Default <code>nresults</code> <code>int</code> <p>Number of results to return.</p> <code>5</code> <p>Returns:</p> Type Description <code>'List[Tuple[float, int]]'</code> <p>A list of <code>(cost, routeno)</code> tuples.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def lowest_costs(self, nresults: int = 5) -&gt; \"List[Tuple[float, int]]\":\n    \"\"\"Returns a list of `(cost, routeno)` tuples with up to the `nresult`\n    lowest costs and their corresponding route numbers.\n\n    Arguments:\n        nresults: Number of results to return.\n\n    Returns:\n        A list of `(cost, routeno)` tuples.\n    \"\"\"\n    try:\n        import numpy as np  # pylint: disable=import-outside-toplevel\n    except ImportError as exc:\n        raise RuntimeError(\n            \"Mappings.lowest_costs() requires numpy.\\n\"\n            \"Install it with\\n\\n\"\n            \"    pip install numpy\"\n        ) from exc\n\n    result = []\n    n = 0  # total number of routes\n\n    # Loop over all toplevel routes leading into this mapping step\n    for inputs in self.input_routes:\n        # For each route, loop over all input arguments of this step\n        # The number of permutations we must consider is the product\n        # of the total number of routes to each input argument.\n        #\n        # We (potentially drastic) limit the possibilities by only\n        # considering the `nresults` routes with lowest costs into\n        # each argument.  This gives at maximum\n        #\n        #     nresults * number_of_input_arguments\n        #\n        # possibilities. We calculate the costs for all of them and\n        # store them in an array with two columns: `cost` and `routeno`.\n        # The `results` list is extended with the cost array\n        # for each toplevel route leading into this step.\n        base = np.rec.fromrecords(\n            [(0.0, 0)], names=\"cost,routeno\", formats=\"f8,i8\"\n        )\n        m = 1\n        for input in inputs.values():\n            if isinstance(input, MappingStep):\n                nroutes = input.number_of_routes()\n                res = np.rec.fromrecords(\n                    sorted(\n                        input.lowest_costs(nresults=nresults),\n                        key=lambda x: x[1],\n                    ),\n                    # [\n                    #     row\n                    #     for row in sorted(\n                    #         input.lowest_costs(nresults=nresults),\n                    #         key=lambda x: x[1],\n                    #     )\n                    # ],\n                    dtype=base.dtype,\n                )\n                res1 = res.repeat(len(base))\n                base = np.tile(base, len(res))\n                base.cost += res1.cost\n                base.routeno += res1.routeno * m\n                m *= nroutes\n            else:\n                base.cost += input.cost\n\n        # Reduce the length of base (makes probably only sense in\n        # the case self.cost is a callable, but it doesn't hurt...)\n        base.sort()\n        base = base[:nresults]\n        base.routeno += n\n        n += m\n\n        # Add the cost for this step to `res`.  If `self.cost` is\n        # a callable, we call it with the input for each routeno\n        # as arguments.  Otherwise `self.cost` is the cost of this\n        # mapping step.\n        if callable(self.cost):\n            for i, rno in enumerate(base.routeno):\n                inputs, _ = self.get_inputs(rno)\n                input_iris = [\n                    input.output_iri for input in inputs.values()\n                ]\n                owncost = self.cost(\n                    self.triplestore, input_iris, self.output_iri\n                )\n                base.cost[i] += owncost\n        else:\n            owncost = self.cost\n            base.cost += owncost\n\n        result.extend(base.tolist())\n\n    # Finally sort the results according to cost and return the\n    # `nresults` rows with lowest cost.\n    return sorted(result)[:nresults]\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep.number_of_routes","title":"<code>number_of_routes(self)</code>","text":"<p>Total number of routes to this mapping step.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total number of routes to this mapping step.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def number_of_routes(self) -&gt; int:\n    \"\"\"Total number of routes to this mapping step.\n\n    Returns:\n        Total number of routes to this mapping step.\n    \"\"\"\n    n = 0\n    for inputs in self.input_routes:\n        n += get_nroutes(inputs)\n    return n\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep.show","title":"<code>show(self, routeno=None, name=None, indent=0)</code>","text":"<p>Returns a string representation of the mapping routes to this step.</p> <p>Parameters:</p> Name Type Description Default <code>routeno</code> <code>'Optional[int]'</code> <p>show given route.  The default is to show all routes.</p> <code>None</code> <code>name</code> <code>'Optional[str]'</code> <p>Name of the last mapping step (mainly for internal use).</p> <code>None</code> <code>indent</code> <code>int</code> <p>How of blanks to prepend each line with (mainly for internal use).</p> <code>0</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the mapping routes.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def show(\n    self,\n    routeno: \"Optional[int]\" = None,\n    name: \"Optional[str]\" = None,\n    indent: int = 0,\n) -&gt; str:\n    \"\"\"Returns a string representation of the mapping routes to this step.\n\n    Arguments:\n        routeno: show given route.  The default is to show all routes.\n        name: Name of the last mapping step (mainly for internal use).\n        indent: How of blanks to prepend each line with (mainly for\n            internal use).\n\n    Returns:\n        String representation of the mapping routes.\n    \"\"\"\n    strings = []\n    ind = \" \" * indent\n    strings.append(ind + f'{name if name else \"Step\"}:')\n    strings.append(\n        ind + f\"  steptype: \"\n        f\"{self.steptype.name if self.steptype else None}\"\n    )\n    strings.append(ind + f\"  output_iri: {self.output_iri}\")\n    strings.append(ind + f\"  output_unit: {self.output_unit}\")\n    strings.append(ind + f\"  cost: {self.cost}\")\n    if routeno is None:\n        strings.append(ind + \"  routes:\")\n        for inputs in self.input_routes:\n            t = \"\\n\".join(\n                [\n                    input_.show(name=name_, indent=indent + 6)\n                    for name_, input_ in inputs.items()\n                ]\n            )\n            strings.append(ind + \"    - \" + t[indent + 6 :])\n    else:\n        strings.append(ind + \"  inputs:\")\n        inputs, idx = self.get_inputs(routeno)\n        t = \"\\n\".join(\n            [\n                input_.show(routeno=idx, name=name_, indent=indent + 6)\n                for name_, input_ in inputs.items()\n            ]\n        )\n        strings.append(ind + \"    - \" + t[indent + 6 :])\n    return \"\\n\".join(strings)\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MappingStep.visualise","title":"<code>visualise(self, routeno, output=None, format='png', dot='dot')</code>","text":"<p>Greate a Graphviz visualisation of a given mapping route.</p> <p>Parameters:</p> Name Type Description Default <code>routeno</code> <code>int</code> <p>Number of mapping route to visualise.</p> required <code>output</code> <code>'Optional[str]'</code> <p>If given, write the graph to this file.</p> <code>None</code> <code>format</code> <code>'Optional[str]'</code> <p>File format to use with <code>output</code>.</p> <code>'png'</code> <code>dot</code> <code>str</code> <p>Path to Graphviz dot executable.</p> <code>'dot'</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the graph in dot format.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def visualise(\n    self,\n    routeno: int,\n    output: \"Optional[str]\" = None,\n    format: \"Optional[str]\" = \"png\",\n    dot: str = \"dot\",\n) -&gt; str:\n    \"\"\"Greate a Graphviz visualisation of a given mapping route.\n\n    Arguments:\n        routeno: Number of mapping route to visualise.\n        output: If given, write the graph to this file.\n        format: File format to use with `output`.\n        dot: Path to Graphviz dot executable.\n\n    Returns:\n        String representation of the graph in dot format.\n    \"\"\"\n    strings = []\n    strings.append(\"digraph G {\")\n    strings.append(self._visualise(routeno, \"\", StepType.UNSPECIFIED))\n    strings.append(\"}\")\n    graph = \"\\n\".join(strings) + \"\\n\"\n    if output:\n        subprocess.run(\n            args=[dot, f\"-T{format}\", \"-o\", output],\n            shell=False,  # nosec: B603\n            check=True,\n            input=graph.encode(),\n        )\n    return graph\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.MissingRelationError","title":"<code> MissingRelationError            (MappingError)         </code>","text":"<p>There are missing relations in RDF triples.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>class MissingRelationError(MappingError):\n    \"\"\"There are missing relations in RDF triples.\"\"\"\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.StepType","title":"<code> StepType            (Enum)         </code>","text":"<p>Type of mapping step when going from the output to the inputs.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>class StepType(Enum):\n    \"\"\"Type of mapping step when going from the output to the inputs.\"\"\"\n\n    UNSPECIFIED = 0\n    MAPSTO = 1\n    INV_MAPSTO = -1\n    INSTANCEOF = 2\n    INV_INSTANCEOF = -2\n    SUBCLASSOF = 3\n    INV_SUBCLASSOF = -3\n    FUNCTION = 4\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.UnknownUnitError","title":"<code> UnknownUnitError            (MappingError)         </code>","text":"<p>A unit does not exists in the pint unit registry.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>class UnknownUnitError(MappingError):\n    \"\"\"A unit does not exists in the pint unit registry.\"\"\"\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.Value","title":"<code> Value        </code>","text":"<p>Represents the value of an instance property.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>'Any'</code> <p>Property value.</p> <code>None</code> <code>unit</code> <code>'Optional[str]'</code> <p>Property unit.</p> <code>None</code> <code>iri</code> <code>'Optional[str]'</code> <p>IRI of ontological concept that this value is an instance of.</p> <code>None</code> <code>property_iri</code> <code>'Optional[str]'</code> <p>IRI of datamodel property that this value is an instance of.</p> <code>None</code> <code>cost</code> <code>'Union[float, Callable]'</code> <p>Cost of accessing this value.</p> <code>0.0</code> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>class Value:\n    \"\"\"Represents the value of an instance property.\n\n    Arguments:\n        value: Property value.\n        unit: Property unit.\n        iri: IRI of ontological concept that this value is an instance of.\n        property_iri: IRI of datamodel property that this value is an\n            instance of.\n        cost: Cost of accessing this value.\n    \"\"\"\n\n    # pylint: disable=too-few-public-methods\n\n    def __init__(\n        self,\n        value: \"Any\" = None,\n        unit: \"Optional[str]\" = None,\n        iri: \"Optional[str]\" = None,\n        property_iri: \"Optional[str]\" = None,\n        cost: \"Union[float, Callable]\" = 0.0,\n    ):\n        self._value = value\n        self.unit = unit\n        if iri:\n            self.output_iri = iri\n        elif hasattr(value, __name__):\n            self.output_iri = value.__name__\n        else:\n            self.output_iri = f\"_:value_{id(value)}\"\n        self.property_iri = property_iri\n        self.cost = cost\n\n    value = property(\n        lambda self: self._value() if callable(self._value) else self._value,\n        doc=\"Value of property.\",\n    )\n\n    def __repr__(self):\n        args = []\n        if self.unit:\n            args.append(f\", unit={self.unit}\")\n        if self.output_iri:\n            args.append(f\", iri={self.output_iri}\")\n        if self.property_iri:\n            args.append(f\", property_iri={self.property_iri}\")\n        if self.cost:\n            args.append(f\", cost={self.cost}\")\n        return f\"Value({self._value!r}{''.join(args)})\"\n\n    def get_value(self, unit=None, magnitude=False, quantity=None) -&gt; \"Any\":\n        \"\"\"Returns the evaluated value of given input route number.\n\n        Arguments:\n            unit: return the result in the given unit.\n                Implies `magnitude=True`.\n            magnitude: Whether to only return the magnitude of the evaluated\n                value (with no unit).\n            quantity: Quantity class to use for evaluation.  Defaults to pint.\n\n        Returns:\n            Value.\n        \"\"\"\n        if quantity is None:\n            quantity = Quantity\n        value = self._value() if callable(self._value) else self._value\n        if not isinstance(value, Quantity) and not self.unit:\n            return value\n        q = quantity(value, self.unit)\n        if unit:\n            return q.m_as(unit)\n        if magnitude:\n            return q.m\n        return q\n\n    def show(\n        self,\n        routeno: \"Optional[int]\" = None,\n        name: \"Optional[str]\" = None,\n        indent: int = 0,\n    ) -&gt; str:\n        # pylint: disable=unused-argument\n        \"\"\"Returns a string representation of the Value.\n\n        Arguments:\n            routeno: Unused.  The argument exists for consistency with\n                the corresponding method in Step.\n            name: Name of value.\n            indent: Indentation level.\n\n        Returns:\n            String representation of the value.\n        \"\"\"\n        strings = []\n        ind = \" \" * indent\n        strings.append(ind + f'{name if name else \"Value\"}:')\n        strings.append(ind + f\"  iri: {self.output_iri}\")\n        strings.append(ind + f\"  property_iri: {self.property_iri}\")\n        strings.append(ind + f\"  unit: {self.unit}\")\n        strings.append(ind + f\"  cost: {self.cost}\")\n        strings.append(ind + f\"  value: {self.value}\")\n        return \"\\n\".join(strings)\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.Value.value","title":"<code>value</code>  <code>property</code> <code>readonly</code>","text":"<p>Value of property.</p>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.Value.get_value","title":"<code>get_value(self, unit=None, magnitude=False, quantity=None)</code>","text":"<p>Returns the evaluated value of given input route number.</p> <p>Parameters:</p> Name Type Description Default <code>unit</code> <p>return the result in the given unit. Implies <code>magnitude=True</code>.</p> <code>None</code> <code>magnitude</code> <p>Whether to only return the magnitude of the evaluated value (with no unit).</p> <code>False</code> <code>quantity</code> <p>Quantity class to use for evaluation.  Defaults to pint.</p> <code>None</code> <p>Returns:</p> Type Description <code>'Any'</code> <p>Value.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def get_value(self, unit=None, magnitude=False, quantity=None) -&gt; \"Any\":\n    \"\"\"Returns the evaluated value of given input route number.\n\n    Arguments:\n        unit: return the result in the given unit.\n            Implies `magnitude=True`.\n        magnitude: Whether to only return the magnitude of the evaluated\n            value (with no unit).\n        quantity: Quantity class to use for evaluation.  Defaults to pint.\n\n    Returns:\n        Value.\n    \"\"\"\n    if quantity is None:\n        quantity = Quantity\n    value = self._value() if callable(self._value) else self._value\n    if not isinstance(value, Quantity) and not self.unit:\n        return value\n    q = quantity(value, self.unit)\n    if unit:\n        return q.m_as(unit)\n    if magnitude:\n        return q.m\n    return q\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.Value.show","title":"<code>show(self, routeno=None, name=None, indent=0)</code>","text":"<p>Returns a string representation of the Value.</p> <p>Parameters:</p> Name Type Description Default <code>routeno</code> <code>'Optional[int]'</code> <p>Unused.  The argument exists for consistency with the corresponding method in Step.</p> <code>None</code> <code>name</code> <code>'Optional[str]'</code> <p>Name of value.</p> <code>None</code> <code>indent</code> <code>int</code> <p>Indentation level.</p> <code>0</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the value.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def show(\n    self,\n    routeno: \"Optional[int]\" = None,\n    name: \"Optional[str]\" = None,\n    indent: int = 0,\n) -&gt; str:\n    # pylint: disable=unused-argument\n    \"\"\"Returns a string representation of the Value.\n\n    Arguments:\n        routeno: Unused.  The argument exists for consistency with\n            the corresponding method in Step.\n        name: Name of value.\n        indent: Indentation level.\n\n    Returns:\n        String representation of the value.\n    \"\"\"\n    strings = []\n    ind = \" \" * indent\n    strings.append(ind + f'{name if name else \"Value\"}:')\n    strings.append(ind + f\"  iri: {self.output_iri}\")\n    strings.append(ind + f\"  property_iri: {self.property_iri}\")\n    strings.append(ind + f\"  unit: {self.unit}\")\n    strings.append(ind + f\"  cost: {self.cost}\")\n    strings.append(ind + f\"  value: {self.value}\")\n    return \"\\n\".join(strings)\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.emmo_mapper","title":"<code>emmo_mapper(triplestore)</code>","text":"<p>Finds all function definitions in <code>triplestore</code> based on EMMO.</p> <p>Return a dict mapping output IRIs to a list of</p> <pre><code>(function_iri, [input_iris, ...])\n</code></pre> <p>tuples.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def emmo_mapper(triplestore: \"Triplestore\") -&gt; \"Dict[str, list]\":\n    \"\"\"Finds all function definitions in `triplestore` based on EMMO.\n\n    Return a dict mapping output IRIs to a list of\n\n        (function_iri, [input_iris, ...])\n\n    tuples.\n    \"\"\"\n    Task = EMMO.EMMO_4299e344_a321_4ef2_a744_bacfcce80afc\n    hasInput = EMMO.EMMO_36e69413_8c59_4799_946c_10b05d266e22\n    hasOutput = EMMO.EMMO_c4bace1d_4db0_4cd3_87e9_18122bae2840\n\n    d = defaultdict(list)\n    for task in triplestore.subjects(RDF.type, Task):\n        inputs = list(triplestore.objects(task, hasInput))\n        for output in triplestore.objects(task, hasOutput):\n            d[output].append((task, inputs))\n\n    return d\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.fno_mapper","title":"<code>fno_mapper(triplestore)</code>","text":"<p>Finds all function definitions in <code>triplestore</code> based on the function ontololy (FNO).</p> <p>Parameters:</p> Name Type Description Default <code>triplestore</code> <code>'Triplestore'</code> <p>The triplestore to investigate.</p> required <p>Returns:</p> Type Description <code>'Dict[str, list]'</code> <p>A mapping of output IRIs to a list of</p> <pre><code>(function_iri, [input_iris, ...])\n</code></pre> <p>tuples.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def fno_mapper(triplestore: \"Triplestore\") -&gt; \"Dict[str, list]\":\n    \"\"\"Finds all function definitions in `triplestore` based on the function\n    ontololy (FNO).\n\n    Arguments:\n        triplestore: The triplestore to investigate.\n\n    Returns:\n        A mapping of output IRIs to a list of\n\n            (function_iri, [input_iris, ...])\n\n        tuples.\n    \"\"\"\n    # pylint: disable=too-many-branches\n\n    # Temporary dicts for fast lookup\n    Dfirst = dict(triplestore.subject_objects(RDF.first))\n    Drest = dict(triplestore.subject_objects(RDF.rest))\n    Dexpects = defaultdict(list)\n    Dreturns = defaultdict(list)\n    for s, o in triplestore.subject_objects(FNO.expects):\n        Dexpects[s].append(o)\n    for s, o in triplestore.subject_objects(FNO.returns):\n        Dreturns[s].append(o)\n\n    d = defaultdict(list)\n    for func, lst in Dreturns.items():\n        input_iris = []\n        for exp in Dexpects.get(func, ()):\n            if exp in Dfirst:\n                while exp in Dfirst:\n                    input_iris.append(Dfirst[exp])\n                    if exp not in Drest:\n                        break\n                    exp = Drest[exp]\n            else:\n                # Support also misuse of FNO, where fno:expects refers\n                # directly to input individuals\n                input_iris.append(exp)\n\n        for ret in lst:\n            if ret in Dfirst:\n                while ret in Dfirst:\n                    d[Dfirst[ret]].append((func, input_iris))\n                    if ret not in Drest:\n                        break\n                    ret = Drest[ret]\n            else:\n                # Support also misuse of FNO, where fno:returns refers\n                # directly to the returned individual\n                d[ret].append((func, input_iris))\n\n    return d\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.get_nroutes","title":"<code>get_nroutes(inputs)</code>","text":"<p>Help function returning the number of routes for an input dict.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>'Inputs'</code> <p>Input dictionary.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of routes in the <code>inputs</code> input dictionary.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def get_nroutes(inputs: \"Inputs\") -&gt; int:\n    \"\"\"Help function returning the number of routes for an input dict.\n\n    Arguments:\n        inputs: Input dictionary.\n\n    Returns:\n        Number of routes in the `inputs` input dictionary.\n    \"\"\"\n    nroutes = 1\n    for input in inputs.values():\n        if isinstance(input, MappingStep):\n            nroutes *= input.number_of_routes()\n    return nroutes\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.get_values","title":"<code>get_values(inputs, routeno, quantity=&lt;class 'pint.registry.Quantity'&gt;, magnitudes=False)</code>","text":"<p>Help function returning a dict mapping the input names to actual value of expected input unit.</p> <p>There exists <code>get_nroutes(inputs)</code> routes to populate <code>inputs</code>. <code>routeno</code> is the index of the specific route we will use to obtain the values.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>'dict[str, Any]'</code> <p>Input dictionary.</p> required <code>routeno</code> <code>int</code> <p>Route number index.</p> required <code>quantity</code> <code>'Type[Quantity]'</code> <p>A unit quantity class.</p> <code>&lt;class 'pint.registry.Quantity'&gt;</code> <code>magnitudes</code> <code>bool</code> <p>Whether to only return the magnitude of the evaluated value (with no unit).</p> <code>False</code> <p>Returns:</p> Type Description <code>'dict[str, Any]'</code> <p>A mapping between input names and values of expected input unit.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def get_values(\n    inputs: \"dict[str, Any]\",\n    routeno: int,\n    quantity: \"Type[Quantity]\" = Quantity,\n    magnitudes: bool = False,\n) -&gt; \"dict[str, Any]\":\n    \"\"\"Help function returning a dict mapping the input names to actual value\n    of expected input unit.\n\n    There exists `get_nroutes(inputs)` routes to populate `inputs`.\n    `routeno` is the index of the specific route we will use to obtain the\n    values.\n\n    Arguments:\n        inputs: Input dictionary.\n        routeno: Route number index.\n        quantity: A unit quantity class.\n        magnitudes: Whether to only return the magnitude of the evaluated\n            value (with no unit).\n\n    Returns:\n        A mapping between input names and values of expected input unit.\n    \"\"\"\n    values = {}\n\n    for k, v in inputs.items():\n        if isinstance(v, MappingStep):\n            value = v.eval(routeno=routeno, quantity=quantity)\n            values[k] = (\n                value.to(v.output_unit)\n                if v.output_unit and isinstance(v, quantity)\n                else value\n            )\n        elif isinstance(v, Value):\n            values[k] = v.value if not v.unit else quantity(v.value, v.unit)\n        else:\n            raise TypeError(\n                \"Expected values in inputs to be either `MappingStep` or \"\n                \"`Value` objects.\"\n            )\n\n        if magnitudes:\n            values = {\n                k: v.m if isinstance(v, quantity) else v\n                for k, v in values.items()\n            }\n\n    return values\n</code></pre>"},{"location":"api_reference/mappings/mappings/#tripper.mappings.mappings.mapping_routes","title":"<code>mapping_routes(target, sources, triplestore, function_repo=None, function_mappers=(&lt;function emmo_mapper at 0x7f63fd2d09a0&gt;, &lt;function fno_mapper at 0x7f63fd2d0a40&gt;), default_costs=(('function', 10.0), ('mapsTo', 2.0), ('instanceOf', 1.0), ('subClassOf', 1.0), ('value', 0.0)), value_class=None, mappingstep_class=None, mapsTo='https://w3id.org/emmo/domain/mappings#mapsTo', instanceOf='https://w3id.org/emmo/domain/datamodel#instanceOf', subClassOf='http://www.w3.org/2000/01/rdf-schema#subClassOf', label='http://www.w3.org/2000/01/rdf-schema#label', hasUnit='https://w3id.org/emmo/domain/datamodel#hasUnit', hasCost='https://w3id.org/emmo/domain/datamodel#hasCost', hasAccessFunction='https://w3id.org/emmo#hasAccessFunction', hasDataValue='http://www.w3.org/1999/02/22-rdf-syntax-ns#value')</code>","text":"<p>Find routes of mappings from any source in <code>sources</code> to <code>target</code>.</p> <p>This implementation supports functions (using FnO) and subclass relations.  It also correctly handles transitivity of <code>mapsTo</code> and <code>subClassOf</code> relations.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>str</code> <p>IRI of the target in <code>triplestore</code>.</p> required <code>sources</code> <code>'Union[Dict[str, Union[Value, None]], Sequence[str]]'</code> <p>Dict mapping source IRIs to source values or a sequence of source IRIs (with no explicit values).</p> required <code>triplestore</code> <code>'Triplestore'</code> <p>Triplestore instance for the knowledge graph to traverse.</p> required <p>Additional arguments for fine-grained tuning:     !!! function_repo \"Dict mapping function IRIs to corresponding Python\"         function.  Default is to use <code>triplestore.function_repo</code>.     !!! function_mappers \"Name of mapping standard: \"emmo\" or \"fno\".\"         Alternatively, a sequence of mapping functions that takes         <code>triplestore</code> as argument and return a dict mapping output IRIs         to a list of <code>(function_iri, [input_iris, ...])</code> tuples.     !!! default_costs \"A dict providing default costs of different types\"         of mapping steps (\"function\", \"mapsTo\", \"instanceOf\",         \"subClassOf\", and \"value\").  These costs can be overridden with         'hasCost' relations in the ontology.     !!! value_class \"Optional <code>Value</code> subclass to use instead of <code>Value</code> when\"         creating the returned mapping route.     !!! mappingstep_class \"Optional <code>MappingStep</code> subclass to use instead of\"         <code>MappingStep</code> when creating the returned mapping route.     mapsTo: IRI of 'mapsTo' in <code>triplestore</code>.     instanceOf: IRI of 'instanceOf' in <code>triplestore</code>.     !!! subclassof \"IRI of 'subClassOf' in <code>triples</code>.  Set it to None if\"         subclasses should not be considered.     !!! label \"IRI of 'label' in <code>triplestore</code>.  Used for naming function\"         input parameters.  The default is to use rdfs:label.     !!! hasunit \"IRI of 'hasUnit' in <code>triplestore</code>.  Can be used to explicit\"         specify the unit of a quantity.     !!! hascost \"IRI of 'hasCost' in <code>triplestore</code>.  Used for associating a\"         user-defined cost or cost function with instantiation of a         property.     !!! hasaccessfunction \"IRI of 'hasAccessFunction'.  Used to associate a\"         data source to a function that retrieves the data.     !!! hasdatavalue \"IRI of 'hasDataValue'.  Used to associate a data source\"         with its literal value.</p> <p>Returns:</p> Type Description <code>Input</code> <p>A MappingStep instance.  This is a root of a nested tree of MappingStep instances providing an (efficient) internal description of all possible mapping routes from <code>sources</code> to <code>target</code>.</p> Source code in <code>tripper/mappings/mappings.py</code> <pre><code>def mapping_routes(\n    target: str,\n    sources: \"Union[Dict[str, Union[Value, None]], Sequence[str]]\",\n    triplestore: \"Triplestore\",\n    function_repo: \"Optional[dict]\" = None,\n    function_mappers: \"Union[str, Sequence[Callable]]\" = (\n        emmo_mapper,\n        fno_mapper,\n    ),\n    default_costs: \"Tuple\" = (\n        (\"function\", 10.0),\n        (\"mapsTo\", 2.0),\n        (\"instanceOf\", 1.0),\n        (\"subClassOf\", 1.0),\n        (\"value\", 0.0),\n    ),\n    value_class: \"Optional[Type[Value]]\" = None,\n    mappingstep_class: \"Optional[Type[MappingStep]]\" = None,\n    mapsTo: str = MAP.mapsTo,\n    instanceOf: str = DM.instanceOf,\n    subClassOf: str = RDFS.subClassOf,\n    # description: str = DCTERMS.description,\n    label: str = RDFS.label,\n    hasUnit: str = DM.hasUnit,\n    hasCost: str = DM.hasCost,  # TODO - add hasCost to the DM ontology\n    hasAccessFunction: str = hasAccessFunction,  # pylint: disable=redefined-outer-name\n    hasDataValue: str = hasDataValue,  # pylint: disable=redefined-outer-name\n) -&gt; Input:\n    \"\"\"Find routes of mappings from any source in `sources` to `target`.\n\n    This implementation supports functions (using FnO) and subclass\n    relations.  It also correctly handles transitivity of `mapsTo` and\n    `subClassOf` relations.\n\n    Arguments:\n        target: IRI of the target in `triplestore`.\n        sources: Dict mapping source IRIs to source values or a sequence\n            of source IRIs (with no explicit values).\n        triplestore: Triplestore instance for the knowledge graph to traverse.\n\n    Additional arguments for fine-grained tuning:\n        function_repo: Dict mapping function IRIs to corresponding Python\n            function.  Default is to use `triplestore.function_repo`.\n        function_mappers: Name of mapping standard: \"emmo\" or \"fno\".\n            Alternatively, a sequence of mapping functions that takes\n            `triplestore` as argument and return a dict mapping output IRIs\n            to a list of `(function_iri, [input_iris, ...])` tuples.\n        default_costs: A dict providing default costs of different types\n            of mapping steps (\"function\", \"mapsTo\", \"instanceOf\",\n            \"subClassOf\", and \"value\").  These costs can be overridden with\n            'hasCost' relations in the ontology.\n        value_class: Optional `Value` subclass to use instead of `Value` when\n            creating the returned mapping route.\n        mappingstep_class: Optional `MappingStep` subclass to use instead of\n            `MappingStep` when creating the returned mapping route.\n        mapsTo: IRI of 'mapsTo' in `triplestore`.\n        instanceOf: IRI of 'instanceOf' in `triplestore`.\n        subClassOf: IRI of 'subClassOf' in `triples`.  Set it to None if\n            subclasses should not be considered.\n        label: IRI of 'label' in `triplestore`.  Used for naming function\n            input parameters.  The default is to use rdfs:label.\n        hasUnit: IRI of 'hasUnit' in `triplestore`.  Can be used to explicit\n            specify the unit of a quantity.\n        hasCost: IRI of 'hasCost' in `triplestore`.  Used for associating a\n            user-defined cost or cost function with instantiation of a\n            property.\n        hasAccessFunction: IRI of 'hasAccessFunction'.  Used to associate a\n            data source to a function that retrieves the data.\n        hasDataValue: IRI of 'hasDataValue'.  Used to associate a data source\n            with its literal value.\n\n    Returns:\n        A MappingStep instance.  This is a root of a nested tree of\n        MappingStep instances providing an (efficient) internal description\n        of all possible mapping routes from `sources` to `target`.\n    \"\"\"\n    # pylint: disable=too-many-arguments,too-many-locals,too-many-statements\n\n    if target in sources:\n        return Value(iri=target)\n\n    if isinstance(sources, Sequence):\n        sources = {iri: None for iri in sources}\n\n    if function_repo is None:\n        function_repo = triplestore.function_repo\n\n    if isinstance(function_mappers, str):\n        fmd = {\"emmo\": emmo_mapper, \"fno\": fno_mapper}\n        function_mappers = [fmd[name] for name in function_mappers.split(\",\")]\n\n    default_costs = dict(default_costs)\n\n    if value_class is None:\n        value_class = Value\n\n    if mappingstep_class is None:\n        mappingstep_class = MappingStep\n\n    # Create lookup tables for fast access to triplestore content\n    soMaps = defaultdict(list)  # (s, mapsTo, o)     ==&gt; soMaps[s]  -&gt; [o, ..]\n    osMaps = defaultdict(\n        list\n    )  # (o, inv(mapsTo), s)     ==&gt; osMaps[o]  -&gt; [s, ..]\n    osSubcl = defaultdict(\n        list\n    )  # (o, inv(subClassOf), s) ==&gt; osSubcl[o] -&gt; [s, ..]\n    soInst = {}  # (s, instanceOf, o) ==&gt; soInst[s]  -&gt; o\n    osInst = defaultdict(\n        list\n    )  # (o, inv(instanceOf), s) ==&gt; osInst[o]  -&gt; [s, ..]\n    for s, o in triplestore.subject_objects(mapsTo):\n        soMaps[s].append(o)\n        osMaps[o].append(s)\n    for s, o in triplestore.subject_objects(subClassOf):\n        osSubcl[o].append(s)\n    for s, o in triplestore.subject_objects(instanceOf):\n        if s in soInst:\n            raise InconsistentTriplesError(\n                f\"The same individual can only relate to one datamodel \"\n                f\"property via {instanceOf} relations.\"\n            )\n        soInst[s] = o\n        osInst[o].append(s)\n    soName = dict(triplestore.subject_objects(label))\n    soUnit = dict(triplestore.subject_objects(hasUnit))\n    soCost = dict(triplestore.subject_objects(hasCost))\n    soAFun = dict(triplestore.subject_objects(hasAccessFunction))\n    soDVal = dict(triplestore.subject_objects(hasDataValue))\n\n    def getfunc(func_iri, default=None):\n        \"\"\"Returns callable function corresponding to `func_iri`.\n        Raises CannotGetFunctionError if func_iri cannot be found.\"\"\"\n        if func_iri is None:\n            return None\n        if func_iri in function_repo and function_repo[func_iri]:\n            return function_repo[func_iri]\n        try:\n            return (\n                triplestore._get_function(  # pylint: disable=protected-access\n                    func_iri\n                )\n            )\n        except CannotGetFunctionError:\n            return default\n\n    def getcost(target, stepname):\n        \"\"\"Returns the cost assigned to IRI `target` for a mapping step\n        of type `stepname`.\"\"\"\n        cost = soCost.get(target, default_costs[stepname])\n        if cost is None or callable(cost) or isinstance(cost, float):\n            return cost\n        return getfunc(cost, float(parse_literal(cost)))\n\n    def walk(target, visited, step):\n        \"\"\"Walk backward in rdf graph from `node` to sources.\"\"\"\n        if target in visited:\n            return\n        visited.add(target)\n\n        def addnode(node, steptype, stepname):\n            if node in visited:\n                return\n            step.steptype = steptype\n            step.cost = getcost(target, stepname)\n            if node in soAFun:\n                value = value_class(\n                    value=getfunc(soAFun[node]),\n                    unit=soUnit.get(node),\n                    iri=node,\n                    property_iri=soInst.get(node),\n                    cost=getcost(node, \"value\"),\n                )\n                step.add_input(value, name=soName.get(node))\n            elif node in soDVal:\n                literal = parse_literal(soDVal[node])\n                value = value_class(\n                    value=literal.to_python(),\n                    unit=soUnit.get(node),\n                    iri=node,\n                    property_iri=soInst.get(node),\n                    cost=getcost(node, \"value\"),\n                )\n                step.add_input(value, name=soName.get(node))\n            elif node in sources:\n                value = value_class(\n                    value=sources[node],\n                    unit=soUnit.get(node),\n                    iri=node,\n                    property_iri=soInst.get(node),\n                    cost=getcost(node, \"value\"),\n                )\n                step.add_input(value, name=soName.get(node))\n            else:\n                prevstep = mappingstep_class(\n                    output_iri=node,\n                    output_unit=soUnit.get(node),\n                    triplestore=triplestore,\n                )\n                step.add_input(prevstep, name=soName.get(node))\n                walk(node, visited, prevstep)\n\n        for node in osInst[target]:\n            addnode(node, StepType.INV_INSTANCEOF, \"instanceOf\")\n\n        for node in soMaps[target]:\n            addnode(node, StepType.MAPSTO, \"mapsTo\")\n\n        for node in osMaps[target]:\n            addnode(node, StepType.INV_MAPSTO, \"mapsTo\")\n\n        for node in osSubcl[target]:\n            addnode(node, StepType.INV_SUBCLASSOF, \"subClassOf\")\n\n        for fmap in function_mappers:\n            for func_iri, input_iris in fmap(triplestore)[target]:\n                step.steptype = StepType.FUNCTION\n                step.cost = getcost(func_iri, \"function\")\n                step.function = getfunc(func_iri)\n                step.join_mode = True\n                for input_iri in input_iris:\n                    step0 = mappingstep_class(\n                        output_iri=input_iri,\n                        output_unit=soUnit.get(input_iri),\n                        triplestore=triplestore,\n                    )\n                    step.add_input(step0, name=soName.get(input_iri))\n                    walk(input_iri, visited, step0)\n                step.join_input()\n\n    visited = set()\n    step = mappingstep_class(\n        output_iri=target,\n        output_unit=soUnit.get(target),\n        triplestore=triplestore,\n    )\n    if target in soInst:\n        # It is only initially we want to follow instanceOf in forward\n        # direction.  Later on we will only follow mapsTo and instanceOf in\n        # backward direction.\n        visited.add(target)  # do we really wan't this?  Yes, I think so...\n        source = soInst[target]\n        step.steptype = StepType.INSTANCEOF\n        step.cost = getcost(source, \"instanceOf\")\n        step0 = mappingstep_class(\n            output_iri=source,\n            output_unit=soUnit.get(source),\n            triplestore=triplestore,\n        )\n        step.add_input(step0, name=soName.get(target))\n        step = step0\n        target = source\n\n    if target not in soMaps:\n        raise MissingRelationError(f'Missing \"mapsTo\" relation on: {target}')\n    walk(target, visited, step)\n\n    return step\n</code></pre>"},{"location":"api_reference/units/units/","title":"units","text":"<p>A module for handling units using EMMO as the main resource.</p> <p>Pint is used for programatic unit conversions.</p>"},{"location":"api_reference/units/units/#tripper.units.units.Dimension","title":"<code> Dimension            (tuple)         </code>","text":"<p>SI quantity dimension</p>"},{"location":"api_reference/units/units/#tripper.units.units.Dimension.__getnewargs__","title":"<code>__getnewargs__(self)</code>  <code>special</code>","text":"<p>Return self as a plain tuple.  Used by copy and pickle.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def __getnewargs__(self):\n    'Return self as a plain tuple.  Used by copy and pickle.'\n    return _tuple(self)\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Dimension.__new__","title":"<code>__new__(_cls, T, L, M, I, H, N, J)</code>  <code>special</code> <code>staticmethod</code>","text":"<p>Create new instance of Dimension(T, L, M, I, H, N, J)</p>"},{"location":"api_reference/units/units/#tripper.units.units.Dimension.__repr__","title":"<code>__repr__(self)</code>  <code>special</code>","text":"<p>Return a nicely formatted representation string</p> Source code in <code>tripper/units/units.py</code> <pre><code>def __repr__(self):\n    'Return a nicely formatted representation string'\n    return self.__class__.__name__ + repr_fmt % self\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.InvalidDimensionStringError","title":"<code> InvalidDimensionStringError            (UnitError)         </code>","text":"<p>Invalid dimension string.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class InvalidDimensionStringError(UnitError):\n    \"\"\"Invalid dimension string.\"\"\"\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.MissingDimensionStringError","title":"<code> MissingDimensionStringError            (UnitError)         </code>","text":"<p>Unit does not have a dimensional string.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class MissingDimensionStringError(UnitError):\n    \"\"\"Unit does not have a dimensional string.\"\"\"\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.MissingQuantityError","title":"<code> MissingQuantityError            (UnitError)         </code>","text":"<p>Quantity not found in ontology.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class MissingQuantityError(UnitError):\n    \"\"\"Quantity not found in ontology.\"\"\"\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.MissingUnitError","title":"<code> MissingUnitError            (UnitError)         </code>","text":"<p>Unit not found in ontology.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class MissingUnitError(UnitError):\n    \"\"\"Unit not found in ontology.\"\"\"\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.NoDefaultUnitRegistryError","title":"<code> NoDefaultUnitRegistryError            (UnitError)         </code>","text":"<p>No default unit registry has been defined.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class NoDefaultUnitRegistryError(UnitError):\n    \"\"\"No default unit registry has been defined.\"\"\"\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Quantity","title":"<code> Quantity            (Quantity)         </code>","text":"<p>A subclass of pint.Quantity with support for tripper.units.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class Quantity(pint.Quantity):\n    \"\"\"A subclass of pint.Quantity with support for tripper.units.\"\"\"\n\n    def _get_dimension(self) -&gt; Dimension:\n        \"\"\"Return a Dimension object with the dimensionality of this\n        quantity.\n        \"\"\"\n\n        def asint(x):\n            \"\"\"Convert floats close to whole integers to int.\"\"\"\n            return int(x) if abs(int(x) - x) &lt; 1e-7 else x\n\n        q = self.to_base_units()\n        return Dimension(\n            *tuple(\n                asint(\n                    q.u.dimensionality.get(\n                        f\"[{getattr(Dimension, dim).__doc__}]\", 0\n                    )\n                )\n                for dim in Dimension._fields\n            )\n        )\n\n    def to_ontology_units(self) -&gt; \"Quantity\":\n        \"\"\"Return new quantity rescale to a unit with the same\n        dimensionality that exists in the ontology.\n\n        Notes:\n            This function tries to select the \"simplest\" unit among all the\n            units with compatible physical dimensionality in the ontology.\n\n            This is done according to the following heuristics:\n\n            1. Find units with compatible physical dimensionality in the\n               ontology.\n            2. Among these units, select the unit that minimises the absolute\n               value of the sum of the powers of each unit component.\n\n               Example: among the units\n\n                   Pa = Pa^1       -&gt; sum=1\n                   J/m^3 = J^1/m^3 -&gt; sum=1+3=4\n                   N/m^2 = N^1/m^2 -&gt; sum=1+2=3\n\n               Pa will be selected.\n            3. If two units have the same sum, the unit that minimises\n               `log10(magnitude/5)` is selected, where `magnitude` is the\n               magnitude of the quantity when expressed in SI base units.\n\n        \"\"\"\n        # pylint: disable=protected-access\n        ureg = self._REGISTRY\n\n        try:\n            return self.m * ureg.get_unit(symbol=f\"{self.u:~P}\")\n        except (MissingUnitError, pint.OffsetUnitCalculusError):\n            pass\n\n        units = ureg._get_tripper_units()\n        dim = self._get_dimension()\n        compatible_units = []\n        for info in units.units.values():\n            if info.dimension == dim:\n                q = self.to(info.name)\n                try:\n                    d = units._parse_unitname(info.name)\n                except MissingUnitError:\n                    d = 1.0, {q.u.name: 1}\n                compatible_units.append((info.name, q.m, d))\n\n        def sortkey(x):\n            \"\"\"Returns sort key for `compatible_units`.\"\"\"\n            # This function prioritise compact unit expression with\n            # small exponents.  Lower priority is given to pre-factor\n            # close to five.\n            _, m, (_, d) = x\n            return 100 * sum(abs(v) for v in d.values()) + abs(\n                math.log10(m / 5)\n            )\n\n        compatible_units.sort(key=sortkey)\n        name, mult, _ = compatible_units[0]\n        return ureg.Quantity(mult, name)\n\n    def ito_ontology_units(self) -&gt; None:\n        \"\"\"Inplace rescale to ontology units.\"\"\"\n        q = self.to_ontology_units()\n        self.ito(q.u)\n\n    dimension = property(\n        lambda self: self._get_dimension(),\n        doc=\"Named tuple with the SI dimensionality of the quantity.\",\n    )\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Quantity.dimension","title":"<code>dimension</code>  <code>property</code> <code>readonly</code>","text":"<p>Named tuple with the SI dimensionality of the quantity.</p>"},{"location":"api_reference/units/units/#tripper.units.units.Quantity.ito_ontology_units","title":"<code>ito_ontology_units(self)</code>","text":"<p>Inplace rescale to ontology units.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def ito_ontology_units(self) -&gt; None:\n    \"\"\"Inplace rescale to ontology units.\"\"\"\n    q = self.to_ontology_units()\n    self.ito(q.u)\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Quantity.to_ontology_units","title":"<code>to_ontology_units(self)</code>","text":"<p>Return new quantity rescale to a unit with the same dimensionality that exists in the ontology.</p> <p>Notes</p> <p>This function tries to select the \"simplest\" unit among all the units with compatible physical dimensionality in the ontology.</p> <p>This is done according to the following heuristics:</p> <ol> <li>Find units with compatible physical dimensionality in the    ontology.</li> <li>Among these units, select the unit that minimises the absolute    value of the sum of the powers of each unit component.</li> </ol> <p>Example: among the units</p> <pre><code>   Pa = Pa^1       -&gt; sum=1\n   J/m^3 = J^1/m^3 -&gt; sum=1+3=4\n   N/m^2 = N^1/m^2 -&gt; sum=1+2=3\n</code></pre> <p>Pa will be selected. 3. If two units have the same sum, the unit that minimises    <code>log10(magnitude/5)</code> is selected, where <code>magnitude</code> is the    magnitude of the quantity when expressed in SI base units.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def to_ontology_units(self) -&gt; \"Quantity\":\n    \"\"\"Return new quantity rescale to a unit with the same\n    dimensionality that exists in the ontology.\n\n    Notes:\n        This function tries to select the \"simplest\" unit among all the\n        units with compatible physical dimensionality in the ontology.\n\n        This is done according to the following heuristics:\n\n        1. Find units with compatible physical dimensionality in the\n           ontology.\n        2. Among these units, select the unit that minimises the absolute\n           value of the sum of the powers of each unit component.\n\n           Example: among the units\n\n               Pa = Pa^1       -&gt; sum=1\n               J/m^3 = J^1/m^3 -&gt; sum=1+3=4\n               N/m^2 = N^1/m^2 -&gt; sum=1+2=3\n\n           Pa will be selected.\n        3. If two units have the same sum, the unit that minimises\n           `log10(magnitude/5)` is selected, where `magnitude` is the\n           magnitude of the quantity when expressed in SI base units.\n\n    \"\"\"\n    # pylint: disable=protected-access\n    ureg = self._REGISTRY\n\n    try:\n        return self.m * ureg.get_unit(symbol=f\"{self.u:~P}\")\n    except (MissingUnitError, pint.OffsetUnitCalculusError):\n        pass\n\n    units = ureg._get_tripper_units()\n    dim = self._get_dimension()\n    compatible_units = []\n    for info in units.units.values():\n        if info.dimension == dim:\n            q = self.to(info.name)\n            try:\n                d = units._parse_unitname(info.name)\n            except MissingUnitError:\n                d = 1.0, {q.u.name: 1}\n            compatible_units.append((info.name, q.m, d))\n\n    def sortkey(x):\n        \"\"\"Returns sort key for `compatible_units`.\"\"\"\n        # This function prioritise compact unit expression with\n        # small exponents.  Lower priority is given to pre-factor\n        # close to five.\n        _, m, (_, d) = x\n        return 100 * sum(abs(v) for v in d.values()) + abs(\n            math.log10(m / 5)\n        )\n\n    compatible_units.sort(key=sortkey)\n    name, mult, _ = compatible_units[0]\n    return ureg.Quantity(mult, name)\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Unit","title":"<code> Unit            (Unit)         </code>","text":"<p>A subclass of pint.Unit with additional methods and properties.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class Unit(pint.Unit):\n    \"\"\"A subclass of pint.Unit with additional methods and properties.\"\"\"\n\n    def _get_info(self) -&gt; dict:\n        \"\"\"Return a dict with attribute access describing the unit.\n\n        SeeAlso:\n            tripper.units.UnitRegistry.get_unit_info()\n        \"\"\"\n        ureg = self._REGISTRY\n        return ureg.get_unit_info(str(self))\n\n    info = property(\n        lambda self: self._get_info(),\n        doc=\"Dict with attribute access describing this unit.\",\n    )\n    name = property(\n        lambda self: self._get_info().name,\n        doc=\"Preferred label of the unit in the ontology.\",\n    )\n    emmoIRI = property(\n        lambda self: self._get_info().emmoIRI,\n        doc=\"IRI of the unit in the EMMO ontology.\",\n    )\n    qudtIRI = property(\n        lambda self: self._get_info().qudtIRI,\n        doc=\"IRI of the unit in the QUDT ontology.\",\n    )\n    omIRI = property(\n        lambda self: self._get_info().omIRI,\n        doc=\"IRI of the unit in the OM ontology.\",\n    )\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Unit.emmoIRI","title":"<code>emmoIRI</code>  <code>property</code> <code>readonly</code>","text":"<p>IRI of the unit in the EMMO ontology.</p>"},{"location":"api_reference/units/units/#tripper.units.units.Unit.info","title":"<code>info</code>  <code>property</code> <code>readonly</code>","text":"<p>Dict with attribute access describing this unit.</p>"},{"location":"api_reference/units/units/#tripper.units.units.Unit.name","title":"<code>name</code>  <code>property</code> <code>readonly</code>","text":"<p>Preferred label of the unit in the ontology.</p>"},{"location":"api_reference/units/units/#tripper.units.units.Unit.omIRI","title":"<code>omIRI</code>  <code>property</code> <code>readonly</code>","text":"<p>IRI of the unit in the OM ontology.</p>"},{"location":"api_reference/units/units/#tripper.units.units.Unit.qudtIRI","title":"<code>qudtIRI</code>  <code>property</code> <code>readonly</code>","text":"<p>IRI of the unit in the QUDT ontology.</p>"},{"location":"api_reference/units/units/#tripper.units.units.UnitError","title":"<code> UnitError            (TripperError)         </code>","text":"<p>Base Error for units module.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class UnitError(TripperError):\n    \"\"\"Base Error for units module.\"\"\"\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry","title":"<code> UnitRegistry            (UnitRegistry)         </code>","text":"<p>A subclass of pint.UnitRegistry with support for loading units from ontologies.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class UnitRegistry(pint.UnitRegistry):\n    \"\"\"A subclass of pint.UnitRegistry with support for loading units\n    from ontologies.\n    \"\"\"\n\n    Unit: \"TypeAlias\" = Unit\n    Quantity: \"TypeAlias\" = Quantity\n\n    def __init__(\n        self,\n        *args: \"Any\",\n        ts: \"Optional[Triplestore]\" = None,\n        url: \"Optional[Union[str, Path]]\" = None,\n        format: \"Optional[str]\" = None,  # pylint: disable=redefined-builtin\n        name: \"Optional[str]\" = None,\n        formalisation: str = \"emmo\",\n        include_prefixed: bool = False,\n        cache: \"Optional[bool]\" = True,\n        **kwargs: \"Any\",\n    ) -&gt; None:\n        # FIXME: Remove doctest SKIP comments when support for\n        # Python 3.8 is dropped\n        \"\"\"Initialise a Units class from triplestore `ts`\n\n        Arguments:\n            args: Positional arguments passed to pint.UnitRegistry().\n            ts: Triplestore object containing the ontology to load\n                units from.\n            url: URL (or path) to triplestore from where to load the unit\n                definitions if `ts` is not given.\n                Default: \"https://w3id.org/emmo/{EMMO_VERSION}\"\n            format: Optional format of the source referred to by `url`.\n            name: A (versioned) name for the triplestore. Used for caching.\n                Ex: \"emmo-1.0.0\".\n            formalisation: The ontological formalisation with which units\n                and quantities are represented. Currently only \"emmo\" is\n                supported.\n            include_prefixed: Whether to also include prefixed units.\n            cache: Whether to cache the unit table. If `cache` is:\n                - True: Load cache if it exists, otherwise create new cache.\n                - False: Don't load cache, but (over)write new cache.\n                - None: Don't use cache.\n            kwargs: Keyword arguments passed to pint.UnitRegistry().\n\n        Examples:\n            &gt;&gt;&gt; from tripper.units import UnitRegistry\n            &gt;&gt;&gt; ureg = UnitRegistry()\n            &gt;&gt;&gt; u = ureg.Metre\n            &gt;&gt;&gt; u\n            &lt;Unit('Metre')&gt;\n\n            &gt;&gt;&gt; u.emmoIRI  # doctest: +SKIP\n            'https://w3id.org/emmo#Metre'\n\n            &gt;&gt;&gt; q = ureg.Quantity(\"3 h\")\n            &gt;&gt;&gt; q\n            &lt;Quantity(3, 'Hour')&gt;\n\n            &gt;&gt;&gt; q.u.qudtIRI  # doctest: +SKIP\n            'http://qudt.org/vocab/unit/HR'\n\n        \"\"\"\n        url, name = _default_url_name(url, name)\n\n        self._tripper_cachedir = get_cachedir(create=cache is not None)\n        self._tripper_units = None\n        self._tripper_unitsargs = {\n            \"ts\": ts,\n            \"url\": url,\n            \"format\": format,\n            \"name\": name,\n            \"formalisation\": formalisation,\n            \"include_prefixed\": include_prefixed,\n            \"cache\": cache,\n        }\n        unitsfile = self._tripper_cachedir / f\"units-{name}.txt\"\n        self._tripper_unitsfile = unitsfile\n\n        # If no explicit `filename` is not provided, ensure that an unit\n        # definition file has been created and assign `filename` to it.\n        if \"filename\" not in kwargs:\n            kwargs[\"filename\"] = unitsfile\n            if not unitsfile.exists():\n                units = self._get_tripper_units()\n                units.write_pint_units(unitsfile)\n\n        if cache:\n            kwargs.setdefault(\"cache_folder\", self._tripper_cachedir)\n\n        super().__init__(*args, **kwargs)\n\n    def parse_units(  # pylint: disable=arguments-differ\n        self, units, *args, **kwargs\n    ):\n        try:\n            return super().parse_units(units, *args, **kwargs)\n        except pint.UndefinedUnitError:\n            pass\n        info = self.get_unit_info(symbol=units)\n        return super().parse_units(info.name, *args, **kwargs)\n\n    def _get_tripper_units(self) -&gt; Units:\n        \"\"\"Returns a tripper.units.Units instance for the current ontology.\"\"\"\n        if not self._tripper_units:\n            self._tripper_units = Units(\n                **self._tripper_unitsargs  # type: ignore\n            )\n        return self._tripper_units  # type: ignore\n\n    def get_unit(\n        self,\n        name: \"Optional[str]\" = None,\n        symbol: \"Optional[str]\" = None,\n        iri: \"Optional[str]\" = None,\n        unitCode: \"Optional[str]\" = None,\n    ) -&gt; \"Unit\":\n        \"\"\"Return Pint unit matching the unit name, symbol, iri or unitCode\n         as defined in the ontology.\n\n        Arguments:\n            name: Search for unit by name (prefLabel). May also be an IRI.\n                Ex: \"Ampere\".\n            symbol: Search for unit by symbol or UCUM code. Ex: \"A\", \"km\"\n            iri: Search for unit by IRI.\n            unitCode: Search for unit by UNECE common code. Ex: \"MTS\"\n\n        Returns:\n            Return matching Pint unit.\n        \"\"\"\n        info = self.get_unit_info(\n            name=name, symbol=symbol, iri=iri, unitCode=unitCode\n        )\n        return self(info.name).u\n\n    def get_unit_info(\n        self,\n        name: \"Optional[str]\" = None,\n        symbol: \"Optional[str]\" = None,\n        iri: \"Optional[str]\" = None,\n        unitCode: \"Optional[str]\" = None,\n    ) -&gt; AttrDict:\n        \"\"\"Return information about a unit name, symbol, iri or unitCode\n         as defined in the ontology.\n\n        Arguments:\n            name: Search for unit by name (prefLabel). May also be an IRI.\n                Ex: \"Ampere\"\n            symbol: Search for unit by symbol or UCUM code. Ex: \"A\", \"km\"\n            iri: Search for unit by IRI.\n            unitCode: Search for unit by UNECE common code. Ex: \"MTS\"\n\n        Returns:\n            dict: A dict with attribute access describing the unit.\n            The dict has the following keys:\n\n            - name: Preferred label.\n            - description: Unit description.\n            - aliases: List of alternative labels.\n            - symbols: List with unit symbols.\n            - dimension: Named tuple with quantity dimension.\n            - emmoIRI: IRI of the unit in the EMMO ontology.\n            - qudtIRI: IRI of the unit in the QUDT ontology.\n            - omIRI: IRI of the unit in the OM ontology.\n            - ucumCodes: List of UCUM codes for the unit.\n            - unitCode: UNECE common code for the unit.\n            - multiplier: Unit multiplier for converting to the\n              corresponding SI unit.\n            - offset: Unit offset for converting to the corresponding SI unit.\n\n        \"\"\"\n        units = self._get_tripper_units()\n        return units.get_unit(\n            name=name, symbol=symbol, iri=iri, unitCode=unitCode\n        )\n\n    def get_quantity(\n        self,\n        name: \"Optional[str]\" = None,\n        iri: \"Optional[str]\" = None,\n        iso80000Ref: \"Optional[str]\" = None,\n        value: float = 1.0,\n    ) -&gt; \"Quantity\":\n        \"\"\"Convert quantity name, iri or iso80000ref in the ontology to a Pint\n        quantity object.\n\n        Arguments:\n            name: Access quantity by its name (skos:prefLabel in the ontology).\n                Ex: \"Energy\".\n            iri: Access quantity by its IRI, which may refer to EMMO, QUDT,\n                OM or IUPAC.\n            iso80000Ref: Access quantity by its ISO80000 reference. Ex: 5-20-1\n            value: Value of the quantity value in units of SI base units.\n\n        \"\"\"\n        # pylint: disable=too-many-branches\n        units = self._get_tripper_units()\n        if name:\n            if name not in units.quantities:\n                raise MissingQuantityError(name)\n            info = units.quantities[name]\n        elif iri:\n            iri_attr = \"emmoIRI\", \"qudtIRI\", \"omIRI\", \"iupacIRI\"\n            found = False\n            for info in units.quantities.values():\n                for attr in iri_attr:\n                    if info[attr] == iri:\n                        found = True\n                        break\n                if found:\n                    break\n            else:\n                raise MissingQuantityError(f\"iri={iri}\")\n        elif iso80000Ref:\n            for info in units.quantities.values():\n                if info.iso80000Ref == iso80000Ref:\n                    break\n            else:\n                raise MissingQuantityError(f\"iso80000Ref={iso80000Ref}\")\n        else:\n            raise ValueError(\n                \"Missing argument to get_quantity(). Either `name`, \"\n                \"`iri` or `iso80000Ref` must be provided\"\n            )\n        q = self.Quantity(value, base_unit_expression(info.dimension))\n        return q.to_ontology_units()\n\n    def load_quantity(self, ts: Triplestore, iri: str) -&gt; Quantity:\n        \"\"\"Load quantity individual or (sub)class from the triplestore.\n\n        Arguments:\n            ts: Triplestore to load from.\n            iri: IRI of quantity individual to load.\n\n        Returns:\n            Quantity: Pint representation of the quantity.\n\n        Notes:\n            The quantity must have a value, hence EMMO.Energy will return\n            and exception since it is defined without a value in the ontology.\n        \"\"\"\n        value, unit = load_emmo_quantity(ts, iri)\n        return self.Quantity(\n            value, self.get_unit(iri=unit) if \":\" in unit else unit\n        )\n\n    def save_quantity(\n        self,\n        ts: Triplestore,\n        q: Quantity,\n        iri: str,\n        type: \"Optional[str]\" = None,  # pylint: disable=redefined-builtin\n        tbox: bool = False,\n        si_datatype: bool = True,\n        annotations: \"Optional[dict]\" = None,\n    ) -&gt; None:\n        \"\"\"Save a Pint quantity individual to triplestore.\n\n        Arguments:\n            ts: Triplestore to save to.\n            q: Quantity to save.\n            iri: IRI of the quantity in the triplestore.\n            type: IRI of the type or superclass (if `tbox` is true) of the\n                quantity.\n            tbox: Whether to document the quantity in the tbox.\n            si_datatype: Whether to represent the value using the\n                `emmo:SIQuantityDatatype` datatype.\n            annotations: Additional annotations describing the quantity.\n                Use Literal() for literal annotations.\n\n        \"\"\"\n        save_emmo_quantity(\n            ts=ts,\n            q=q,\n            iri=iri,\n            type=type,\n            tbox=tbox,\n            si_datatype=si_datatype,\n            annotations=annotations,\n        )\n\n    def clear_cache(self):\n        \"\"\"Clear caches related to this unit registry.\"\"\"\n        if self._tripper_unitsfile.exists():\n            self._tripper_unitsfile.unlink()\n\n        name = self._tripper_unitsargs[\"name\"]\n        include_prefixed = self._tripper_unitsargs[\"include_prefixed\"]\n        pf = \"-prefixed\" if include_prefixed else \"\"\n        cachedir = self._tripper_cachedir\n        ontofile = cachedir / f\"units-{name}.ntriples\"\n        unitsfile = cachedir / f\"units{pf}-{name}.pickle\"\n        if ontofile.exists():\n            ontofile.unlink()\n        if unitsfile.exists():\n            unitsfile.unlink()\n\n    def set_as_default(self) -&gt; \"Union[UnitRegistry, None]\":\n        \"\"\"Set current unit registry as the default one.\n\n        This unit registry can then be accessed with `get_ureg()`.\n\n        Returns the previous default unit registry.\"\"\"\n        global _unit_reg  # pylint: disable=global-statement\n        old = _unit_reg\n        _unit_reg = self\n        return old\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.Quantity","title":"<code> Quantity            (Quantity)         </code>","text":"<p>A subclass of pint.Quantity with support for tripper.units.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class Quantity(pint.Quantity):\n    \"\"\"A subclass of pint.Quantity with support for tripper.units.\"\"\"\n\n    def _get_dimension(self) -&gt; Dimension:\n        \"\"\"Return a Dimension object with the dimensionality of this\n        quantity.\n        \"\"\"\n\n        def asint(x):\n            \"\"\"Convert floats close to whole integers to int.\"\"\"\n            return int(x) if abs(int(x) - x) &lt; 1e-7 else x\n\n        q = self.to_base_units()\n        return Dimension(\n            *tuple(\n                asint(\n                    q.u.dimensionality.get(\n                        f\"[{getattr(Dimension, dim).__doc__}]\", 0\n                    )\n                )\n                for dim in Dimension._fields\n            )\n        )\n\n    def to_ontology_units(self) -&gt; \"Quantity\":\n        \"\"\"Return new quantity rescale to a unit with the same\n        dimensionality that exists in the ontology.\n\n        Notes:\n            This function tries to select the \"simplest\" unit among all the\n            units with compatible physical dimensionality in the ontology.\n\n            This is done according to the following heuristics:\n\n            1. Find units with compatible physical dimensionality in the\n               ontology.\n            2. Among these units, select the unit that minimises the absolute\n               value of the sum of the powers of each unit component.\n\n               Example: among the units\n\n                   Pa = Pa^1       -&gt; sum=1\n                   J/m^3 = J^1/m^3 -&gt; sum=1+3=4\n                   N/m^2 = N^1/m^2 -&gt; sum=1+2=3\n\n               Pa will be selected.\n            3. If two units have the same sum, the unit that minimises\n               `log10(magnitude/5)` is selected, where `magnitude` is the\n               magnitude of the quantity when expressed in SI base units.\n\n        \"\"\"\n        # pylint: disable=protected-access\n        ureg = self._REGISTRY\n\n        try:\n            return self.m * ureg.get_unit(symbol=f\"{self.u:~P}\")\n        except (MissingUnitError, pint.OffsetUnitCalculusError):\n            pass\n\n        units = ureg._get_tripper_units()\n        dim = self._get_dimension()\n        compatible_units = []\n        for info in units.units.values():\n            if info.dimension == dim:\n                q = self.to(info.name)\n                try:\n                    d = units._parse_unitname(info.name)\n                except MissingUnitError:\n                    d = 1.0, {q.u.name: 1}\n                compatible_units.append((info.name, q.m, d))\n\n        def sortkey(x):\n            \"\"\"Returns sort key for `compatible_units`.\"\"\"\n            # This function prioritise compact unit expression with\n            # small exponents.  Lower priority is given to pre-factor\n            # close to five.\n            _, m, (_, d) = x\n            return 100 * sum(abs(v) for v in d.values()) + abs(\n                math.log10(m / 5)\n            )\n\n        compatible_units.sort(key=sortkey)\n        name, mult, _ = compatible_units[0]\n        return ureg.Quantity(mult, name)\n\n    def ito_ontology_units(self) -&gt; None:\n        \"\"\"Inplace rescale to ontology units.\"\"\"\n        q = self.to_ontology_units()\n        self.ito(q.u)\n\n    dimension = property(\n        lambda self: self._get_dimension(),\n        doc=\"Named tuple with the SI dimensionality of the quantity.\",\n    )\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.Quantity.dimension","title":"<code>dimension</code>  <code>property</code> <code>readonly</code>","text":"<p>Named tuple with the SI dimensionality of the quantity.</p>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.Quantity.ito_ontology_units","title":"<code>ito_ontology_units(self)</code>","text":"<p>Inplace rescale to ontology units.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def ito_ontology_units(self) -&gt; None:\n    \"\"\"Inplace rescale to ontology units.\"\"\"\n    q = self.to_ontology_units()\n    self.ito(q.u)\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.Quantity.to_ontology_units","title":"<code>to_ontology_units(self)</code>","text":"<p>Return new quantity rescale to a unit with the same dimensionality that exists in the ontology.</p> <p>Notes</p> <p>This function tries to select the \"simplest\" unit among all the units with compatible physical dimensionality in the ontology.</p> <p>This is done according to the following heuristics:</p> <ol> <li>Find units with compatible physical dimensionality in the    ontology.</li> <li>Among these units, select the unit that minimises the absolute    value of the sum of the powers of each unit component.</li> </ol> <p>Example: among the units</p> <pre><code>   Pa = Pa^1       -&gt; sum=1\n   J/m^3 = J^1/m^3 -&gt; sum=1+3=4\n   N/m^2 = N^1/m^2 -&gt; sum=1+2=3\n</code></pre> <p>Pa will be selected. 3. If two units have the same sum, the unit that minimises    <code>log10(magnitude/5)</code> is selected, where <code>magnitude</code> is the    magnitude of the quantity when expressed in SI base units.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def to_ontology_units(self) -&gt; \"Quantity\":\n    \"\"\"Return new quantity rescale to a unit with the same\n    dimensionality that exists in the ontology.\n\n    Notes:\n        This function tries to select the \"simplest\" unit among all the\n        units with compatible physical dimensionality in the ontology.\n\n        This is done according to the following heuristics:\n\n        1. Find units with compatible physical dimensionality in the\n           ontology.\n        2. Among these units, select the unit that minimises the absolute\n           value of the sum of the powers of each unit component.\n\n           Example: among the units\n\n               Pa = Pa^1       -&gt; sum=1\n               J/m^3 = J^1/m^3 -&gt; sum=1+3=4\n               N/m^2 = N^1/m^2 -&gt; sum=1+2=3\n\n           Pa will be selected.\n        3. If two units have the same sum, the unit that minimises\n           `log10(magnitude/5)` is selected, where `magnitude` is the\n           magnitude of the quantity when expressed in SI base units.\n\n    \"\"\"\n    # pylint: disable=protected-access\n    ureg = self._REGISTRY\n\n    try:\n        return self.m * ureg.get_unit(symbol=f\"{self.u:~P}\")\n    except (MissingUnitError, pint.OffsetUnitCalculusError):\n        pass\n\n    units = ureg._get_tripper_units()\n    dim = self._get_dimension()\n    compatible_units = []\n    for info in units.units.values():\n        if info.dimension == dim:\n            q = self.to(info.name)\n            try:\n                d = units._parse_unitname(info.name)\n            except MissingUnitError:\n                d = 1.0, {q.u.name: 1}\n            compatible_units.append((info.name, q.m, d))\n\n    def sortkey(x):\n        \"\"\"Returns sort key for `compatible_units`.\"\"\"\n        # This function prioritise compact unit expression with\n        # small exponents.  Lower priority is given to pre-factor\n        # close to five.\n        _, m, (_, d) = x\n        return 100 * sum(abs(v) for v in d.values()) + abs(\n            math.log10(m / 5)\n        )\n\n    compatible_units.sort(key=sortkey)\n    name, mult, _ = compatible_units[0]\n    return ureg.Quantity(mult, name)\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.Unit","title":"<code> Unit            (Unit)         </code>","text":"<p>A subclass of pint.Unit with additional methods and properties.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class Unit(pint.Unit):\n    \"\"\"A subclass of pint.Unit with additional methods and properties.\"\"\"\n\n    def _get_info(self) -&gt; dict:\n        \"\"\"Return a dict with attribute access describing the unit.\n\n        SeeAlso:\n            tripper.units.UnitRegistry.get_unit_info()\n        \"\"\"\n        ureg = self._REGISTRY\n        return ureg.get_unit_info(str(self))\n\n    info = property(\n        lambda self: self._get_info(),\n        doc=\"Dict with attribute access describing this unit.\",\n    )\n    name = property(\n        lambda self: self._get_info().name,\n        doc=\"Preferred label of the unit in the ontology.\",\n    )\n    emmoIRI = property(\n        lambda self: self._get_info().emmoIRI,\n        doc=\"IRI of the unit in the EMMO ontology.\",\n    )\n    qudtIRI = property(\n        lambda self: self._get_info().qudtIRI,\n        doc=\"IRI of the unit in the QUDT ontology.\",\n    )\n    omIRI = property(\n        lambda self: self._get_info().omIRI,\n        doc=\"IRI of the unit in the OM ontology.\",\n    )\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.Unit.emmoIRI","title":"<code>emmoIRI</code>  <code>property</code> <code>readonly</code>","text":"<p>IRI of the unit in the EMMO ontology.</p>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.Unit.info","title":"<code>info</code>  <code>property</code> <code>readonly</code>","text":"<p>Dict with attribute access describing this unit.</p>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.Unit.name","title":"<code>name</code>  <code>property</code> <code>readonly</code>","text":"<p>Preferred label of the unit in the ontology.</p>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.Unit.omIRI","title":"<code>omIRI</code>  <code>property</code> <code>readonly</code>","text":"<p>IRI of the unit in the OM ontology.</p>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.Unit.qudtIRI","title":"<code>qudtIRI</code>  <code>property</code> <code>readonly</code>","text":"<p>IRI of the unit in the QUDT ontology.</p>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.__init__","title":"<code>__init__(self, *args, *, ts=None, url=None, format=None, name=None, formalisation='emmo', include_prefixed=False, cache=True, **kwargs)</code>  <code>special</code>","text":"<p>Initialise a Units class from triplestore <code>ts</code></p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Any</code> <p>Positional arguments passed to pint.UnitRegistry().</p> <code>()</code> <code>ts</code> <code>Optional[Triplestore]</code> <p>Triplestore object containing the ontology to load units from.</p> <code>None</code> <code>url</code> <code>Optional[Union[str, Path]]</code> <p>URL (or path) to triplestore from where to load the unit definitions if <code>ts</code> is not given. Default: \"https://w3id.org/emmo/{EMMO_VERSION}\"</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Optional format of the source referred to by <code>url</code>.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>A (versioned) name for the triplestore. Used for caching. Ex: \"emmo-1.0.0\".</p> <code>None</code> <code>formalisation</code> <code>str</code> <p>The ontological formalisation with which units and quantities are represented. Currently only \"emmo\" is supported.</p> <code>'emmo'</code> <code>include_prefixed</code> <code>bool</code> <p>Whether to also include prefixed units.</p> <code>False</code> <code>cache</code> <code>Optional[bool]</code> <p>Whether to cache the unit table. If <code>cache</code> is: - True: Load cache if it exists, otherwise create new cache. - False: Don't load cache, but (over)write new cache. - None: Don't use cache.</p> <code>True</code> <code>kwargs</code> <code>Any</code> <p>Keyword arguments passed to pint.UnitRegistry().</p> <code>{}</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from tripper.units import UnitRegistry\n&gt;&gt;&gt; ureg = UnitRegistry()\n&gt;&gt;&gt; u = ureg.Metre\n&gt;&gt;&gt; u\n&lt;Unit('Metre')&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; u.emmoIRI\n'https://w3id.org/emmo#Metre'\n</code></pre> <pre><code>&gt;&gt;&gt; q = ureg.Quantity(\"3 h\")\n&gt;&gt;&gt; q\n&lt;Quantity(3, 'Hour')&gt;\n</code></pre> <pre><code>&gt;&gt;&gt; q.u.qudtIRI\n'http://qudt.org/vocab/unit/HR'\n</code></pre> Source code in <code>tripper/units/units.py</code> <pre><code>def __init__(\n    self,\n    *args: \"Any\",\n    ts: \"Optional[Triplestore]\" = None,\n    url: \"Optional[Union[str, Path]]\" = None,\n    format: \"Optional[str]\" = None,  # pylint: disable=redefined-builtin\n    name: \"Optional[str]\" = None,\n    formalisation: str = \"emmo\",\n    include_prefixed: bool = False,\n    cache: \"Optional[bool]\" = True,\n    **kwargs: \"Any\",\n) -&gt; None:\n    # FIXME: Remove doctest SKIP comments when support for\n    # Python 3.8 is dropped\n    \"\"\"Initialise a Units class from triplestore `ts`\n\n    Arguments:\n        args: Positional arguments passed to pint.UnitRegistry().\n        ts: Triplestore object containing the ontology to load\n            units from.\n        url: URL (or path) to triplestore from where to load the unit\n            definitions if `ts` is not given.\n            Default: \"https://w3id.org/emmo/{EMMO_VERSION}\"\n        format: Optional format of the source referred to by `url`.\n        name: A (versioned) name for the triplestore. Used for caching.\n            Ex: \"emmo-1.0.0\".\n        formalisation: The ontological formalisation with which units\n            and quantities are represented. Currently only \"emmo\" is\n            supported.\n        include_prefixed: Whether to also include prefixed units.\n        cache: Whether to cache the unit table. If `cache` is:\n            - True: Load cache if it exists, otherwise create new cache.\n            - False: Don't load cache, but (over)write new cache.\n            - None: Don't use cache.\n        kwargs: Keyword arguments passed to pint.UnitRegistry().\n\n    Examples:\n        &gt;&gt;&gt; from tripper.units import UnitRegistry\n        &gt;&gt;&gt; ureg = UnitRegistry()\n        &gt;&gt;&gt; u = ureg.Metre\n        &gt;&gt;&gt; u\n        &lt;Unit('Metre')&gt;\n\n        &gt;&gt;&gt; u.emmoIRI  # doctest: +SKIP\n        'https://w3id.org/emmo#Metre'\n\n        &gt;&gt;&gt; q = ureg.Quantity(\"3 h\")\n        &gt;&gt;&gt; q\n        &lt;Quantity(3, 'Hour')&gt;\n\n        &gt;&gt;&gt; q.u.qudtIRI  # doctest: +SKIP\n        'http://qudt.org/vocab/unit/HR'\n\n    \"\"\"\n    url, name = _default_url_name(url, name)\n\n    self._tripper_cachedir = get_cachedir(create=cache is not None)\n    self._tripper_units = None\n    self._tripper_unitsargs = {\n        \"ts\": ts,\n        \"url\": url,\n        \"format\": format,\n        \"name\": name,\n        \"formalisation\": formalisation,\n        \"include_prefixed\": include_prefixed,\n        \"cache\": cache,\n    }\n    unitsfile = self._tripper_cachedir / f\"units-{name}.txt\"\n    self._tripper_unitsfile = unitsfile\n\n    # If no explicit `filename` is not provided, ensure that an unit\n    # definition file has been created and assign `filename` to it.\n    if \"filename\" not in kwargs:\n        kwargs[\"filename\"] = unitsfile\n        if not unitsfile.exists():\n            units = self._get_tripper_units()\n            units.write_pint_units(unitsfile)\n\n    if cache:\n        kwargs.setdefault(\"cache_folder\", self._tripper_cachedir)\n\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.clear_cache","title":"<code>clear_cache(self)</code>","text":"<p>Clear caches related to this unit registry.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear caches related to this unit registry.\"\"\"\n    if self._tripper_unitsfile.exists():\n        self._tripper_unitsfile.unlink()\n\n    name = self._tripper_unitsargs[\"name\"]\n    include_prefixed = self._tripper_unitsargs[\"include_prefixed\"]\n    pf = \"-prefixed\" if include_prefixed else \"\"\n    cachedir = self._tripper_cachedir\n    ontofile = cachedir / f\"units-{name}.ntriples\"\n    unitsfile = cachedir / f\"units{pf}-{name}.pickle\"\n    if ontofile.exists():\n        ontofile.unlink()\n    if unitsfile.exists():\n        unitsfile.unlink()\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.get_quantity","title":"<code>get_quantity(self, name=None, iri=None, iso80000Ref=None, value=1.0)</code>","text":"<p>Convert quantity name, iri or iso80000ref in the ontology to a Pint quantity object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Access quantity by its name (skos:prefLabel in the ontology). Ex: \"Energy\".</p> <code>None</code> <code>iri</code> <code>Optional[str]</code> <p>Access quantity by its IRI, which may refer to EMMO, QUDT, OM or IUPAC.</p> <code>None</code> <code>iso80000Ref</code> <code>Optional[str]</code> <p>Access quantity by its ISO80000 reference. Ex: 5-20-1</p> <code>None</code> <code>value</code> <code>float</code> <p>Value of the quantity value in units of SI base units.</p> <code>1.0</code> Source code in <code>tripper/units/units.py</code> <pre><code>def get_quantity(\n    self,\n    name: \"Optional[str]\" = None,\n    iri: \"Optional[str]\" = None,\n    iso80000Ref: \"Optional[str]\" = None,\n    value: float = 1.0,\n) -&gt; \"Quantity\":\n    \"\"\"Convert quantity name, iri or iso80000ref in the ontology to a Pint\n    quantity object.\n\n    Arguments:\n        name: Access quantity by its name (skos:prefLabel in the ontology).\n            Ex: \"Energy\".\n        iri: Access quantity by its IRI, which may refer to EMMO, QUDT,\n            OM or IUPAC.\n        iso80000Ref: Access quantity by its ISO80000 reference. Ex: 5-20-1\n        value: Value of the quantity value in units of SI base units.\n\n    \"\"\"\n    # pylint: disable=too-many-branches\n    units = self._get_tripper_units()\n    if name:\n        if name not in units.quantities:\n            raise MissingQuantityError(name)\n        info = units.quantities[name]\n    elif iri:\n        iri_attr = \"emmoIRI\", \"qudtIRI\", \"omIRI\", \"iupacIRI\"\n        found = False\n        for info in units.quantities.values():\n            for attr in iri_attr:\n                if info[attr] == iri:\n                    found = True\n                    break\n            if found:\n                break\n        else:\n            raise MissingQuantityError(f\"iri={iri}\")\n    elif iso80000Ref:\n        for info in units.quantities.values():\n            if info.iso80000Ref == iso80000Ref:\n                break\n        else:\n            raise MissingQuantityError(f\"iso80000Ref={iso80000Ref}\")\n    else:\n        raise ValueError(\n            \"Missing argument to get_quantity(). Either `name`, \"\n            \"`iri` or `iso80000Ref` must be provided\"\n        )\n    q = self.Quantity(value, base_unit_expression(info.dimension))\n    return q.to_ontology_units()\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.get_unit","title":"<code>get_unit(self, name=None, symbol=None, iri=None, unitCode=None)</code>","text":"<p>Return Pint unit matching the unit name, symbol, iri or unitCode  as defined in the ontology.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Search for unit by name (prefLabel). May also be an IRI. Ex: \"Ampere\".</p> <code>None</code> <code>symbol</code> <code>Optional[str]</code> <p>Search for unit by symbol or UCUM code. Ex: \"A\", \"km\"</p> <code>None</code> <code>iri</code> <code>Optional[str]</code> <p>Search for unit by IRI.</p> <code>None</code> <code>unitCode</code> <code>Optional[str]</code> <p>Search for unit by UNECE common code. Ex: \"MTS\"</p> <code>None</code> <p>Returns:</p> Type Description <code>Unit</code> <p>Return matching Pint unit.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def get_unit(\n    self,\n    name: \"Optional[str]\" = None,\n    symbol: \"Optional[str]\" = None,\n    iri: \"Optional[str]\" = None,\n    unitCode: \"Optional[str]\" = None,\n) -&gt; \"Unit\":\n    \"\"\"Return Pint unit matching the unit name, symbol, iri or unitCode\n     as defined in the ontology.\n\n    Arguments:\n        name: Search for unit by name (prefLabel). May also be an IRI.\n            Ex: \"Ampere\".\n        symbol: Search for unit by symbol or UCUM code. Ex: \"A\", \"km\"\n        iri: Search for unit by IRI.\n        unitCode: Search for unit by UNECE common code. Ex: \"MTS\"\n\n    Returns:\n        Return matching Pint unit.\n    \"\"\"\n    info = self.get_unit_info(\n        name=name, symbol=symbol, iri=iri, unitCode=unitCode\n    )\n    return self(info.name).u\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.get_unit_info","title":"<code>get_unit_info(self, name=None, symbol=None, iri=None, unitCode=None)</code>","text":"<p>Return information about a unit name, symbol, iri or unitCode  as defined in the ontology.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Search for unit by name (prefLabel). May also be an IRI. Ex: \"Ampere\"</p> <code>None</code> <code>symbol</code> <code>Optional[str]</code> <p>Search for unit by symbol or UCUM code. Ex: \"A\", \"km\"</p> <code>None</code> <code>iri</code> <code>Optional[str]</code> <p>Search for unit by IRI.</p> <code>None</code> <code>unitCode</code> <code>Optional[str]</code> <p>Search for unit by UNECE common code. Ex: \"MTS\"</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict with attribute access describing the unit. The dict has the following keys:</p> <ul> <li>name: Preferred label.</li> <li>description: Unit description.</li> <li>aliases: List of alternative labels.</li> <li>symbols: List with unit symbols.</li> <li>dimension: Named tuple with quantity dimension.</li> <li>emmoIRI: IRI of the unit in the EMMO ontology.</li> <li>qudtIRI: IRI of the unit in the QUDT ontology.</li> <li>omIRI: IRI of the unit in the OM ontology.</li> <li>ucumCodes: List of UCUM codes for the unit.</li> <li>unitCode: UNECE common code for the unit.</li> <li>multiplier: Unit multiplier for converting to the   corresponding SI unit.</li> <li>offset: Unit offset for converting to the corresponding SI unit.</li> </ul> Source code in <code>tripper/units/units.py</code> <pre><code>def get_unit_info(\n    self,\n    name: \"Optional[str]\" = None,\n    symbol: \"Optional[str]\" = None,\n    iri: \"Optional[str]\" = None,\n    unitCode: \"Optional[str]\" = None,\n) -&gt; AttrDict:\n    \"\"\"Return information about a unit name, symbol, iri or unitCode\n     as defined in the ontology.\n\n    Arguments:\n        name: Search for unit by name (prefLabel). May also be an IRI.\n            Ex: \"Ampere\"\n        symbol: Search for unit by symbol or UCUM code. Ex: \"A\", \"km\"\n        iri: Search for unit by IRI.\n        unitCode: Search for unit by UNECE common code. Ex: \"MTS\"\n\n    Returns:\n        dict: A dict with attribute access describing the unit.\n        The dict has the following keys:\n\n        - name: Preferred label.\n        - description: Unit description.\n        - aliases: List of alternative labels.\n        - symbols: List with unit symbols.\n        - dimension: Named tuple with quantity dimension.\n        - emmoIRI: IRI of the unit in the EMMO ontology.\n        - qudtIRI: IRI of the unit in the QUDT ontology.\n        - omIRI: IRI of the unit in the OM ontology.\n        - ucumCodes: List of UCUM codes for the unit.\n        - unitCode: UNECE common code for the unit.\n        - multiplier: Unit multiplier for converting to the\n          corresponding SI unit.\n        - offset: Unit offset for converting to the corresponding SI unit.\n\n    \"\"\"\n    units = self._get_tripper_units()\n    return units.get_unit(\n        name=name, symbol=symbol, iri=iri, unitCode=unitCode\n    )\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.load_quantity","title":"<code>load_quantity(self, ts, iri)</code>","text":"<p>Load quantity individual or (sub)class from the triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to load from.</p> required <code>iri</code> <code>str</code> <p>IRI of quantity individual to load.</p> required <p>Returns:</p> Type Description <code>Quantity</code> <p>Pint representation of the quantity.</p> <p>Notes</p> <p>The quantity must have a value, hence EMMO.Energy will return and exception since it is defined without a value in the ontology.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def load_quantity(self, ts: Triplestore, iri: str) -&gt; Quantity:\n    \"\"\"Load quantity individual or (sub)class from the triplestore.\n\n    Arguments:\n        ts: Triplestore to load from.\n        iri: IRI of quantity individual to load.\n\n    Returns:\n        Quantity: Pint representation of the quantity.\n\n    Notes:\n        The quantity must have a value, hence EMMO.Energy will return\n        and exception since it is defined without a value in the ontology.\n    \"\"\"\n    value, unit = load_emmo_quantity(ts, iri)\n    return self.Quantity(\n        value, self.get_unit(iri=unit) if \":\" in unit else unit\n    )\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.parse_units","title":"<code>parse_units(self, units, *args, **kwargs)</code>","text":"<p>Parse a units expression and returns a UnitContainer with the canonical names.</p> <p>The expression can only contain products, ratios and powers of units.</p>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.parse_units--parameters","title":"Parameters","text":"<p>input_string : str as_delta : bool or None     if the expression has multiple units, the parser will     interpret non multiplicative units as their <code>delta_</code> counterparts. (Default value = None) case_sensitive : bool or None     Control if unit parsing is case sensitive. Defaults to None, which uses the     registry's setting.</p>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.parse_units--returns","title":"Returns","text":"<pre><code>pint.Unit\n</code></pre> Source code in <code>tripper/units/units.py</code> <pre><code>def parse_units(  # pylint: disable=arguments-differ\n    self, units, *args, **kwargs\n):\n    try:\n        return super().parse_units(units, *args, **kwargs)\n    except pint.UndefinedUnitError:\n        pass\n    info = self.get_unit_info(symbol=units)\n    return super().parse_units(info.name, *args, **kwargs)\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.save_quantity","title":"<code>save_quantity(self, ts, q, iri, type=None, tbox=False, si_datatype=True, annotations=None)</code>","text":"<p>Save a Pint quantity individual to triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to save to.</p> required <code>q</code> <code>Quantity</code> <p>Quantity to save.</p> required <code>iri</code> <code>str</code> <p>IRI of the quantity in the triplestore.</p> required <code>type</code> <code>Optional[str]</code> <p>IRI of the type or superclass (if <code>tbox</code> is true) of the quantity.</p> <code>None</code> <code>tbox</code> <code>bool</code> <p>Whether to document the quantity in the tbox.</p> <code>False</code> <code>si_datatype</code> <code>bool</code> <p>Whether to represent the value using the <code>emmo:SIQuantityDatatype</code> datatype.</p> <code>True</code> <code>annotations</code> <code>Optional[dict]</code> <p>Additional annotations describing the quantity. Use Literal() for literal annotations.</p> <code>None</code> Source code in <code>tripper/units/units.py</code> <pre><code>def save_quantity(\n    self,\n    ts: Triplestore,\n    q: Quantity,\n    iri: str,\n    type: \"Optional[str]\" = None,  # pylint: disable=redefined-builtin\n    tbox: bool = False,\n    si_datatype: bool = True,\n    annotations: \"Optional[dict]\" = None,\n) -&gt; None:\n    \"\"\"Save a Pint quantity individual to triplestore.\n\n    Arguments:\n        ts: Triplestore to save to.\n        q: Quantity to save.\n        iri: IRI of the quantity in the triplestore.\n        type: IRI of the type or superclass (if `tbox` is true) of the\n            quantity.\n        tbox: Whether to document the quantity in the tbox.\n        si_datatype: Whether to represent the value using the\n            `emmo:SIQuantityDatatype` datatype.\n        annotations: Additional annotations describing the quantity.\n            Use Literal() for literal annotations.\n\n    \"\"\"\n    save_emmo_quantity(\n        ts=ts,\n        q=q,\n        iri=iri,\n        type=type,\n        tbox=tbox,\n        si_datatype=si_datatype,\n        annotations=annotations,\n    )\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.UnitRegistry.set_as_default","title":"<code>set_as_default(self)</code>","text":"<p>Set current unit registry as the default one.</p> <p>This unit registry can then be accessed with <code>get_ureg()</code>.</p> <p>Returns the previous default unit registry.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def set_as_default(self) -&gt; \"Union[UnitRegistry, None]\":\n    \"\"\"Set current unit registry as the default one.\n\n    This unit registry can then be accessed with `get_ureg()`.\n\n    Returns the previous default unit registry.\"\"\"\n    global _unit_reg  # pylint: disable=global-statement\n    old = _unit_reg\n    _unit_reg = self\n    return old\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Units","title":"<code> Units        </code>","text":"<p>A class representing all units in an ontology.</p> Source code in <code>tripper/units/units.py</code> <pre><code>class Units:\n    \"\"\"A class representing all units in an ontology.\"\"\"\n\n    # pylint: disable=too-many-instance-attributes\n\n    def __init__(\n        self,\n        ts: \"Optional[Triplestore]\" = None,\n        url: \"Optional[Union[str, Path]]\" = None,\n        format: \"Optional[str]\" = None,  # pylint: disable=redefined-builtin\n        name: \"Optional[str]\" = None,\n        formalisation: str = \"emmo\",\n        include_prefixed: bool = False,\n        cache: \"Union[bool, None]\" = True,\n    ) -&gt; None:\n        \"\"\"Initialise a Units class from triplestore `ts`\n\n        Arguments:\n            ts: Triplestore object containing the ontology to load\n                units from.\n            url: URL (or path) to the ontology from where unit\n                definitions are loaded if `ts` is not given.\n                Default: \"https://w3id.org/emmo/{EMMO_VERSION}\"\n            format: Optional format of the source referred to by `url`.\n            name: A (versioned) name for the triplestore. Used for caching.\n                Ex: \"emmo-1.0.0\".\n            formalisation: The ontological formalisation with which units\n                and quantities are represented. Currently only \"emmo\" is\n                supported.\n            include_prefixed: Whether to also include prefixed units.\n            cache: Whether to cache the unit table. If `cache` is:\n                - True: Load cache if it exists, otherwise create new cache.\n                - False: Don't load cache, but (over)write new cache.\n                - None: Don't use cache.\n\n        \"\"\"\n        url, name = _default_url_name(url, name)\n\n        if ts:\n            self.ts = ts\n        else:\n            self.ts = get_unit_triplestore(\n                url=url, format=format, name=name, cache=cache\n            )\n\n        self.ns = Namespace(\n            iri=\"https://w3id.org/emmo#\",\n            label_annotations=True,\n            check=True,\n            triplestore=self.ts,\n        )\n\n        pf = \"-prefixed\" if include_prefixed else \"\"\n        fname = f\"units{pf}-{name}.pickle\"\n        cachedir = get_cachedir(create=True)\n        cachefile = cachedir / fname\n        if cache and cachefile.exists():\n            with open(cachefile, \"rb\") as f:\n                d = pickle.load(f)  # nosec\n                self.units = d[\"units\"]\n                self.quantities = d[\"quantities\"]\n                self.constants = d[\"constants\"]\n        else:\n            print(\"* caching units... \", end=\"\", flush=True)\n            self.units = self._parse_units(include_prefixed=include_prefixed)\n            print(\"done\\n* caching quantities... \", end=\"\", flush=True)\n            self.quantities = self._parse_quantities(constants=False)\n            print(\"done\\n* caching constants... \", end=\"\", flush=True)\n            self.constants = self._parse_quantities(constants=True)\n            print(\"done\")\n\n        if cache is not None:\n            with open(cachefile, \"wb\") as f:\n                d = {\n                    \"units\": self.units,\n                    \"quantities\": self.quantities,\n                    \"constants\": self.constants,\n                }\n                pickle.dump(d, f)\n\n        self.url = url\n        self.name = name\n        self.formalisation = formalisation\n        self.cachedir = cachedir\n        self.cachefile = cachefile\n\n    def _emmo_unit_iris(self, include_prefixed: bool = False) -&gt; list:\n        \"\"\"Return a list with the IRIs of all EMMO units.\n\n        If `include_prefixed` is true, also return prefixed units.\n        \"\"\"\n        prefixfilter = (\n            \"\"\n            if include_prefixed\n            else (\n                \"FILTER NOT EXISTS \"\n                f\"{{ ?unit rdfs:subClassOf &lt;{EMMO.PrefixedUnit}&gt; . }}\"\n            )\n        )\n        query = f\"\"\"\n        PREFIX rdfs: &lt;{RDFS}&gt;\n        SELECT ?unit\n        WHERE {{\n          ?unit rdfs:subClassOf+ ?dimunit .\n          ?dimunit rdfs:subClassOf ?r .\n          ?r owl:onProperty &lt;{EMMO.hasDimensionString}&gt; .\n          {prefixfilter}\n        }}\n        \"\"\"\n        # Bug in mupy... seems to be fixed in master\n        return [iri for iri, in self.ts.query(query)]  # type: ignore\n\n    def _emmo_quantity_iris(self, constants: bool = False) -&gt; list:\n        \"\"\"Return a list with the IRIs of all EMMO quantities.\n\n        If `constants` is true, only physical constants are returned,\n        otherwise all quantities, but physical constants are returned.\n        \"\"\"\n        # Note, we liberately do not include\n        #\n        #    ?q rdfs:subClassOf+ &lt;{EMMO.Quantity}&gt; .\n        #\n        # in the query, since that makes it very slow.\n        constonly = \"\" if constants else \"NOT\"\n        query = f\"\"\"\n        PREFIX rdfs: &lt;{RDFS}&gt;\n        PREFIX owl: &lt;{OWL}&gt;\n        SELECT ?q\n        WHERE {{\n          # {{\n          #   SELECT ?q WHERE {{\n          #     ?q rdfs:subClassOf+ &lt;{EMMO.Quantity}&gt; .\n          #   }}\n          # }}\n          ?q rdfs:subClassOf* ?r .\n          ?r owl:onProperty &lt;{EMMO.hasMeasurementUnit}&gt; .\n          FILTER {constonly} EXISTS {{\n            ?q rdfs:subClassOf+ &lt;{EMMO.PhysicalConstant}&gt; .\n          }}\n          FILTER (!isBlank(?q))\n        }}\n        \"\"\"\n        # Bug in mupy... seems to be fixed in master\n        return [iri for iri, in self.ts.query(query)]  # type: ignore\n\n    def _get_unit_symbols(self, iri: str) -&gt; list:\n        \"\"\"Return a list with unit symbols for unit with the given IRI.\"\"\"\n        symbols = []\n        for r in self.ts.restrictions(\n            iri, self.ns.unitSymbolValue, type=\"value\"\n        ):\n            symbols.append(str(r[\"value\"]))\n\n        for symbol in self.ts.value(iri, self.ns.unitSymbol, any=None):\n            s = str(symbol)\n            symbols.append(f\"1{s}\" if s.startswith(\"/\") else s)\n\n        return symbols\n\n    def _get_unit_dimension_string(self, iri: str) -&gt; str:\n        \"\"\"Return the dimension string for unit with the given IRI.\"\"\"\n        query = f\"\"\"\n        PREFIX rdfs: &lt;{RDFS}&gt;\n        PREFIX owl: &lt;{OWL}&gt;\n        SELECT ?dimstr\n        WHERE {{\n          &lt;{iri}&gt; rdfs:subClassOf+ ?r .\n          ?r owl:onProperty &lt;{EMMO.hasDimensionString}&gt; .\n          ?r owl:hasValue ?dimstr .\n        }}\n        \"\"\"\n        result = self.ts.query(query)\n        if result:\n            # Bug in mupy\n            return result[0][0]  # type: ignore\n        raise MissingDimensionStringError(iri)\n\n    def _get_quantity_dimension_string(self, iri: str) -&gt; str:\n        \"\"\"Return the dimension string for quantity with the given IRI.\"\"\"\n        # Note, the use of subquery greately speeds up this SPARQL query\n        query = f\"\"\"\n        PREFIX rdfs: &lt;{RDFS}&gt;\n        PREFIX owl: &lt;{OWL}&gt;\n        SELECT ?dimstr\n        WHERE {{\n          {{\n            SELECT ?dimunit WHERE {{\n              &lt;{iri}&gt; rdfs:subClassOf+ ?r .\n              ?r owl:onProperty &lt;{EMMO.hasMeasurementUnit}&gt; .\n              ?r owl:someValuesFrom ?dimunit .\n            }}\n          }}\n          ?dimunit rdfs:subClassOf+ ?rr .\n          ?rr owl:onProperty &lt;{EMMO.hasDimensionString}&gt; .\n          ?rr owl:hasValue ?dimstr .\n        }}\n        \"\"\"  # nosec\n        result = self.ts.query(query)\n        if result:\n            # Bug in mupy\n            return result[0][0]  # type: ignore\n        raise MissingDimensionStringError(iri)\n\n    def _parse_dimension_string(self, dimstr: str) -&gt; \"Any\":\n        # TODO: replace return type with namedtuple when that is supported\n        # by mupy\n        \"\"\"Parse dimension string and return it as a named tuple.\"\"\"\n        m = re.match(\n            \"^T([+-][1-9][0-9]*|0) L([+-][1-9][0-9]*|0) M([+-][1-9][0-9]*|0) \"\n            \"I([+-][1-9][0-9]*|0) [\u0398\u03f4H]([+-][1-9][0-9]*|0) \"\n            \"N([+-][1-9][0-9]*|0) J([+-][1-9][0-9]*|0)$\",\n            dimstr,\n        )\n        if m:\n            return Dimension(*[int(d) for d in m.groups()])\n        raise InvalidDimensionStringError(dimstr)\n\n    def _parse_unitname(self, unitname: str) -&gt; \"Tuple[float, dict]\":\n        \"\"\"Parse CamelCase unit name and return a multiplication factor (from\n        prefixes) and a dict mapping each unit component to its power.\n\n        \"\"\"\n        exp = {\n            \"Square\": 2,\n            \"Cube\": 3,\n            \"Cubic\": 3,\n            \"Quartic\": 4,\n            \"Quintic\": 5,\n            \"Sextic\": 6,\n            \"Septic\": 7,\n            \"Heptic\": 7,\n            \"Octic\": 8,\n            \"Nonic\": 9,\n        }\n        # TODO: infer prefixes from EMMO (requires inferred ontology or\n        # current dev branch)\n        prefixes = {\n            \"Quecto\": 1e-30,\n            \"Ronto\": 1e-27,\n            \"Yocto\": 1e-24,\n            \"Zepto\": 1e-21,\n            \"Atto\": 1e-18,\n            \"Femto\": 1e-15,\n            \"Pico\": 1e-12,\n            \"Nano\": 1e-9,\n            \"Micro\": 1e-6,\n            \"Milli\": 1e-3,\n            \"Centi\": 1e-2,\n            \"Deci\": 1e-1,\n            \"Deca\": 1e1,\n            \"Hecto\": 1e2,\n            \"Kilo\": 1e3,\n            \"Mega\": 1e6,\n            \"Giga\": 1e9,\n            \"Tera\": 1e12,\n            \"Peta\": 1e15,\n            \"Exa\": 1e18,\n            \"Zetta\": 1e21,\n            \"Yotta\": 1e24,\n            \"Ronna\": 1e27,\n            \"Quetta\": 1e30,\n        }\n        d = {}\n        sign = power = 1\n        mult = prefix = 1.0\n        for token in re.split(\"(?&lt;!^)(?=[A-Z])\", unitname):\n            if token in (\"Per\", \"Reciprocal\"):\n                sign, power, prefix = -1, 1, 1.0\n            elif token in exp:\n                power = exp[token]\n            elif token in prefixes:\n                prefix = prefixes[token]\n            elif token in self.units:\n                mult *= prefix ** (sign * power)\n                d[token] = sign * power\n                power, prefix = 1, 1.0\n            else:\n                raise MissingUnitError(\n                    f\"No such unit (or prefix or degree): {token}\"\n                )\n        return mult, d\n\n    def _parse_units(self, include_prefixed=False) -&gt; \"dict[str, AttrDict]\":\n        \"\"\"Parse EMMO units and return them as an iterator over named\n        tuples.\n\n        Arguments:\n            include_prefixed: Whether to return prefixed units.\n\n        Returns:\n            Dict mapping unit names to AttrDicts with describing the unit.\n            See the get_unit() method for details.\n\n        \"\"\"\n        d = {}\n        for iri in self._emmo_unit_iris(include_prefixed=include_prefixed):\n            name = str(self.ts.value(iri, SKOS.prefLabel))\n            description = self.ts.value(iri, EMMO.elucidation)\n            if not description:\n                description = self.ts.value(iri, EMMO.definition)\n            dimstr = str(self._get_unit_dimension_string(iri))\n            dimension = self._parse_dimension_string(dimstr)\n            mult = list(\n                self.ts.restrictions(\n                    iri, property=EMMO.hasSIConversionMultiplier\n                )\n            )\n            offset = list(\n                self.ts.restrictions(iri, property=EMMO.hasSIConversionOffset)\n            )\n            qudtIRI = self.ts.value(iri, EMMO.qudtReference, any=True)\n            omIRI = self.ts.value(iri, EMMO.omReference, any=True)\n\n            for unitCodeIRI in (SCHEMA.unitCode, EMMO.uneceCommonCode):\n                unitCode = self.ts.value(iri, unitCodeIRI)\n                if unitCode:\n                    break\n\n            d[name] = AttrDict(\n                name=name,\n                description=description,\n                aliases=[\n                    str(s) for s in self.ts.value(iri, SKOS.altLabel, any=None)\n                ],\n                symbols=self._get_unit_symbols(iri),\n                dimension=dimension,\n                emmoIRI=iri,\n                qudtIRI=str(qudtIRI) if qudtIRI else None,\n                omIRI=str(omIRI) if omIRI else None,\n                ucumCodes=[\n                    str(s) for s in self.ts.value(iri, EMMO.ucumCode, any=None)\n                ],\n                unitCode=str(unitCode) if unitCode else None,\n                multiplier=float(mult[0][\"value\"]) if mult else None,\n                offset=float(offset[0][\"value\"]) if offset else None,\n            )\n        return d\n\n    def _parse_quantities(self, constants: bool) -&gt; \"dict[str, AttrDict]\":\n        \"\"\"Parse EMMO quantities and return them as an iterator over named\n        tuples.\n\n        Arguments:\n            constants: If true, only parse phycical constants. Otherwise,\n                parse all quantities, but physical constants.\n\n        Returns:\n            Dict mapping quantity names to AttrDicts with describing the\n                quantity.  See the get_quantity() method for details.\n        \"\"\"\n        d = {}\n        for iri in self._emmo_quantity_iris(constants=constants):\n            name = str(self.ts.value(iri, SKOS.prefLabel))\n            description = self.ts.value(iri, EMMO.elucidation)\n            if not description:\n                description = self.ts.value(iri, EMMO.definition)\n            dimstr = str(self._get_quantity_dimension_string(iri))\n            dimension = self._parse_dimension_string(dimstr)\n            qudtIRI = self.ts.value(iri, EMMO.qudtReference, any=True)\n            omIRI = self.ts.value(iri, EMMO.omReference, any=True)\n            iupacIRI = self.ts.value(iri, EMMO.iupacReference, any=True)\n            iso80000Ref = self.ts.value(iri, EMMO.ISO80000Reference, any=True)\n\n            d[name] = AttrDict(\n                name=name,\n                description=description,\n                aliases=[\n                    str(s) for s in self.ts.value(iri, SKOS.altLabel, any=None)\n                ],\n                dimension=dimension,\n                emmoIRI=iri,\n                qudtIRI=str(qudtIRI) if qudtIRI else None,\n                omIRI=str(omIRI) if omIRI else None,\n                iupacIRI=str(iupacIRI) if iupacIRI else None,\n                iso80000Ref=str(iso80000Ref) if iso80000Ref else None,\n            )\n        return d\n\n    def get_unit(\n        self,\n        name: \"Optional[str]\" = None,\n        symbol: \"Optional[str]\" = None,\n        iri: \"Optional[str]\" = None,\n        unitCode: \"Optional[str]\" = None,\n    ) -&gt; AttrDict:\n        \"\"\"Find unit by one of the arguments and return a dict describing it.\n\n        Arguments:\n            name: Search for unit by name. May also be an IRI. Ex: \"Ampere\"\n            symbol: Search for unit by symbol or UCUM code. Ex: \"A\", \"km\"\n            iri: Search for unit by IRI.\n            unitCode: Search for unit by UNECE common code. Ex: \"MTS\"\n\n        Returns:\n            dict: A dict with attribute access describing the unit. The dict\n            has the following keys:\n\n            - name: Preferred label.\n            - description: Unit description.\n            - aliases: List of alternative labels.\n            - symbols: List with unit symbols.\n            - dimension: Named tuple with quantity dimension.\n            - emmoIRI: IRI of the unit in the EMMO ontology.\n            - qudtIRI: IRI of the unit in the QUDT ontology.\n            - omIRI: IRI of the unit in the OM ontology.\n            - ucumCodes: List of UCUM codes for the unit.\n            - unitCode: UNECE common code for the unit.\n            - multiplier: Unit multiplier.\n            - offset: Unit offset.\n        \"\"\"\n        if name and name in self.units:\n            return self.units[name]\n\n        if not iri and name and \":\" in name:  # allow name to be an IRI\n            r = urlparse(name)\n            if r.scheme and r.netloc:\n                iri = name\n\n        for unit in self.units.values():\n            if name and name in unit.aliases:\n                return unit\n            if symbol and (symbol in unit.symbols or symbol in unit.ucumCodes):\n                return unit\n            if iri and iri in (unit.emmoIRI, unit.qudtIRI, unit.omIRI):\n                return unit\n            if unitCode and unitCode == unit.unitCode:\n                return unit\n\n        # As a fallback, try case insensitive search for labels\n        if name:\n            lowername = name.lower()\n            for unit in self.units.values():\n                if lowername == unit.name.lower() or lowername in set(\n                    s.lower() for s in unit.aliases\n                ):\n                    return unit\n\n        msg = (\n            f\"name={name}\"\n            if name\n            else (\n                f\"symbol={symbol}\"\n                if symbol\n                else (\n                    f\"iri={iri}\"\n                    if iri\n                    else (\n                        f\"unitCode={unitCode}\"\n                        if unitCode\n                        else \"missing search argument\"\n                    )\n                )\n            )\n        )\n        raise MissingUnitError(msg)\n\n    def write_pint_units(self, filename: \"Union[str, Path]\") -&gt; None:\n        \"\"\"Write Pint units definition to `filename`.\"\"\"\n        # pylint: disable=too-many-branches\n\n        # For now, include prefixes and base units...\n        # TODO: infer from ontology\n        content: list = [\n            \"# Pint units definitions file\",\n            (\n                f\"# Created with tripper.units from {self.formalisation} \"\n                f\"- {self.name}\"\n            ),\n            \"\",\n            \"@defaults\",\n            \"    group = international\",\n            \"    system = SI\",\n            \"@end\",\n            \"\",\n            \"# Decimal prefixes\",\n            \"Quecto- = 1e-30 = q-\",\n            \"Ronto- = 1e-27 = r-\",\n            \"Yocto- = 1e-24 = y-\",\n            \"Zepto- = 1e-21 = z-\",\n            \"Atto- =  1e-18 = a-\",\n            \"Femto- = 1e-15 = f-\",\n            \"Pico- =  1e-12 = p-\",\n            \"Nano- =  1e-9  = n-\",\n            \"Micro- = 1e-6  = \u00b5- = \u03bc- = u-\",\n            \"Milli- = 1e-3  = m-\",\n            \"Centi- = 1e-2  = c-\",\n            \"Deci- =  1e-1  = d-\",\n            \"Deca- =  1e+1  = da- = deka-\",\n            \"Hecto- = 1e2   = h-\",\n            \"Kilo- =  1e3   = k-\",\n            \"Mega- =  1e6   = M-\",\n            \"Giga- =  1e9   = G-\",\n            \"Tera- =  1e12  = T-\",\n            \"Peta- =  1e15  = P-\",\n            \"Exa- =   1e18  = E-\",\n            \"Zetta- = 1e21  = Z-\",\n            \"Yotta- = 1e24  = Y-\",\n            \"Ronna- = 1e27  = R-\",\n            \"Quetta- = 1e30 = Q-\",\n            \"\",\n            \"# Binary prefixes\",\n            \"Kibi- = 2**10 = Ki-\",\n            \"Mebi- = 2**20 = Mi-\",\n            \"Gibi- = 2**30 = Gi-\",\n            \"Tebi- = 2**40 = Ti-\",\n            \"Pebi- = 2**50 = Pi-\",\n            \"Exbi- = 2**60 = Ei-\",\n            \"Zebi- = 2**70 = Zi-\",\n            \"Yobi- = 2**80 = Yi-\",\n            \"\",\n            \"# Base units\",\n            \"Second = [Time] = s = second\",\n            \"Metre = [Length] = m = metre = meter\",\n            # Note: gram is used instead of kilogram to get prefixes right...\n            \"Gram = [Mass] = g = gram\",\n            \"Ampere = [ElectricCurrent] = A = ampere\",\n            \"Kelvin = [ThermodynamicTemperature]; offset: 0 = K = kelvin\",\n            \"Mole = [AmountOfSubstance] = mole = mol\",\n            \"Candela = [LuminousIntensity] = cd = candela\",\n            \"\",\n            \"# Units\",\n            \"Kilogram = 1000*Gram = kg = kilogram\",\n        ]\n\n        # Regex that converts \"CamelCase\" to \"snake_case\".\n        # Keeps a sequence of uppercase together.\n        # Ex: HTTPResponseCodeXYZ -&gt; http_response_code_xyz\n        snake_pattern = re.compile(\n            r\"(?&lt;=[a-z])(?=[A-Z])|(?&lt;=[A-Z])(?=[A-Z][a-z])\"\n        )\n\n        # Units\n        skipunits = {\n            \"Second\",\n            \"Metre\",\n            \"Gram\",\n            \"Kilogram\",\n            \"Ampere\",\n            \"Kelvin\",\n            \"Mole\",\n            \"Candela\",\n        }\n        for unit in self.units.values():\n            if unit.name in skipunits:\n                continue\n\n            baseexpr = base_unit_expression(unit.dimension)\n            mult = f\"{unit.multiplier} * \" if unit.multiplier != 1.0 else \"\"\n            if unit.multiplier or unit.offset:\n                if unit.multiplier and unit.offset:\n                    s = [\n                        f\"{unit.name} = {mult}{baseexpr}; \"\n                        f\"offset: {-unit.offset}\"\n                    ]\n                elif unit.multiplier:\n                    s = [f\"{unit.name} = {mult}{baseexpr}\"]\n                elif unit.offset:\n                    s = [f\"{unit.name} = {baseexpr}; offset: {-unit.offset}\"]\n                else:\n                    assert False, \"should never happen\"  # nosec\n            elif baseexpr == \"1\":\n                s = [f\"{unit.name} = []\"]\n            else:\n                s = [f\"{unit.name} = {baseexpr}\"]\n\n            # table = str.maketrans({x: None for x in \" .\u22c5\u00b7/*{}\"})\n            table = str.maketrans({x: None for x in \" ./*{}\"})\n            symbols = [s for s in unit.symbols if s.translate(table) == s]\n            if symbols:\n                s.extend(f\" = {symbol}\" for symbol in symbols)\n            else:\n                s.append(\" = _\")\n\n            snake = snake_pattern.sub(\"_\", unit.name).lower()\n            if snake != unit.name:\n                s.append(f\" = {snake}\")\n\n            for alias in unit.aliases:\n                if \" \" not in alias:\n                    s.append(f\" = {alias}\")\n\n            for code in [unit.unitCode] + unit.ucumCodes:\n                if code and f\" = {code}\" not in s:\n                    # code = re.sub(r\"([a-zA-Z])([0-9+-])\", r\"\\1^\\2\", code)\n                    s.append(f\" = {code}\")\n\n            # IRIs cannot be added to unit registry since they contain\n            # invalid characters like ':', '/' and '#'.\n            #\n            # for iri in unit.emmoIRI, unit.qudtIRI, unit.omIRI:\n            #     if iri and f\" = {iri}\" not in s:\n            #         s.append(f\" = {iri}\")\n\n            content.append(\"\".join(s))\n\n        # Quantities\n        content.append(\"\")\n        content.append(\"# Quantities:\")\n        dimensions = [getattr(Dimension, d).__doc__ for d in Dimension._fields]\n        for q in self.quantities.values():\n            if q.name in dimensions:\n                continue\n            expr = [\n                f\"[{dim}]\" if exp == 1 else f\"[{dim}]**{exp}\"\n                for dim, exp in zip(dimensions, q.dimension)\n                if exp\n            ]\n            content.append(f\"[{q.name}] = \" + \" * \".join(expr))\n\n        # Constants\n        # TODO: Add physical constants. See the PINT constants_en.txt file.\n\n        # Unit systems\n        content.extend(\n            [\n                \"\",\n                \"# Unit systems:\",\n                \"@system SI\",\n                \"    Second\",\n                \"    Metre\",\n                \"    Kilogram\",\n                \"    Ampere\",\n                \"    Kelvin\",\n                \"    Mole\",\n                \"    Candela\",\n                \"@end\",\n            ]\n        )\n\n        with open(filename, \"wt\", encoding=\"utf-8\") as f:\n            f.write(\"\\n\".join(content) + \"\\n\")\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Clear caches used by this Units object.\"\"\"\n        cachefile = self.cachefile\n        if cachefile.exists():\n            cachefile.unlink()\n\n        tsfile = self.cachedir / f\"{self.name}.ntriples\"\n        if tsfile.exists():\n            tsfile.unlink()\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Units.__init__","title":"<code>__init__(self, ts=None, url=None, format=None, name=None, formalisation='emmo', include_prefixed=False, cache=True)</code>  <code>special</code>","text":"<p>Initialise a Units class from triplestore <code>ts</code></p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Optional[Triplestore]</code> <p>Triplestore object containing the ontology to load units from.</p> <code>None</code> <code>url</code> <code>Optional[Union[str, Path]]</code> <p>URL (or path) to the ontology from where unit definitions are loaded if <code>ts</code> is not given. Default: \"https://w3id.org/emmo/{EMMO_VERSION}\"</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Optional format of the source referred to by <code>url</code>.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>A (versioned) name for the triplestore. Used for caching. Ex: \"emmo-1.0.0\".</p> <code>None</code> <code>formalisation</code> <code>str</code> <p>The ontological formalisation with which units and quantities are represented. Currently only \"emmo\" is supported.</p> <code>'emmo'</code> <code>include_prefixed</code> <code>bool</code> <p>Whether to also include prefixed units.</p> <code>False</code> <code>cache</code> <code>Union[bool, None]</code> <p>Whether to cache the unit table. If <code>cache</code> is: - True: Load cache if it exists, otherwise create new cache. - False: Don't load cache, but (over)write new cache. - None: Don't use cache.</p> <code>True</code> Source code in <code>tripper/units/units.py</code> <pre><code>def __init__(\n    self,\n    ts: \"Optional[Triplestore]\" = None,\n    url: \"Optional[Union[str, Path]]\" = None,\n    format: \"Optional[str]\" = None,  # pylint: disable=redefined-builtin\n    name: \"Optional[str]\" = None,\n    formalisation: str = \"emmo\",\n    include_prefixed: bool = False,\n    cache: \"Union[bool, None]\" = True,\n) -&gt; None:\n    \"\"\"Initialise a Units class from triplestore `ts`\n\n    Arguments:\n        ts: Triplestore object containing the ontology to load\n            units from.\n        url: URL (or path) to the ontology from where unit\n            definitions are loaded if `ts` is not given.\n            Default: \"https://w3id.org/emmo/{EMMO_VERSION}\"\n        format: Optional format of the source referred to by `url`.\n        name: A (versioned) name for the triplestore. Used for caching.\n            Ex: \"emmo-1.0.0\".\n        formalisation: The ontological formalisation with which units\n            and quantities are represented. Currently only \"emmo\" is\n            supported.\n        include_prefixed: Whether to also include prefixed units.\n        cache: Whether to cache the unit table. If `cache` is:\n            - True: Load cache if it exists, otherwise create new cache.\n            - False: Don't load cache, but (over)write new cache.\n            - None: Don't use cache.\n\n    \"\"\"\n    url, name = _default_url_name(url, name)\n\n    if ts:\n        self.ts = ts\n    else:\n        self.ts = get_unit_triplestore(\n            url=url, format=format, name=name, cache=cache\n        )\n\n    self.ns = Namespace(\n        iri=\"https://w3id.org/emmo#\",\n        label_annotations=True,\n        check=True,\n        triplestore=self.ts,\n    )\n\n    pf = \"-prefixed\" if include_prefixed else \"\"\n    fname = f\"units{pf}-{name}.pickle\"\n    cachedir = get_cachedir(create=True)\n    cachefile = cachedir / fname\n    if cache and cachefile.exists():\n        with open(cachefile, \"rb\") as f:\n            d = pickle.load(f)  # nosec\n            self.units = d[\"units\"]\n            self.quantities = d[\"quantities\"]\n            self.constants = d[\"constants\"]\n    else:\n        print(\"* caching units... \", end=\"\", flush=True)\n        self.units = self._parse_units(include_prefixed=include_prefixed)\n        print(\"done\\n* caching quantities... \", end=\"\", flush=True)\n        self.quantities = self._parse_quantities(constants=False)\n        print(\"done\\n* caching constants... \", end=\"\", flush=True)\n        self.constants = self._parse_quantities(constants=True)\n        print(\"done\")\n\n    if cache is not None:\n        with open(cachefile, \"wb\") as f:\n            d = {\n                \"units\": self.units,\n                \"quantities\": self.quantities,\n                \"constants\": self.constants,\n            }\n            pickle.dump(d, f)\n\n    self.url = url\n    self.name = name\n    self.formalisation = formalisation\n    self.cachedir = cachedir\n    self.cachefile = cachefile\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Units.clear_cache","title":"<code>clear_cache(self)</code>","text":"<p>Clear caches used by this Units object.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Clear caches used by this Units object.\"\"\"\n    cachefile = self.cachefile\n    if cachefile.exists():\n        cachefile.unlink()\n\n    tsfile = self.cachedir / f\"{self.name}.ntriples\"\n    if tsfile.exists():\n        tsfile.unlink()\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Units.get_unit","title":"<code>get_unit(self, name=None, symbol=None, iri=None, unitCode=None)</code>","text":"<p>Find unit by one of the arguments and return a dict describing it.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>Optional[str]</code> <p>Search for unit by name. May also be an IRI. Ex: \"Ampere\"</p> <code>None</code> <code>symbol</code> <code>Optional[str]</code> <p>Search for unit by symbol or UCUM code. Ex: \"A\", \"km\"</p> <code>None</code> <code>iri</code> <code>Optional[str]</code> <p>Search for unit by IRI.</p> <code>None</code> <code>unitCode</code> <code>Optional[str]</code> <p>Search for unit by UNECE common code. Ex: \"MTS\"</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dict with attribute access describing the unit. The dict has the following keys:</p> <ul> <li>name: Preferred label.</li> <li>description: Unit description.</li> <li>aliases: List of alternative labels.</li> <li>symbols: List with unit symbols.</li> <li>dimension: Named tuple with quantity dimension.</li> <li>emmoIRI: IRI of the unit in the EMMO ontology.</li> <li>qudtIRI: IRI of the unit in the QUDT ontology.</li> <li>omIRI: IRI of the unit in the OM ontology.</li> <li>ucumCodes: List of UCUM codes for the unit.</li> <li>unitCode: UNECE common code for the unit.</li> <li>multiplier: Unit multiplier.</li> <li>offset: Unit offset.</li> </ul> Source code in <code>tripper/units/units.py</code> <pre><code>def get_unit(\n    self,\n    name: \"Optional[str]\" = None,\n    symbol: \"Optional[str]\" = None,\n    iri: \"Optional[str]\" = None,\n    unitCode: \"Optional[str]\" = None,\n) -&gt; AttrDict:\n    \"\"\"Find unit by one of the arguments and return a dict describing it.\n\n    Arguments:\n        name: Search for unit by name. May also be an IRI. Ex: \"Ampere\"\n        symbol: Search for unit by symbol or UCUM code. Ex: \"A\", \"km\"\n        iri: Search for unit by IRI.\n        unitCode: Search for unit by UNECE common code. Ex: \"MTS\"\n\n    Returns:\n        dict: A dict with attribute access describing the unit. The dict\n        has the following keys:\n\n        - name: Preferred label.\n        - description: Unit description.\n        - aliases: List of alternative labels.\n        - symbols: List with unit symbols.\n        - dimension: Named tuple with quantity dimension.\n        - emmoIRI: IRI of the unit in the EMMO ontology.\n        - qudtIRI: IRI of the unit in the QUDT ontology.\n        - omIRI: IRI of the unit in the OM ontology.\n        - ucumCodes: List of UCUM codes for the unit.\n        - unitCode: UNECE common code for the unit.\n        - multiplier: Unit multiplier.\n        - offset: Unit offset.\n    \"\"\"\n    if name and name in self.units:\n        return self.units[name]\n\n    if not iri and name and \":\" in name:  # allow name to be an IRI\n        r = urlparse(name)\n        if r.scheme and r.netloc:\n            iri = name\n\n    for unit in self.units.values():\n        if name and name in unit.aliases:\n            return unit\n        if symbol and (symbol in unit.symbols or symbol in unit.ucumCodes):\n            return unit\n        if iri and iri in (unit.emmoIRI, unit.qudtIRI, unit.omIRI):\n            return unit\n        if unitCode and unitCode == unit.unitCode:\n            return unit\n\n    # As a fallback, try case insensitive search for labels\n    if name:\n        lowername = name.lower()\n        for unit in self.units.values():\n            if lowername == unit.name.lower() or lowername in set(\n                s.lower() for s in unit.aliases\n            ):\n                return unit\n\n    msg = (\n        f\"name={name}\"\n        if name\n        else (\n            f\"symbol={symbol}\"\n            if symbol\n            else (\n                f\"iri={iri}\"\n                if iri\n                else (\n                    f\"unitCode={unitCode}\"\n                    if unitCode\n                    else \"missing search argument\"\n                )\n            )\n        )\n    )\n    raise MissingUnitError(msg)\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.Units.write_pint_units","title":"<code>write_pint_units(self, filename)</code>","text":"<p>Write Pint units definition to <code>filename</code>.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def write_pint_units(self, filename: \"Union[str, Path]\") -&gt; None:\n    \"\"\"Write Pint units definition to `filename`.\"\"\"\n    # pylint: disable=too-many-branches\n\n    # For now, include prefixes and base units...\n    # TODO: infer from ontology\n    content: list = [\n        \"# Pint units definitions file\",\n        (\n            f\"# Created with tripper.units from {self.formalisation} \"\n            f\"- {self.name}\"\n        ),\n        \"\",\n        \"@defaults\",\n        \"    group = international\",\n        \"    system = SI\",\n        \"@end\",\n        \"\",\n        \"# Decimal prefixes\",\n        \"Quecto- = 1e-30 = q-\",\n        \"Ronto- = 1e-27 = r-\",\n        \"Yocto- = 1e-24 = y-\",\n        \"Zepto- = 1e-21 = z-\",\n        \"Atto- =  1e-18 = a-\",\n        \"Femto- = 1e-15 = f-\",\n        \"Pico- =  1e-12 = p-\",\n        \"Nano- =  1e-9  = n-\",\n        \"Micro- = 1e-6  = \u00b5- = \u03bc- = u-\",\n        \"Milli- = 1e-3  = m-\",\n        \"Centi- = 1e-2  = c-\",\n        \"Deci- =  1e-1  = d-\",\n        \"Deca- =  1e+1  = da- = deka-\",\n        \"Hecto- = 1e2   = h-\",\n        \"Kilo- =  1e3   = k-\",\n        \"Mega- =  1e6   = M-\",\n        \"Giga- =  1e9   = G-\",\n        \"Tera- =  1e12  = T-\",\n        \"Peta- =  1e15  = P-\",\n        \"Exa- =   1e18  = E-\",\n        \"Zetta- = 1e21  = Z-\",\n        \"Yotta- = 1e24  = Y-\",\n        \"Ronna- = 1e27  = R-\",\n        \"Quetta- = 1e30 = Q-\",\n        \"\",\n        \"# Binary prefixes\",\n        \"Kibi- = 2**10 = Ki-\",\n        \"Mebi- = 2**20 = Mi-\",\n        \"Gibi- = 2**30 = Gi-\",\n        \"Tebi- = 2**40 = Ti-\",\n        \"Pebi- = 2**50 = Pi-\",\n        \"Exbi- = 2**60 = Ei-\",\n        \"Zebi- = 2**70 = Zi-\",\n        \"Yobi- = 2**80 = Yi-\",\n        \"\",\n        \"# Base units\",\n        \"Second = [Time] = s = second\",\n        \"Metre = [Length] = m = metre = meter\",\n        # Note: gram is used instead of kilogram to get prefixes right...\n        \"Gram = [Mass] = g = gram\",\n        \"Ampere = [ElectricCurrent] = A = ampere\",\n        \"Kelvin = [ThermodynamicTemperature]; offset: 0 = K = kelvin\",\n        \"Mole = [AmountOfSubstance] = mole = mol\",\n        \"Candela = [LuminousIntensity] = cd = candela\",\n        \"\",\n        \"# Units\",\n        \"Kilogram = 1000*Gram = kg = kilogram\",\n    ]\n\n    # Regex that converts \"CamelCase\" to \"snake_case\".\n    # Keeps a sequence of uppercase together.\n    # Ex: HTTPResponseCodeXYZ -&gt; http_response_code_xyz\n    snake_pattern = re.compile(\n        r\"(?&lt;=[a-z])(?=[A-Z])|(?&lt;=[A-Z])(?=[A-Z][a-z])\"\n    )\n\n    # Units\n    skipunits = {\n        \"Second\",\n        \"Metre\",\n        \"Gram\",\n        \"Kilogram\",\n        \"Ampere\",\n        \"Kelvin\",\n        \"Mole\",\n        \"Candela\",\n    }\n    for unit in self.units.values():\n        if unit.name in skipunits:\n            continue\n\n        baseexpr = base_unit_expression(unit.dimension)\n        mult = f\"{unit.multiplier} * \" if unit.multiplier != 1.0 else \"\"\n        if unit.multiplier or unit.offset:\n            if unit.multiplier and unit.offset:\n                s = [\n                    f\"{unit.name} = {mult}{baseexpr}; \"\n                    f\"offset: {-unit.offset}\"\n                ]\n            elif unit.multiplier:\n                s = [f\"{unit.name} = {mult}{baseexpr}\"]\n            elif unit.offset:\n                s = [f\"{unit.name} = {baseexpr}; offset: {-unit.offset}\"]\n            else:\n                assert False, \"should never happen\"  # nosec\n        elif baseexpr == \"1\":\n            s = [f\"{unit.name} = []\"]\n        else:\n            s = [f\"{unit.name} = {baseexpr}\"]\n\n        # table = str.maketrans({x: None for x in \" .\u22c5\u00b7/*{}\"})\n        table = str.maketrans({x: None for x in \" ./*{}\"})\n        symbols = [s for s in unit.symbols if s.translate(table) == s]\n        if symbols:\n            s.extend(f\" = {symbol}\" for symbol in symbols)\n        else:\n            s.append(\" = _\")\n\n        snake = snake_pattern.sub(\"_\", unit.name).lower()\n        if snake != unit.name:\n            s.append(f\" = {snake}\")\n\n        for alias in unit.aliases:\n            if \" \" not in alias:\n                s.append(f\" = {alias}\")\n\n        for code in [unit.unitCode] + unit.ucumCodes:\n            if code and f\" = {code}\" not in s:\n                # code = re.sub(r\"([a-zA-Z])([0-9+-])\", r\"\\1^\\2\", code)\n                s.append(f\" = {code}\")\n\n        # IRIs cannot be added to unit registry since they contain\n        # invalid characters like ':', '/' and '#'.\n        #\n        # for iri in unit.emmoIRI, unit.qudtIRI, unit.omIRI:\n        #     if iri and f\" = {iri}\" not in s:\n        #         s.append(f\" = {iri}\")\n\n        content.append(\"\".join(s))\n\n    # Quantities\n    content.append(\"\")\n    content.append(\"# Quantities:\")\n    dimensions = [getattr(Dimension, d).__doc__ for d in Dimension._fields]\n    for q in self.quantities.values():\n        if q.name in dimensions:\n            continue\n        expr = [\n            f\"[{dim}]\" if exp == 1 else f\"[{dim}]**{exp}\"\n            for dim, exp in zip(dimensions, q.dimension)\n            if exp\n        ]\n        content.append(f\"[{q.name}] = \" + \" * \".join(expr))\n\n    # Constants\n    # TODO: Add physical constants. See the PINT constants_en.txt file.\n\n    # Unit systems\n    content.extend(\n        [\n            \"\",\n            \"# Unit systems:\",\n            \"@system SI\",\n            \"    Second\",\n            \"    Metre\",\n            \"    Kilogram\",\n            \"    Ampere\",\n            \"    Kelvin\",\n            \"    Mole\",\n            \"    Candela\",\n            \"@end\",\n        ]\n    )\n\n    with open(filename, \"wt\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\".join(content) + \"\\n\")\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.base_unit_expression","title":"<code>base_unit_expression(dimension)</code>","text":"<p>Return a base unit expression corresponding to <code>dimension</code>.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def base_unit_expression(dimension: Dimension) -&gt; str:\n    \"\"\"Return a base unit expression corresponding to `dimension`.\"\"\"\n    base_units = (\n        \"Second\",\n        \"Metre\",\n        \"Kilogram\",\n        \"Ampere\",\n        \"Kelvin\",\n        \"Mole\",\n        \"Candela\",\n    )\n    expr = []\n    for b, e in zip(base_units, dimension):\n        if e:\n            power = \"\" if e == 1 else f\"**{e}\"\n            expr.append(f\"{b}{power}\")\n    return \" * \".join(expr) if expr else \"1\"\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.get_unit_triplestore","title":"<code>get_unit_triplestore(url=None, format=None, name=None, cache=True)</code>","text":"<p>Return, potentially cached, triplestore object that defines the units.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>Optional[Union[str, Path]]</code> <p>URL or path to unit triplestore. Default: \"https://w3id.org/emmo/{EMMO_VERSION}\"</p> <code>None</code> <code>format</code> <code>Optional[str]</code> <p>Optional format of the source referred to by <code>url</code>.</p> <code>None</code> <code>name</code> <code>Optional[str]</code> <p>A (versioned) name for the triplestore. Used for caching. Ex: \"emmo-1.0.0\".</p> <code>None</code> <code>cache</code> <code>Union[bool, None]</code> <p>Whether to load from cache. If <code>cache</code> is: - True: Load cache if it exists, otherwise create new cache. - False: Don't load cache, but (over)write new cache. - None: Don't use cache.</p> <code>True</code> Source code in <code>tripper/units/units.py</code> <pre><code>def get_unit_triplestore(\n    url: \"Optional[Union[str, Path]]\" = None,\n    format: \"Optional[str]\" = None,  # pylint: disable=redefined-builtin\n    name: \"Optional[str]\" = None,\n    cache: \"Union[bool, None]\" = True,\n) -&gt; Triplestore:\n    \"\"\"Return, potentially cached, triplestore object that defines the units.\n\n    Arguments:\n        url: URL or path to unit triplestore.\n            Default: \"https://w3id.org/emmo/{EMMO_VERSION}\"\n        format: Optional format of the source referred to by `url`.\n        name: A (versioned) name for the triplestore. Used for caching.\n            Ex: \"emmo-1.0.0\".\n        cache: Whether to load from cache. If `cache` is:\n            - True: Load cache if it exists, otherwise create new cache.\n            - False: Don't load cache, but (over)write new cache.\n            - None: Don't use cache.\n\n    \"\"\"\n    global _unit_ts  # pylint: disable=global-statement\n    if not _unit_ts:\n        _unit_ts = Triplestore(\"rdflib\")\n        url, name = _default_url_name(url, name)\n\n        cachefile = get_cachedir() / f\"units-{name}.ntriples\"\n        if cache and cachefile.exists():\n            _unit_ts.parse(cachefile, format=\"ntriples\")\n        else:\n            print(\"* caching units triplestore... \", end=\"\", flush=True)\n            _unit_ts.parse(url, format=format)\n            print(\"done\")\n            if cache is not None:\n                try:\n                    _unit_ts.serialize(\n                        cachefile, format=\"ntriples\", encoding=\"utf-8\"\n                    )\n                except PermissionError as exc:\n                    warnings.warn(\n                        f\"{exc}: {cachefile}\",\n                        category=PermissionWarning,\n                    )\n    return _unit_ts\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.get_ureg","title":"<code>get_ureg(*args, *, nocreate=False, **kwargs)</code>","text":"<p>Return default unit registry.</p> <p>If a default unit registry has been set (using the <code>UnitRegistry.set_as_default()</code> method), it is returned.</p> <p>If no default unit registry has been set and <code>nocreate</code> is true, a <code>NoDefaultUnitRegistryError</code> exception is raised. Otherwise, a new default unit registry is created using the <code>args</code> and <code>kwargs</code> arguments.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def get_ureg(*args, nocreate=False, **kwargs) -&gt; \"UnitRegistry\":\n    \"\"\"Return default unit registry.\n\n    If a default unit registry has been set (using the\n    `UnitRegistry.set_as_default()` method), it is returned.\n\n    If no default unit registry has been set and `nocreate` is true,\n    a `NoDefaultUnitRegistryError` exception is raised. Otherwise,\n    a new default unit registry is created using the `args` and `kwargs`\n    arguments.\n\n    \"\"\"\n    global _unit_reg  # pylint: disable=global-statement\n    if not _unit_reg:\n        if nocreate:\n            raise NoDefaultUnitRegistryError(\"No default unit registry\")\n        ureg = UnitRegistry(*args, **kwargs)\n        _unit_reg = ureg\n    else:\n        ureg = _unit_reg\n    return ureg\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.load_emmo_quantity","title":"<code>load_emmo_quantity(ts, iri)</code>","text":"<p>Return value and unit for EMMO quantity with given <code>iri</code>.</p> Source code in <code>tripper/units/units.py</code> <pre><code>def load_emmo_quantity(ts: Triplestore, iri: str) -&gt; \"Tuple[float, str]\":\n    \"\"\"Return value and unit for EMMO quantity with given `iri`.\"\"\"\n    isclass = ts.query(f\"ASK {{&lt;{iri}&gt; a owl:Class}}\")\n\n    def value_restriction_query(prop, target=\"hasValue\", number=False):\n        \"\"\"Return SPARQL query for value restriction for property `prop`.\"\"\"\n        crit1 = f\"?r owl:{target} ?qvalue .\"\n        crit2 = f\"?r owl:{target} ?v .\\n  ?v &lt;{EMMO.hasNumberValue}&gt; ?qvalue .\"\n        crit = crit2 if number else crit1\n        return f\"\"\"\n        PREFIX rdfs: &lt;{RDFS}&gt;\n        SELECT ?qvalue\n        WHERE {{\n          &lt;{iri}&gt; rdfs:subClassOf+ ?r .\n          # ?r a owl:Restriction .\n          ?r owl:onProperty &lt;{prop}&gt; .\n          {crit}\n        }}\n        \"\"\"\n\n    # Check for emmo:hasSIQuantityValue\n    if isclass:\n        result = ts.query(value_restriction_query(EMMO.hasSIQuantityValue))\n        qval = result[0][0] if result else None\n    else:\n        result = ts.value(iri, EMMO.hasSIQuantityValue)\n        qval = str(result) if result else None\n\n    if qval:\n        m = re.match(\n            r\"([+-]?[0-9]*\\.?[0-9]*([eE][+-]?[0-9]+)?)[ *\u22c5]*(.*)?\", qval\n        )\n        if m:\n            value, _, unit = m.groups()\n            return float(value), unit\n\n    # Check for emmo:hasNumericalPart/emmo:hasReferencePart\n    if isclass:\n        num = ts.query(\n            value_restriction_query(EMMO.hasNumericalPart, number=True)\n        )\n        ref = ts.query(\n            value_restriction_query(\n                EMMO.hasReferencePart, target=\"someValuesFrom\"\n            )\n        )[0][0]\n    else:\n        num = ts.query(f\"\"\"\n        SELECT ?qvalue WHERE {{\n          &lt;{iri}&gt; &lt;{EMMO.hasNumericalPart}&gt; ?v .\n          ?v &lt;{EMMO.hasNumberValue}&gt; ?qvalue .\n        }}\n        \"\"\")\n        ref = ts.value(iri, EMMO.hasReferencePart)\n\n    return float(num[0][0]), ref\n</code></pre>"},{"location":"api_reference/units/units/#tripper.units.units.save_emmo_quantity","title":"<code>save_emmo_quantity(ts, q, iri, type=None, tbox=False, si_datatype=True, annotations=None)</code>","text":"<p>Save quantity to triplestore.</p> <p>Parameters:</p> Name Type Description Default <code>ts</code> <code>Triplestore</code> <p>Triplestore to save to.</p> required <code>q</code> <code>Quantity</code> <p>Quantity to save.</p> required <code>iri</code> <code>str</code> <p>IRI of the quantity in the triplestore.</p> required <code>type</code> <code>Optional[str]</code> <p>IRI of the type or superclass (if <code>tbox</code> is true) of the quantity.</p> <code>None</code> <code>tbox</code> <code>bool</code> <p>Whether to document the quantity in the tbox.</p> <code>False</code> <code>si_datatype</code> <code>bool</code> <p>Whether to represent the value using the <code>emmo:SIQuantityDatatype</code> datatype.</p> <code>True</code> <code>annotations</code> <code>Optional[dict]</code> <p>Additional annotations describing the quantity. Use Literal() for literal annotations.</p> <code>None</code> Source code in <code>tripper/units/units.py</code> <pre><code>def save_emmo_quantity(\n    ts: Triplestore,\n    q: \"Quantity\",\n    iri: str,\n    type: \"Optional[str]\" = None,  # pylint: disable=redefined-builtin\n    tbox: bool = False,\n    si_datatype: bool = True,\n    annotations: \"Optional[dict]\" = None,\n) -&gt; None:\n    \"\"\"Save quantity to triplestore.\n\n    Arguments:\n        ts: Triplestore to save to.\n        q: Quantity to save.\n        iri: IRI of the quantity in the triplestore.\n        type: IRI of the type or superclass (if `tbox` is true) of the\n            quantity.\n        tbox: Whether to document the quantity in the tbox.\n        si_datatype: Whether to represent the value using the\n            `emmo:SIQuantityDatatype` datatype.\n        annotations: Additional annotations describing the quantity.\n            Use Literal() for literal annotations.\n\n    \"\"\"\n    if EMMO not in ts.namespaces.values():\n        ts.bind(\"emmo\", EMMO)\n\n    if ts.has(iri):\n        raise IRIExistsError(iri)\n\n    if not type:\n        type = EMMO.Quantity\n\n    triples = [(iri, RDF.type, OWL.Class if tbox else type)]\n    if tbox:\n        r = bnode_iri(\"restriction\")\n        triples.extend(\n            [\n                (iri, RDFS.subClassOf, type),\n                (iri, RDFS.subClassOf, r),\n                (r, RDF.type, OWL.Restriction),\n            ]\n        )\n        if si_datatype:\n            triples.extend(\n                [\n                    (r, OWL.onProperty, EMMO.hasSIQuantityValue),\n                    (r, OWL.hasValue, Literal(q)),\n                ]\n            )\n        else:\n            v = bnode_iri(\"value\")\n            r2 = bnode_iri(\"restriction\")\n            q2 = q.to_ontology_units()\n            triples.extend(\n                [\n                    (r, OWL.onProperty, EMMO.hasNumericalPart),\n                    (r, OWL.hasValue, v),\n                    (v, EMMO.hasNumberValue, Literal(q2.m)),\n                    (iri, RDFS.subClassOf, r2),\n                    (r2, RDF.type, OWL.Restriction),\n                    (r2, OWL.onProperty, EMMO.hasReferencePart),\n                    (r2, OWL.someValuesFrom, q2.u.emmoIRI),\n                ]\n            )\n    else:\n        if si_datatype:\n            triples.append((iri, EMMO.hasSIQuantityValue, Literal(q)))\n        else:\n            v = bnode_iri(\"value\")\n            q2 = q.to_ontology_units()\n            triples.extend(\n                [\n                    (iri, EMMO.hasNumericalPart, v),\n                    (v, EMMO.hasNumberValue, Literal(q2.m)),\n                    (iri, EMMO.hasReferencePart, q2.u.emmoIRI),\n                ]\n            )\n\n    if annotations:\n        for pred, obj in annotations.items():\n            triples.append((iri, pred, obj))\n\n    ts.add_triples(triples)\n</code></pre>"},{"location":"datadoc/classdoc/","title":"Documenting abstract resources","text":""},{"location":"datadoc/classdoc/#documenting-abstract-resources","title":"Documenting abstract resources","text":"<p>Resources that do not exist are called abstract resources. A typical example of an abstract resource is a procedure that has not been performed (yet), like a general description of a task in a workflow.</p> <p>It is highly desirable to document such a resource as a class (i.e. in the TBox). It is consistent with the physicalistic/nominalistic approach taken by major ontological frameworks for applied sciences and makes it easy to document actual executions of such a workflow task as an individual of this class.</p> <pre><code>{\n    \"@id\": \"ex:MySimulation\",\n    \"@type\": \"owl:Class\",\n    \"subClassOf\": \"PhysicsBasedSimulation\",\n    \"prefLabel\": \"MySimulation\",\n    \"elucidation\": \"A simulation using my physics-based materials models.\",\n    \"isParticipatedBy\": {\n        \"@id\": \"ex:MyModel\",\n        \"@type\": \"owl:Class\"  # restrictionType defaults to some\n    },\n    \"command\": \"mymodel infile.txt -o outfile.txt\",  # Command to execute the model\n    \"workdir\": \".\",  # Relative path to working directory\n    \"environment\": {\"VAR1\": \"val1\", \"VAR2\": \"val2\"},  # Environment variables (JSON-encoded)\n    \"install_command\": \"pip install git+ssh://git@github.com:MyProj/myrepo.git@master\",\n    \"hasInput\": [\n        {\n            \"@id\": \"ex:MyInputDataset\",\n            \"@type\": \"owl:Class\",\n            \"subClassOf\": [\"dcat:Dataset\", \"emmo:Dataset\"],\n            \"restrictionType\": \"exactly 1\",  # defaults to \"some\"\n            \"prefLabel\": \"MyInputDataset.\",\n            \"elucidation\": \"My input dataset.\",\n            \"distribution\": {\n                \"accessURL\": \"file:./infile.txt\",\n                \"accessService\": {\n                    \"conformsTo\": \"https://github.com/SINTEF/dlite/\",\n                    \"generator\": \"ex:inputplugin\"  # for generating input\n                }\n            }\n        }\n    ],\n    \"hasOutput\": [\n        {\n            \"@id\": \"ex:MyOutputDataset\",\n            \"@type\": \"owl:Class\",\n            \"subClassOf\": [\"dcat:Dataset\", \"emmo:Dataset\"],\n            \"restrictionType\": \"exactly 1\",  # defaults to \"some\"\n            \"prefLabel\": \"MyOutputDataset.\",\n            \"elucidation\": \"My output dataset.\",\n            \"distribution\": {\n                \"accessURL\": \"file:./outfile.txt\",\n                \"accessService\": {\n                    \"conformsTo\": \"https://github.com/SINTEF/dlite/\",\n                    \"parser\": \"ex:outputplugin\",  # for parsing output\n                    \"generator\": \"ex:outputplugin\"  # for storing output to external storage\n                }\n            }\n        }\n    ]\n}\n</code></pre>"},{"location":"datadoc/customisation/","title":"Customisations","text":""},{"location":"datadoc/customisation/#user-defined-prefixes","title":"User-defined prefixes","text":"<p>A namespace prefix is a mapping from a prefix to a namespace URL. For example</p> <pre><code>owl: http://www.w3.org/2002/07/owl#\n</code></pre> <p>Tripper already include a default list of predefined prefixes. Additional prefixed can be provided in two ways.</p>"},{"location":"datadoc/customisation/#with-the-prefixes-argument","title":"With the <code>prefixes</code> argument","text":"<p>Several functions in the API (like store(), told() and TableDoc.parse_csv()) takes a <code>prefixes</code> argument with which additional namespace prefixes can provided.</p> <p>This may be handy when used from the Python API.</p>"},{"location":"datadoc/customisation/#with-custom-context","title":"With custom context","text":"<p>Additional prefixes can also be provided via a custom JSON-LD context as a <code>\"prefix\": \"namespace URL\"</code> mapping.</p> <p>See User-defined keywords for how this is done.</p>"},{"location":"datadoc/customisation/#user-defined-keywords","title":"User-defined keywords","text":"<p>Tripper already include a long list of predefined keywords, that are defined in the default JSON-LD context. A description of how to define new concepts in the JSON-LD context is given by JSON-LD 1.1 document, and can be tested in the JSON-LD Playground.</p> <p>A new custom keyword can be added by providing mapping in a custom JSON-LD context from the keyword to the IRI of the corresponding concept in an ontology.</p> <p>Lets assume that you already have a domain ontology with base IRI http://example.com/myonto#, that defines the concepts for the keywords you want to use for the data documentation.</p> <p>First, you can add the prefix for the base IRI of your domain ontology to a custom JSON-LD context</p> <pre><code>\"myonto\": \"http://example.com/myonto#\",\n</code></pre> <p>How the keywords should be specified in the context depends on whether they correspond to a data property or an object property in the ontology and whether a given datatype is expected.</p>"},{"location":"datadoc/customisation/#simple-literal","title":"Simple literal","text":"<p>Simple literals keywords correspond to data properties with no specific datatype (just a plain string).</p> <p>Assume you want to add the keyword <code>batchNumber</code> to relate documented samples to the number assigned to the batch they are taken from. It corresponds to the data property http://example.com/myonto#batchNumber in your domain ontology. By adding the following mapping to your custom JSON-LD context, <code>batchNumber</code> becomes available as a keyword for your data documentation:</p> <pre><code>\"batchNumber\": \"myonto:batchNumber\",\n</code></pre>"},{"location":"datadoc/customisation/#literal-with-specific-datatype","title":"Literal with specific datatype","text":"<p>If <code>batchNumber</code> must always be an integer, you can specify this by replacing the above mapping with the following:</p> <pre><code>\"batchNumber\": {\n    \"@id\": \"myonto:batchNumber\",\n    \"@type\": \"xsd:integer\"\n},\n</code></pre> <p>Here \"@id\" refer to the IRI <code>batchNumber</code> is mapped to and \"@type\" its datatype. In this case we use <code>xsd:integer</code>, which is defined in the W3C <code>xsd</code> vocabulary.</p>"},{"location":"datadoc/customisation/#object-property","title":"Object property","text":"<p>Object properties are relations between two individuals in the knowledge base.</p> <p>If you want to say more about the batches, you may want to store them as individuals in the knowledge base. In that case, you may want to add a keyword <code>fromBatch</code> which relate your sample to the batch it was taken from. In your ontology you may define <code>fromBatch</code> as a object property with IRI: http://example.com/myonto/fromBatch.</p> <pre><code>\"fromBatch\": {\n    \"@id\": \"myonto:fromBatch\",\n    \"@type\": \"@id\"\n},\n</code></pre> <p>Here the special value \"@id\" for the \"@type\" means that the value of <code>fromBatch</code> must be an IRI.</p>"},{"location":"datadoc/customisation/#creating-a-context-with-keywords-from-an-ontology","title":"Creating a context with keywords from an ontology","text":"<p>Creating a context with keywords manually can be strenuous and is prone to human mistakes. It is therefore advisable to only use one source of truth, namely the ontology.</p> <p>The context can be generated from a triplestore with the ontology with the Keywords class:</p> <pre><code>from tripper import Triplestore\nfrom tripper.datadoc import get_keywords\n\nts = Triplestore('rdflib')\n\nts.parse(\n    'https://raw.githubusercontent.com/EMMC-ASBL/tripper/refs/heads/master/tests/ontologies/family.ttl',\n    format='turtle',\n)\n\nkw =  get_keywords() # create an Keywords instance populated with the default keywords (ddoc:datadoc)\n# Before loading the keywords file it is required that all namespaces have a prefix.\n# The family namespace does not have prefix by defualt and it must be added:\nkw.add_prefix('fam', 'http://onto-ns.com/ontologies/examples/family#')\n\n# We can now load the ontology into the keywords\nkw.load_rdf(ts, redefine='skip') # keywords that are already defined are skipped\n# or\nlw.load_rdf(ts, redefine='allow') # keywords that are already defined are redefined\n</code></pre> <p>Note that there are a few considerations when generating a context from an ontology:</p> <p>First of all, labels that are the same as predefined keywords must be handled with care. The default behaviour is that if this is attempted, an error is raised (<code>redefine = raise</code>). This choice have been made to ensure that redefining predefined keywords is a conscious decision. In order to redefine an existing keyword, the argument <code>redefine</code> of the <code>load_rdf()</code> method must be set to <code>allow</code>. A warning will be emitted for each keyword that is redefined. In order to generate keywords from an ontology without redefining existing keywords, the <code>redefine</code> argument can be set to <code>skip</code>, in which case existing keywords are left unchanged and a warning is emitted for each new keyword that is skipped to the advantage of the existing keyword.</p>"},{"location":"datadoc/customisation/#providing-a-custom-context","title":"Providing a custom context","text":"<p>A custom context with defined keywords can be provided for all the interfaces described in the section Documenting a resource.</p>"},{"location":"datadoc/customisation/#python-dict","title":"Python dict","text":"<p>Both for the single-resource and multi-resource dicts, you can add a <code>\"@context\"</code> key to the dict who's value is</p> <ul> <li>a string containing a resolvable URL to the custom context,</li> <li>a dict with the custom context or</li> <li>a list of the aforementioned strings and dicts.</li> </ul> <p>For example</p> <pre><code>{\n    \"@context\": [\n        # URL to a JSON file, typically a domain-specific context\n        \"https://json-ld.org/contexts/person.jsonld\",\n\n        # Local context\n        {\n            \"fromBatch\": {\n                \"@id\": \"myonto:fromBatch\",\n                \"@type\": \"@id\"\n            }\n        }\n    ],\n\n    # Documenting of the resource using keywords defined in the context\n    ...\n}\n</code></pre> <p>Note that the default context is always included and doesn't need to be specified explicitly.</p>"},{"location":"datadoc/customisation/#yaml-file","title":"YAML file","text":"<p>Since the YAML representation is just a YAML serialisation of a multi-resource dict, custom context can be provided by adding a <code>\"@context\"</code> keyword.</p> <p>For example, the following YAML file defines a custom context defining the <code>myonto</code> prefix as well as the <code>batchNumber</code> and <code>fromBatch</code> keywords. An additional \"kb\" prefix (used for documented resources) is defined with the <code>prefixes</code> keyword.</p> <pre><code>---\n\n# Custom context\n\"@context\":\n  myonto: http://example.com/myonto#\n\n  batchNumber:\n    \"@id\": myonto:batchNumber\n    \"@type\": xsd:integer\n\n  fromBatch:\n    \"@id\": myonto:fromBatch\n    \"@type\": \"@id\"\n\n\n# Additional prefixes\nprefixes:\n  kb: http://example.com/kb#\n\n\nresources:\n  # Samples\n  - \"@id\": kb:sampleA\n    \"@type\": chameo:Sample\n    fromBatch: kb:batch1\n\n  - \"@id\": kb:sampleB\n    \"@type\": chameo:Sample\n    fromBatch: kb:batch1\n\n  - \"@id\": kb:sampleC\n    \"@type\": chameo:Sample\n    fromBatch: kb:batch2\n\n  # Batches\n  - \"@id\": kb:batch1\n    \"@type\": myonto:Batch\n    batchNumber: 1\n\n  - \"@id\": kb:batch2\n    \"@type\": myonto:Batch\n    batchNumber: 2\n</code></pre> <p>You can save this context to a triplestore with</p> <pre><code>&gt;&gt;&gt; from tripper import Triplestore\n&gt;&gt;&gt; from tripper.datadoc import save_datadoc\n&gt;&gt;&gt;\n&gt;&gt;&gt; ts = Triplestore(\"rdflib\")\n&gt;&gt;&gt; save_datadoc(  # doctest: +ELLIPSIS\n...     ts,\n...     \"https://raw.githubusercontent.com/EMMC-ASBL/tripper/refs/heads/master/tests/input/custom_context.yaml\",\n... )\n{'@context': ...}\n</code></pre> <p>The content of the triplestore should now be</p> <pre><code>&gt;&gt;&gt; print(ts.serialize())\n@prefix chameo: &lt;https://w3id.org/emmo/domain/characterisation-methodology/chameo#&gt; .\n@prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; .\n@prefix kb: &lt;http://example.com/kb#&gt; .\n@prefix myonto: &lt;http://example.com/myonto#&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\n&lt;BLANKLINE&gt;\nkb:sampleA a rdfs:Resource,\n        dcat:Resource,\n        chameo:Sample ;\n    myonto:fromBatch kb:batch1 .\n&lt;BLANKLINE&gt;\nkb:sampleB a rdfs:Resource,\n        dcat:Resource,\n        chameo:Sample ;\n    myonto:fromBatch kb:batch1 .\n&lt;BLANKLINE&gt;\nkb:sampleC a rdfs:Resource,\n        dcat:Resource,\n        chameo:Sample ;\n    myonto:fromBatch kb:batch2 .\n&lt;BLANKLINE&gt;\nkb:batch2 a myonto:Batch,\n        rdfs:Resource,\n        dcat:Resource ;\n    myonto:batchNumber 2 .\n&lt;BLANKLINE&gt;\nkb:batch1 a myonto:Batch,\n        rdfs:Resource,\n        dcat:Resource ;\n    myonto:batchNumber 1 .\n&lt;BLANKLINE&gt;\n&lt;BLANKLINE&gt;\n</code></pre>"},{"location":"datadoc/customisation/#table","title":"Table","text":"<p>The <code>__init__()</code> method of the TableDoc class takes a <code>context</code> argument with witch user-defined context can be provided. The value of the <code>context</code> argument is the same as for the <code>@context</code> key of a Python dict.</p>"},{"location":"datadoc/documenting-a-resource/","title":"Documenting a resource","text":"<p>In the tripper.datadoc sub-package, the documents documenting the resources internally represented as JSON-LD documents are stored as Python dicts. However, the API tries to hide the complexities of JSON-LD behind simple interfaces. To support different use cases, the sub-package provide several interfaces for data documentation, including Python dicts, YAML files and tables. These are further described below.</p>"},{"location":"datadoc/documenting-a-resource/#documenting-as-a-python-dict","title":"Documenting as a Python dict","text":"<p>The API supports two Python dict representations, one for documenting a single resource and one for documenting multiple resources.</p>"},{"location":"datadoc/documenting-a-resource/#single-resource-dict","title":"Single-resource dict","text":"<p>Below is a simple example of how to document a SEM image dataset as a Python dict:</p> <pre><code>&gt;&gt;&gt; dataset = {\n...     \"@id\": \"kb:image1\",\n...     \"@type\": \"sem:SEMImage\",\n...     \"creator\": {\n...         \"name\": \"Sigurd Wenner\",\n...     },\n...     \"description\": \"Back-scattered SEM image of cement, polished with 1 um diamond compound.\",\n...     \"distribution\": {\n...         \"downloadURL\": \"https://github.com/EMMC-ASBL/tripper/raw/refs/heads/master/tests/input/77600-23-001_5kV_400x_m001.tif\",\n...         \"mediaType\": \"https://www.iana.org/assignments/media-types/image/tiff\"\n...     }\n... }\n</code></pre> <p>The keywords are defined in the default JSON-LD context and documented under Predefined keywords.</p> <p>This example uses two namespace prefixes not included in the predefined prefixes. We therefore have to define them explicitly</p> <pre><code>&gt;&gt;&gt; prefixes = {\n...     \"sem\": \"https://w3id.org/emmo/domain/sem/0.1#\",\n...     \"kb\": \"http://example.com/kb/\"\n... }\n</code></pre> <p>Warning</p> <p>Prefixes and keywords shares the same namespace and must therefore be distinct.</p> <p>This is a concequence of JSON-LD and cannot be changed by Tripper. A good rule of thumb is to write keywords out as full words and use short abberiviations (about 2-5 characters) for prefixes.</p> <p>Side note</p> <p>This dict is actually a JSON-LD document with an implicit context. You can use told() to create a valid JSON-LD document from it. In addition to add a <code>@context</code> field, this function also adds some implicit <code>@type</code> declarations.</p> <pre><code>&gt;&gt;&gt; import json\n&gt;&gt;&gt; from tripper.datadoc import told\n&gt;&gt;&gt; d = told(dataset, prefixes=prefixes)\n&gt;&gt;&gt; print(json.dumps(d, indent=4))  # doctest: +SKIP\n</code></pre> <pre><code>{\n    \"@context\": \"https://raw.githubusercontent.com/EMMC-ASBL/tripper/refs/heads/master/tripper/context/0.3/context.json\",\n    \"@id\": \"http://example.com/kb/image1\",\n    \"@type\": \"https://w3id.org/emmo/domain/sem/0.1#SEMImage\",\n    \"creator\": {\n        \"@type\": [\n            \"http://xmlns.com/foaf/0.1/Agent\",\n            \"https://w3id.org/emmo#EMMO_2480b72b_db8d_460f_9a5f_c2912f979046\"\n        ],\n        \"name\": \"Sigurd Wenner\"\n    },\n    \"description\": \"Back-scattered SEM image of cement, polished with 1 um diamond compound.\",\n    \"distribution\": {\n        \"@type\": \"http://www.w3.org/ns/dcat#Distribution\",\n        \"downloadURL\": \"https://github.com/EMMC-ASBL/tripper/raw/refs/heads/master/tests/input/77600-23-001_5kV_400x_m001.tif\",\n        \"mediaType\": \"https://www.iana.org/assignments/media-types/image/tiff\"\n    }\n}\n</code></pre> <p>You can use store() to save this documentation to a triplestore. Since the prefixes \"sem\" and \"kb\" are not included in the Predefined prefixes, they are have to be provided explicitly.</p> <pre><code>&gt;&gt;&gt; from tripper import Triplestore\n&gt;&gt;&gt; from tripper.datadoc import store\n&gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n&gt;&gt;&gt; d = store(ts, dataset, prefixes=prefixes)\n</code></pre> <p>The returned <code>AttrDict</code> instance is an updated copy of <code>dataset</code> (casted to a dict subclass with attribute access). It correspond to a valid JSON-LD document and is the same as returned by told().</p> <p>You can use <code>ts.serialize()</code> to list the content of the triplestore (defaults to turtle):</p> <pre><code>&gt;&gt;&gt; print(ts.serialize())  # doctest: +SKIP\n</code></pre> <pre><code>@prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; .\n@prefix dcterms: &lt;http://purl.org/dc/terms/&gt; .\n@prefix emmo: &lt;https://w3id.org/emmo#&gt; .\n@prefix foaf: &lt;http://xmlns.com/foaf/0.1/&gt; .\n@prefix kb: &lt;http://example.com/kb/&gt; .\n@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .\n@prefix sem: &lt;https://w3id.org/emmo/domain/sem/0.1#&gt; .\n@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\n\nkb:image1 a sem:SEMImage ;\n    dcterms:creator [ a foaf:Agent,\n                emmo:EMMO_2480b72b_db8d_460f_9a5f_c2912f979046 ;\n            foaf:name \"Sigurd Wenner\"^^xsd:string ] ;\n    dcterms:description \"Back-scattered SEM image of cement, polished with 1 um diamond compound.\"^^rdf:langString ;\n    dcat:distribution [ a dcat:Distribution ;\n            dcat:downloadURL \"https://github.com/EMMC-ASBL/tripper/raw/refs/heads/master/tests/input/77600-23-001_5kV_400x_m001.tif\"^^xsd:anyURI ;\n            dcat:mediaType &lt;https://www.iana.org/assignments/media-types/image/tiff&gt; ] .\n</code></pre> <p>Note that the image implicitly has been declared to be an individual of the classes <code>dcat:Dataset</code> and <code>emmo:Dataset</code>. This is because the <code>type</code> argument of store() defaults to \"dataset\".</p>"},{"location":"datadoc/documenting-a-resource/#multi-resource-dict","title":"Multi-resource dict","text":"<p>It is also possible to document multiple resources as a Python dict.</p> <p>Note</p> <p>Unlike the single-resource dict representation, the multi-resource dict representation is not valid (possible incomplete) JSON-LD.</p> <p>The root of this dict representation accepts the following keywords:</p> <ul> <li>domain: Optional name of one of more domains to load keywords for.   Defaults to \"default\".</li> <li>keywordfile: Optional YAML file with keyword definitions to parse.   May also be an URI in which case it will be accessed via HTTP GET.</li> <li>@context: Optional user-defined context to be appended to the documentation of all resources.</li> <li>base: Base IRI against which to resolve relative IRIs.</li> <li>prefixes: A dict mapping namespace prefixes to their corresponding URLs.</li> <li>\\&lt;Class&gt;: Class name followed by a list of valid single-resource dicts for the specified class.   The class name must be defined by the domain, in a keywordfile or in a custom @context.   The \"default\" domain already include many common classes, like Resource, Dataset, Distribution, DataService, Agent...</li> </ul> <p>See semdata.yaml for an example of a YAML representation of a multi-resource dict documentation.</p>"},{"location":"datadoc/documenting-a-resource/#documenting-as-a-yaml-file","title":"Documenting as a YAML file","text":"<p>The save_datadoc() function allow to save a YAML file in multi-resource format to a triplestore. Saving semdata.yaml to a triplestore can e.g. be done with</p> <pre><code>&gt;&gt;&gt; from tripper.datadoc import save_datadoc\n&gt;&gt;&gt; save_datadoc(  # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n...    ts,\n...    \"https://raw.githubusercontent.com/EMMC-ASBL/tripper/refs/heads/master/tests/input/semdata.yaml\"\n... )\n{'@graph': [...], ...}\n</code></pre>"},{"location":"datadoc/documenting-a-resource/#documenting-as-table","title":"Documenting as table","text":"<p>The TableDoc class can be used to document multiple resources as rows in a table.</p> <p>The table must have a header row with defined keywords (either predefined or provided with a custom context). Nested fields may be specified as dot-separated keywords. For example, the table</p> @id distribution.downloadURL :a http://example.com/a.txt :b http://example.com/b.txt <p>correspond to the following turtle representation:</p> <pre><code>:a dcat:distribution [\n    a dcat:Distribution ;\n    downloadURL \"http://example.com/a.txt\" ] .\n\n:b dcat:distribution [\n    a dcat:Distribution ;\n    downloadURL \"http://example.com/b.txt\" ] .\n</code></pre> <p>The below example shows how to save all datasets listed in the CSV file semdata.csv to a triplestore.</p> <pre><code>&gt;&gt;&gt; from tripper.datadoc import TableDoc\n\n&gt;&gt;&gt; td = TableDoc.parse_csv(\n...     \"https://raw.githubusercontent.com/EMMC-ASBL/tripper/refs/heads/master/tests/input/semdata.csv\",\n...     prefixes={\n...         \"sem\": \"https://w3id.org/emmo/domain/sem/0.1#\",\n...         \"semdata\": \"https://he-matchmaker.eu/data/sem/\",\n...         \"sample\": \"https://he-matchmaker.eu/sample/\",\n...         \"mat\": \"https://he-matchmaker.eu/material/\",\n...         \"dm\": \"http://onto-ns.com/meta/characterisation/0.1/SEMImage#\",\n...         \"par\": \"http://sintef.no/dlite/parser#\",\n...         \"gen\": \"http://sintef.no/dlite/generator#\",\n...     },\n... )\n&gt;&gt;&gt; td.save(ts)\n</code></pre>"},{"location":"datadoc/fetching-resources-from-a-triplestore/","title":"Working with already documented resources","text":"<p>The tripper.datadoc module also includes functionality for easy searching of the documented resources.</p> <p>For these examples there must be a triplestore instance available, poplated with some data. <pre><code>&gt;&gt;&gt; from tripper import Triplestore\n&gt;&gt;&gt; from tripper.datadoc import save_datadoc\n&gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n&gt;&gt;&gt; save_datadoc(ts,\"https://raw.githubusercontent.com/EMMC-ASBL/tripper/refs/heads/master/tests/input/semdata.yaml\") # doctest: +ELLIPSIS\n{'@graph': [...], ...}\n</code></pre></p>"},{"location":"datadoc/fetching-resources-from-a-triplestore/#searching-the-knowledge-base","title":"Searching the knowledge base","text":""},{"location":"datadoc/fetching-resources-from-a-triplestore/#get-all-iris-of-all-datasets-in-the-kb","title":"Get all IRIs of all datasets in the kb","text":"<pre><code>&gt;&gt;&gt; from tripper.datadoc import search\n&gt;&gt;&gt; search(ts) # doctest: +ELLIPSIS\n[...]\n</code></pre> <p>This will return a list of all datasets in the knowledge base.</p>"},{"location":"datadoc/fetching-resources-from-a-triplestore/#search-with-filtering-criteria","title":"Search with filtering criteria","text":"<p>Before adding specific filtering criteria it is important to bind non-standard prefixes to corresponding namespaces (standard prefixes defined in the keywords file, like dcterms, dcat, etc do not need to be defined again):</p> <pre><code>&gt;&gt;&gt; DATA = ts.bind(\"data\", \"http://example.com/data#\")\n&gt;&gt;&gt; MAT = ts.bind(\"mat\", \"http://example.com/materials#\")\n</code></pre> <p>It is possible to search for instances of type <code>dcat:Dataset</code> in two ways:</p> <p><pre><code>&gt;&gt;&gt; search(ts, type=\"Dataset\")  # doctest: +ELLIPSIS\n[...]\n\n&gt;&gt;&gt; search(ts, type=\"dcat:Dataset\")  # doctest: +ELLIPSIS\n[...]\n</code></pre> The first shortened version is only possible for predefined keywords that are specifically added in tripper.</p> <p>Note that full iris (e.g. <code>http://www.w3.org/ns/dcat#Dataset</code>) are currently not supported.</p> <p>You can also search for documented resources of other types or include more than one type in the search. <pre><code>&gt;&gt;&gt; SEM = ts.bind(\"sem\", \"https://w3id.com/emmo/domain/sem/0.1#\")\n&gt;&gt;&gt; search(ts, type=\"sem:SEMImage\")  # doctest: +ELLIPSIS\n[...]\n\n&gt;&gt;&gt; search(ts, type=[\"sem:SEMImage\", \"dcat:Dataset\"])  # doctest: +ELLIPSIS\n[...]\n</code></pre></p> <p>It is also possible to filter through other criteria: <pre><code>&gt;&gt;&gt; search(ts, criteria={\"creator.name\": \"Sigurd Wenner\"})  # doctest: +ELLIPSIS\n[...]\n\n&gt;&gt;&gt; search(ts, criteria={\"creator.name\": [\"Sigurd Wenner\", \"Named Lab Assistant\"]})  # doctest: +ELLIPSIS\n[...]\n\n&gt;&gt;&gt; KB = ts.bind('kb', 'http://example.com/kb/' )\n&gt;&gt;&gt; search(ts, criteria={\"@id\": KB.image1}) # doctest: +ELLIPSIS\n[...]\n</code></pre></p> <p>Note that here the object created when binding the <code>kb</code> prefix is a tripper.Namespace, and can be used directly as the second example above.</p>"},{"location":"datadoc/fetching-resources-from-a-triplestore/#fetching-metadata-and-data","title":"Fetching metadata and data","text":"<p>The <code>acquire</code> function can be used to fetch metadata from the triplestore. <pre><code>&gt;&gt;&gt; from tripper.datadoc import acquire\n&gt;&gt;&gt; acquire(ts, 'https://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001/77600-23-001_5kV_400x_m001')  # doctest: +ELLIPSIS +NORMALIZE_WHITESPACE\nAttrDict({\n    '@id': 'https://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001/77600-23-001_5kV_400x_m001',\n    ...\n})\n</code></pre></p> <p>Similarly the load function can be used to fetch the data using the information about the dowload URL in the metadata. The syntax is the same as above. Note though that for this specific example you would need access to a server that is not available to the general public.</p>"},{"location":"datadoc/fetching-resources-from-a-triplestore/#removing-instances-in-the-knowledge-base","title":"Removing instances in the knowledge base","text":"<p>Be very careful when using this, as there is a high risk that you delete data from others if you have access to delete on a shared knowledge base.</p> <p>The same criteria as shown above can be used e.g.:</p> <p><pre><code>&gt;&gt;&gt; from tripper.datadoc import delete\n&gt;&gt;&gt; delete(ts, criteria={\"@id\": KB.image1})\n&gt;&gt;&gt; delete(ts, criteria={\"creator.name\": \"Sigurd Wenner\"})\n</code></pre> It is also possible to remove everything in the triplestore with <code>delete(ts)</code>, but this is strongly discouraged.</p>"},{"location":"datadoc/introduction/","title":"Data documentation","text":""},{"location":"datadoc/introduction/#introduction","title":"Introduction","text":"<p>The data documentation is based on small JSON-LD documents, each documenting a single resource. Examples of resources can be a dataset, an instrument, a sample, etc. All resources are uniquely identified by their IRI.</p> <p>The primary focus of the tripper.datadoc module is to document datasets such that they are consistent with the DCAT vocabulary, but at the same time easily extended additional semantic meaning provided by other ontologies. It is also easy to add and relate the datasets to other types of documents, like people, instruments and samples.</p> <p>The tripper.datadoc module provides a Python API for documenting resources at all four levels of data documentation, including:</p> <ul> <li>Cataloguing: Storing and accessing documents based on their IRI and data properties.   (Addressed FAIR aspects: findability and accessibility).</li> <li>Structural documentation: The structure of a dataset. Provided via DLite data models.   (Addressed FAIR aspects: interoperability).</li> <li>Contextual documentation: Relations between resources, i.e. linked data. Enables contextual search.   (Addressed FAIR aspects: findability and reusability).</li> <li>Semantic documentation: Describe what the resource is using ontologies. In combination with structural documentation, maps the properties of a data model to ontological concepts.   (Addressed FAIR aspects: findability, interoperability and reusability).</li> </ul> <p>The figure below shows illustrates how a dataset is documented in a triplestore.</p> <p></p>"},{"location":"datadoc/keywords-process/","title":"Keywords process","text":"<p>Keywords for theme: ddoc:datadoc</p> <p>The meaning of the columns are as follows:</p> <ul> <li>Keyword: The keyword referring to a property used for the data documentation.</li> <li>Range: Refer to the class for the values of the keyword.</li> <li>Conformance: Whether the keyword is mandatory, recommended or optional when documenting the given type of resources.</li> <li>Definition: The definition of the keyword.</li> <li>Usage note: Notes about how to use the keyword.</li> </ul>"},{"location":"datadoc/keywords-process/#properties-on-dataservice","title":"Properties on DataService","text":"<p>A collection of operations that provides access to one or more datasets or data processing functions.</p> <ul> <li>subClassOf: rdfs:Resource</li> </ul> Keyword Range Conformance Definition Usage note endpointURL rdfs:Literal(xsd:anyURI) mandatory The root location or primary endpoint of the service (an IRI). endpointDescription rdfs:Resource recommended A description of the services available via the end-points, including their operations, parameters etc. servesDataset dcat:Dataset recommended This property refers to a collection of data that this data service can distribute. parser oteio:Parser A parser that can parse the distribution."},{"location":"datadoc/keywords-process/#properties-on-dataset","title":"Properties on Dataset","text":"<p>A collection of data, published or curated by an agent, and available for access or download in one or more representations.</p> <ul> <li>subClassOf: dcat:Resource, emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a</li> </ul> Keyword Range Conformance Definition Usage note distribution dcat:Distribution recommended An available distribution for the dataset. geographicalCoverage dcterms:Location recommended A geographic region that is covered by the dataset. temporalCoverage dcterms:PeriodOfTime recommended A temporal period that the Dataset covers. otherIdentifier adms:Identifier optional A secondary identifier of the Dataset sample dcat:Distribution optional A sample distribution of the dataset. inSeries dcat:DatasetSeries optional A dataset series of which the dataset is part. spatialResolution rdfs:Literal(xsd:decimal) optional The minimum spatial separation resolvable in a dataset, measured in meters. temporalResolution rdfs:Literal(xsd:duration) optional The minimum time period resolvable in the dataset. frequency dcterms:Frequency optional The frequency at which the Dataset is updated. source dcat:Dataset optional A related Dataset from which the described Dataset is derived. wasDerivedFrom dcat:Resource optional A derivation is a transformation of an entity into another, an update of an entity resulting in a new one, or the construction of a new entity based on a pre-existing entity. Relates a resource to another resource it was derived from. wasGeneratedBy prov:Activity optional An activity that generated, or provides the business context for, the creation of the dataset. isInputOf emmo:EMMO_43e9a05d_98af_41b4_92f6_00f79a09bfce A process that this dataset is the input to. isOutputOf emmo:EMMO_43e9a05d_98af_41b4_92f6_00f79a09bfce A process that this dataset is the output to. hasDatum emmo:EMMO_50d6236a_7667_4883_8ae1_9bb5d190423a Relates a dataset to its datum parts. <code>hasDatum</code> relations are normally NOT specified manually, since they are generated from the DLite data model. isDescriptionFor dcat:Resource An object (e.g. a material) that this dataset describes. datamodel oteio:Datamodel URI of DLite datamodel for the dataset. datamodelStorage rdfs:Literal(xsd:anyURI) URL to DLite storage plugin where the datamodel is stored. mappings rdfs:Literal(rdf:JSON) A list of subject-predicate-object triples mapping the datamodel to ontological concepts. mappingFormat rdfs:Literal File format for <code>mappingURL</code>. Defaults to \"turtle\". mappingURL rdfs:Literal(xsd:anyURI) URL to a document defining the mappings of the datamodel. The file format is given by <code>mappingFormat</code>. Defaults to turtle."},{"location":"datadoc/keywords-process/#properties-on-distribution","title":"Properties on Distribution","text":"<p>A physical embodiment of the Dataset in a particular format.</p> <ul> <li>subClassOf: dcat:Resource</li> </ul> Keyword Range Conformance Definition Usage note accessURL rdfs:Resource mandatory A URL that gives access to a Distribution of the Dataset. E.g. landing page, feed, SPARQL endpoint. Use for all cases except a simple download link, in which case downloadURL is preferred. The resource at the access URL may contain information about how to get the Dataset. mediaType dcterms:MediaType recommended The media type of the Distribution as defined in the official register of media types managed by IANA. availability rdfs:Literal(xsd:date) recommended An indication how long it is planned to keep the Distribution of the Dataset available. accessService dcat:DataService optional A data service that gives access to the distribution of the dataset. byteSize rdfs:Literal(xsd:nonNegativeInteger) optional The size of a Distribution in bytes. compressFormat dcterms:MediaType optional The format of the file in which the data is contained in a compressed form, e.g. to reduce the size of the downloadable file. It SHOULD be expressed using a media type as defined in the official register of media types managed by IANA. downloadURL rdfs:Literal(xsd:anyURI) optional A URL that is a direct link to a downloadable file in a given format. The format is indicated by the distribution's 'dterms:format' and/or 'dcat:mediaType'. packageFormat dcterms:MediaType optional The format of the file in which one or more data files are grouped together, e.g. to enable a set of related files to be downloaded together. It SHOULD be expressed using a media type as defined in the official register of media types managed by IANA and prefixed with <code>https://www.iana.org/assignments/media-types/</code>. status skos:Concept optional The status of the distribution in the context of maturity lifecycle. It MUST take one of the values: 'stat:Completed', 'stat:Deprecated', 'stat:UnderDevelopment', 'stat:Withdrawn'. format dcterms:MediaTypeOrExtent optional The file format of the Distribution. dcat:mediaType SHOULD be used if the type of the distribution is defined by IANA. checksum spdx:Checksum optional A mechanism that can be used to verify that the contents of a distribution have not changed. The checksum is related to the downloadURL. generator oteio:Generator A generator that can create the distribution."},{"location":"datadoc/keywords-process/#properties-on-relationship","title":"Properties on Relationship","text":"<p>An association class for attaching additional information to a relationship between DCAT Resources.</p> Keyword Range Conformance Definition Usage note relatedResource rdfs:Resource optional A resource with an unspecified relationship to the cataloged resource. hasRole dcat:Role A function of an entity or agent with respect to another entity or resource."},{"location":"datadoc/keywords-process/#properties-on-resource","title":"Properties on Resource","text":"<p>Resource published or curated by an agent.</p> <ul> <li>subClassOf: rdfs:Resource</li> </ul> Keyword Range Conformance Definition Usage note description rdfs:Literal(rdf:langString) mandatory A free-text account of the resource. This property can be repeated for parallel language versions of the description. title rdfs:Literal(rdf:langString) mandatory A name given to the resource. This property can be repeated for parallel language versions of the name. contactPoint vcard:Kind recommended Contact information that can be used for sending comments about the resource. keyword rdfs:Literal(rdf:langString) recommended A keyword or tag describing the resource. theme skos:Concept recommended A category of the resource.  A resource may be associated with multiple themes. The set of themes used to categorize the resources are organized in a skos:ConceptScheme, skos:Collection, owl:Ontology or similar, describing all the categories and their relations in the catalog. publisher foaf:Agent recommended Agent responsible for making the resource available. versionNotes rdfs:Literal optional A description of the differences between this version and a previous version of the resource. This property can be repeated for parallel language versions of the version notes. hasVersion rdfs:Resource optional A related resource that is a version, edition, or adaptation of the described resource. This property is intended for relating a non-versioned or abstract resource to several versioned resources, e.g., snapshots. landingPage foaf:Document optional A web page that provides access to the resource and/or additional information (e.g. the distribution for a dataset). It is intended to point to a landing page at the original data provider, not to a page on a site of a third party, such as an aggregator. <p>For distributions, if they are accessible only through a landing page (i.e. direct download URLs are not known), then the landing page link should be duplicated as accessURL on a distribution.                                                                    | | qualifiedRelation     | dcat:Relationship                | optional    | A description of a relationship with another resource.                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                | | version               | rdfs:Literal                     | optional    | Version indicator (name or identifier) of a resource.                                                                                        | DCAT does not prescribe how a version name / identifier should be specified, and refers for guidance to [DWBP]'s Best Practice 7: Provide a version indicator.                                                                                                                                                                                                                                                 | | applicableLegislation | eli:LegalResource                | optional    | The legislation that mandates the creation or management of the resource.                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                | | accessRights          | dcterms:RightsStatement          | optional    | Information about who can access the resource or an indication of its security status.                                                       | Access Rights may include information regarding access or restrictions based on privacy, security, or other policies. The following preferred Rights Statement individuals are defined: <code>accr:PUBLIC</code>, <code>accr:NON_PUBLIC</code>, <code>accr:CONFIDENTIAL</code>, <code>accr:RESTRICTED</code>, <code>accr:SENSITIVE</code>                                                                                                                             | | conformsTo            | dcterms:Standard                 | optional    | An implementing rule or other specification.                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                | | contributor           | foaf:Agent                       | optional    | An entity responsible for making contributions to the resource.                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                | | creationDate          | rdfs:Literal                     | optional    | Date of creation                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                | | creator               | foaf:Agent                       | optional    | An entity responsible for producing the resource.                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                | | hasPart               | rdfs:Resource                    | optional    | A related resource that is included either physically or logically in the described resource.                                                |                                                                                                                                                                                                                                                                                                                                                                                                                | | isReferencedBy        | rdfs:Resource                    | optional    | A related resource, such as a publication, that references, cites, or otherwise points to the documented resource.                           |                                                                                                                                                                                                                                                                                                                                                                                                                | | releaseDate           | rdfs:Literal(xsd:date)       | optional    | The date of formal issuance (e.g., publication) of the resource.                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                | | language              | rdfs:Literal(xsd:string)     | optional    | A language of the resource.                                                                                                                  | Recommended practice is to use either a non-literal value representing a language from a controlled vocabulary such as ISO 639-2 or ISO 639-3, or a literal value consisting of an IETF Best Current Practice 47 [IETF-BCP47] language tag.</p> <p>Example values: \"en\", \"it\", \"no\", \"da\"</p> <p>This property can be repeated if the resource is expressed with multiple languages.  | | license               | dcterms:LicenseDocument          | optional    | A licence under which the resource is made available.                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                | | modificationDate      | rdfs:Literal(xsd:date)       | optional    | The most recent date on which the resource was changed or modified.                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                | | documentation         | foaf:Document                    | optional    | A page or document about this resource.                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                | | qualifiedAttribution  | prov:Attribution                 | optional    | An Agent having some form of responsibility for the resource.                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                | | abstract              | rdfs:Literal(rdf:langString) |             | A summary of the resource.                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                | | bibliographicCitation | rdfs:Literal                     |             | A bibliographic reference for the resource.                                                                                                  | Recommended practice is to include sufficient bibliographic detail to identify the resource as unambiguously as possible.                                                                                                                                                                                                                                                                                      | | conformance           | ddoc:ConformanceLevel            |             | Whether the annotation is mandatory, recommended or optional.                                                                                | It MUST take one of the values 'ddoc:mandatory', 'ddoc:recommended' or 'ddoc:optional'.                                                                                                                                                                                                                                                                                                                        | | curationDate          | xsd:date                         |             | Date of curation.                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                | | curator               | foaf:Agent                       |             | The agent that curated the resource.                                                                                                         | Use <code>ddoc:curationDate</code> to refer to the date of curation.                                                                                                                                                                                                                                                                                                                                                      | | statements            | rdfs:Literal(rdf:JSON)       |             | A list of subject-predicate-object triples with additional RDF statements documenting the resource.                                          |                                                                                                                                                                                                                                                                                                                                                                                                                | | deprecated            | rdfs:Literal(xsd:boolean)    |             | The annotation property that indicates that a given entity has been deprecated. It should equal to <code>\"true\"^^xsd:boolean</code>.                    |                                                                                                                                                                                                                                                                                                                                                                                                                | | comment               | rdfs:Literal                     |             | A description of the subject resource. Use <code>description</code> instead.                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                | | domain                | rdfs:Resource                    |             | A domain of the subject property.                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                | | isDefinedBy           | skos:Concept                     |             | Indicate a resource defining the subject resource. This property may be used to indicate an RDF vocabulary in which a resource is described. |                                                                                                                                                                                                                                                                                                                                                                                                                | | label                 | rdfs:Literal                     |             | Provides a human-readable version of a resource's name.                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                | | range                 | rdfs:Resource                    |             | A range of the subject property.                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                | | seeAlso               | skos:Concept                     |             | Indicates a resource that might provide additional information about the subject resource.                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                | | definition            | rdfs:Literal(rdf:langString) |             | A statement or formal explanation of the meaning of a concept.                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                |</p>"},{"location":"datadoc/keywords-process/#properties-on-licensedocument","title":"Properties on LicenseDocument","text":"<p>A legal document giving official permission to do something with a resource.</p> Keyword Range Conformance Definition Usage note type skos:Concept optional A type of the resource. A recommended controlled vocabulary data-type is foreseen. For agents, the value should be chosen from ADMS publisher type."},{"location":"datadoc/keywords-process/#properties-on-location","title":"Properties on Location","text":"<p>A spatial region or named place.</p> Keyword Range Conformance Definition Usage note bbox rdfs:Literal recommended The geographic bounding box of a resource. centroid rdfs:Literal recommended The geographic center (centroid) of a resource. geometry locn:Geometry optional The geographic center (centroid) of a resource."},{"location":"datadoc/keywords-process/#properties-on-agent","title":"Properties on Agent","text":"<p>An agent (eg. person, group, software or physical artifact).</p> <ul> <li>subClassOf: dcterms:Agent, emmo:EMMO_2480b72b_db8d_460f_9a5f_c2912f979046</li> </ul> Keyword Range Conformance Definition Usage note name rdfs:Literal(xsd:string) mandatory A name of the agent. identifier rdfs:Literal optional URI or other unique identifier of the resource being described. Recommended practice is to identify the resource by means of a string conforming to an identification system. Examples include International Standard Book Number (ISBN), Digital Object Identifier (DOI), and Uniform Resource Name (URN).  Persistent identifiers should be provided as HTTP URIs."},{"location":"datadoc/keywords-process/#properties-on-generator","title":"Properties on Generator","text":"<p>A generator that can serialise an instance of a datamodel into a distribution.</p> <ul> <li>subClassOf: oteio:Parser</li> </ul> Keyword Range Conformance Definition Usage note generatorType rdfs:Literal Generator type. Ex: <code>application/vnd.dlite-generate</code>."},{"location":"datadoc/keywords-process/#properties-on-parser","title":"Properties on Parser","text":"<p>A parser that can parse a distribution into an instance of a datamodel.</p> Keyword Range Conformance Definition Usage note configuration rdfs:Literal(rdf:JSON) A JSON string with configurations specific to the parser or generator. parserType rdfs:Literal Parser type. Ex: <code>application/vnd.dlite-parse</code>."},{"location":"datadoc/keywords-process/#properties-on-restriction","title":"Properties on Restriction","text":"<p>The class of property restrictions.</p> Keyword Range Conformance Definition Usage note maxQualifiedCardinality rdfs:Literal(xsd:nonNegativeInteger) The property that determines the cardinality of a maximum qualified cardinality restriction. minQualifiedCardinality rdfs:Literal(xsd:nonNegativeInteger) The property that determines the cardinality of a minimum qualified cardinality restriction. qualifiedCardinality rdfs:Literal(xsd:nonNegativeInteger) The property that determines the cardinality of an exact qualified cardinality restriction."},{"location":"datadoc/keywords-process/#properties-on-class","title":"Properties on Class","text":"<p>The class of classes.</p> Keyword Range Conformance Definition Usage note conceptualisation rdfs:Literal(rdf:langString) A comment that helps the reader to understand how the world has been conceptualised by the ontology authors. elucidation rdfs:Literal(rdf:langString) Short enlightening explanation aimed to facilitate the user in drawing the connection (interpretation) between a OWL entity and the real world object(s) for which it stands.  It should address the real world entities using the concepts introduced by the conceptualisation. subClassOf rdfs:Class The subject is a subclass of a class. subPropertyOf rdf:Property The subject is a subproperty of a property. altLabel rdfs:Literal(rdf:langString) An alternative lexical label for a resource. hiddenLabel rdfs:Literal(rdf:langString) A lexical label for a resource that should be hidden when generating visual displays of the resource, but should still be accessible to free text search operations. prefLabel rdfs:Literal(rdf:langString) A preferred label of the concept."},{"location":"datadoc/keywords-process/#properties-on-resource_1","title":"Properties on Resource","text":"<p>Resource published or curated by an agent.</p> <ul> <li>subClassOf: rdfs:Resource</li> </ul> Keyword Range Conformance Definition Usage note datatype rdfs:Datatype A datatype for annotations and data properties. usageNote rdfs:Literal(rdf:langString) A reference that provides information on how this resource is to be used."},{"location":"datadoc/keywords-process/#properties-on-checksum","title":"Properties on Checksum","text":"<p>A preferred label of the concept.</p> Keyword Range Conformance Definition Usage note algorithm spdx:ChecksumAlgorithm The algorithm used to produce the subject Checksum. checksumValue rdfs:Literal(xsd:hexBinary) A lower case hexadecimal encoded digest value produced using a specific algorithm."},{"location":"datadoc/keywords-process/#properties-on-kind","title":"Properties on Kind","text":"<p>A description following the vCard specification, e.g. to provide telephone number and e-mail address for a contact point.</p> Keyword Range Conformance Definition Usage note hasAddress rdfs:Literal hasCountryName rdfs:Literal hasEmail rdfs:Literal hasFamilyName rdfs:Literal hasGender rdfs:Literal hasGeo rdfs:Literal hasGivenName rdfs:Literal hasHonorificPrefix rdfs:Literal hasHonorificSuffix rdfs:Literal hasInstantMessage rdfs:Literal hasKey rdfs:Literal hasLanguage rdfs:Literal hasLogo rdfs:Literal hasMember rdfs:Literal hasName rdfs:Literal hasNickname rdfs:Literal hasNote rdfs:Literal hasOrganizationName rdfs:Literal hasOrganizationUnit rdfs:Literal hasPhoto rdfs:Literal hasPostalCode rdfs:Literal hasRegion rdfs:Literal hasStreetAddress rdfs:Literal hasTelephone rdfs:Literal hasUID rdfs:Literal hasURL rdfs:Literal"},{"location":"datadoc/keywords/","title":"Predefined keywords","text":"<p>Keywords for theme: ddoc:datadoc</p> <p>The meaning of the columns are as follows:</p> <ul> <li>Keyword: The keyword referring to a property used for the data documentation.</li> <li>Range: Refer to the class for the values of the keyword.</li> <li>Conformance: Whether the keyword is mandatory, recommended or optional when documenting the given type of resources.</li> <li>Definition: The definition of the keyword.</li> <li>Usage note: Notes about how to use the keyword.</li> </ul>"},{"location":"datadoc/keywords/#special-keywords-from-json-ld","title":"Special keywords (from JSON-LD)","text":"<p>See the [JSON-LD specification] for more details.</p> Keyword Range Conformance Definition Usage note @id IRI mandatory IRI identifying the resource to document. @type IRI recommended Ontological class defining the class of a node. @context dict|list optional Context defining namespace prefixes and additional keywords. @base namespace optional Base IRI against which relative IRIs are resolved. @vocab namespace optional Used to expand properties and values in @type with a common prefix IRI. @graph list optional Used for documenting multiple resources."},{"location":"datadoc/keywords/#properties-on-dataservice","title":"Properties on DataService","text":"<p>A collection of operations that provides access to one or more datasets or data processing functions.</p> <ul> <li>subClassOf: rdfs:Resource</li> </ul> Keyword Range Conformance Definition Usage note endpointURL rdfs:Literal(xsd:anyURI) mandatory The root location or primary endpoint of the service (an IRI). endpointDescription rdfs:Resource recommended A description of the services available via the end-points, including their operations, parameters etc. servesDataset dcat:Dataset recommended This property refers to a collection of data that this data service can distribute. parser oteio:Parser A parser that can parse the distribution."},{"location":"datadoc/keywords/#properties-on-dataset","title":"Properties on Dataset","text":"<p>A collection of data, published or curated by an agent, and available for access or download in one or more representations.</p> <ul> <li>subClassOf: dcat:Resource, emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a</li> </ul> Keyword Range Conformance Definition Usage note distribution dcat:Distribution recommended An available distribution for the dataset. geographicalCoverage dcterms:Location recommended A geographic region that is covered by the dataset. temporalCoverage dcterms:PeriodOfTime recommended A temporal period that the Dataset covers. otherIdentifier adms:Identifier optional A secondary identifier of the Dataset sample dcat:Distribution optional A sample distribution of the dataset. inSeries dcat:DatasetSeries optional A dataset series of which the dataset is part. spatialResolution rdfs:Literal(xsd:decimal) optional The minimum spatial separation resolvable in a dataset, measured in meters. temporalResolution rdfs:Literal(xsd:duration) optional The minimum time period resolvable in the dataset. frequency dcterms:Frequency optional The frequency at which the Dataset is updated. source dcat:Dataset optional A related Dataset from which the described Dataset is derived. wasDerivedFrom dcat:Resource optional A derivation is a transformation of an entity into another, an update of an entity resulting in a new one, or the construction of a new entity based on a pre-existing entity. Relates a resource to another resource it was derived from. wasGeneratedBy prov:Activity optional An activity that generated, or provides the business context for, the creation of the dataset. isInputOf emmo:EMMO_43e9a05d_98af_41b4_92f6_00f79a09bfce A process that this dataset is the input to. isOutputOf emmo:EMMO_43e9a05d_98af_41b4_92f6_00f79a09bfce A process that this dataset is the output to. hasDatum emmo:EMMO_50d6236a_7667_4883_8ae1_9bb5d190423a Relates a dataset to its datum parts. <code>hasDatum</code> relations are normally NOT specified manually, since they are generated from the DLite data model. isDescriptionFor dcat:Resource An object (e.g. a material) that this dataset describes. datamodel oteio:Datamodel URI of DLite datamodel for the dataset. datamodelStorage rdfs:Literal(xsd:anyURI) URL to DLite storage plugin where the datamodel is stored. mappings rdfs:Literal(rdf:JSON) A list of subject-predicate-object triples mapping the datamodel to ontological concepts. mappingFormat rdfs:Literal File format for <code>mappingURL</code>. Defaults to \"turtle\". mappingURL rdfs:Literal(xsd:anyURI) URL to a document defining the mappings of the datamodel. The file format is given by <code>mappingFormat</code>. Defaults to turtle."},{"location":"datadoc/keywords/#properties-on-distribution","title":"Properties on Distribution","text":"<p>A physical embodiment of the Dataset in a particular format.</p> <ul> <li>subClassOf: dcat:Resource</li> </ul> Keyword Range Conformance Definition Usage note accessURL rdfs:Resource mandatory A URL that gives access to a Distribution of the Dataset. E.g. landing page, feed, SPARQL endpoint. Use for all cases except a simple download link, in which case downloadURL is preferred. The resource at the access URL may contain information about how to get the Dataset. mediaType dcterms:MediaType recommended The media type of the Distribution as defined in the official register of media types managed by IANA. availability rdfs:Literal(xsd:date) recommended An indication how long it is planned to keep the Distribution of the Dataset available. accessService dcat:DataService optional A data service that gives access to the distribution of the dataset. byteSize rdfs:Literal(xsd:nonNegativeInteger) optional The size of a Distribution in bytes. compressFormat dcterms:MediaType optional The format of the file in which the data is contained in a compressed form, e.g. to reduce the size of the downloadable file. It SHOULD be expressed using a media type as defined in the official register of media types managed by IANA. downloadURL rdfs:Literal(xsd:anyURI) optional A URL that is a direct link to a downloadable file in a given format. The format is indicated by the distribution's 'dterms:format' and/or 'dcat:mediaType'. packageFormat dcterms:MediaType optional The format of the file in which one or more data files are grouped together, e.g. to enable a set of related files to be downloaded together. It SHOULD be expressed using a media type as defined in the official register of media types managed by IANA and prefixed with <code>https://www.iana.org/assignments/media-types/</code>. status skos:Concept optional The status of the distribution in the context of maturity lifecycle. It MUST take one of the values: 'stat:Completed', 'stat:Deprecated', 'stat:UnderDevelopment', 'stat:Withdrawn'. format dcterms:MediaTypeOrExtent optional The file format of the Distribution. dcat:mediaType SHOULD be used if the type of the distribution is defined by IANA. checksum spdx:Checksum optional A mechanism that can be used to verify that the contents of a distribution have not changed. The checksum is related to the downloadURL. generator oteio:Generator A generator that can create the distribution."},{"location":"datadoc/keywords/#properties-on-relationship","title":"Properties on Relationship","text":"<p>An association class for attaching additional information to a relationship between DCAT Resources.</p> Keyword Range Conformance Definition Usage note relatedResource rdfs:Resource optional A resource with an unspecified relationship to the cataloged resource. hasRole dcat:Role A function of an entity or agent with respect to another entity or resource."},{"location":"datadoc/keywords/#properties-on-resource","title":"Properties on Resource","text":"<p>Resource published or curated by an agent.</p> <ul> <li>subClassOf: rdfs:Resource</li> </ul> Keyword Range Conformance Definition Usage note description rdfs:Literal(rdf:langString) mandatory A free-text account of the resource. This property can be repeated for parallel language versions of the description. title rdfs:Literal(rdf:langString) mandatory A name given to the resource. This property can be repeated for parallel language versions of the name. contactPoint vcard:Kind recommended Contact information that can be used for sending comments about the resource. keyword rdfs:Literal(rdf:langString) recommended A keyword or tag describing the resource. theme skos:Concept recommended A category of the resource.  A resource may be associated with multiple themes. The set of themes used to categorize the resources are organized in a skos:ConceptScheme, skos:Collection, owl:Ontology or similar, describing all the categories and their relations in the catalog. publisher foaf:Agent recommended Agent responsible for making the resource available. versionNotes rdfs:Literal optional A description of the differences between this version and a previous version of the resource. This property can be repeated for parallel language versions of the version notes. hasVersion rdfs:Resource optional A related resource that is a version, edition, or adaptation of the described resource. This property is intended for relating a non-versioned or abstract resource to several versioned resources, e.g., snapshots. landingPage foaf:Document optional A web page that provides access to the resource and/or additional information (e.g. the distribution for a dataset). It is intended to point to a landing page at the original data provider, not to a page on a site of a third party, such as an aggregator. <p>For distributions, if they are accessible only through a landing page (i.e. direct download URLs are not known), then the landing page link should be duplicated as accessURL on a distribution.                                                                    | | qualifiedRelation     | dcat:Relationship                | optional    | A description of a relationship with another resource.                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                | | version               | rdfs:Literal                     | optional    | Version indicator (name or identifier) of a resource.                                                                                        | DCAT does not prescribe how a version name / identifier should be specified, and refers for guidance to [DWBP]'s Best Practice 7: Provide a version indicator.                                                                                                                                                                                                                                                 | | applicableLegislation | eli:LegalResource                | optional    | The legislation that mandates the creation or management of the resource.                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                | | accessRights          | dcterms:RightsStatement          | optional    | Information about who can access the resource or an indication of its security status.                                                       | Access Rights may include information regarding access or restrictions based on privacy, security, or other policies. The following preferred Rights Statement individuals are defined: <code>accr:PUBLIC</code>, <code>accr:NON_PUBLIC</code>, <code>accr:CONFIDENTIAL</code>, <code>accr:RESTRICTED</code>, <code>accr:SENSITIVE</code>                                                                                                                             | | conformsTo            | dcterms:Standard                 | optional    | An implementing rule or other specification.                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                | | contributor           | foaf:Agent                       | optional    | An entity responsible for making contributions to the resource.                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                | | creationDate          | rdfs:Literal                     | optional    | Date of creation                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                | | creator               | foaf:Agent                       | optional    | An entity responsible for producing the resource.                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                | | hasPart               | rdfs:Resource                    | optional    | A related resource that is included either physically or logically in the described resource.                                                |                                                                                                                                                                                                                                                                                                                                                                                                                | | isReferencedBy        | rdfs:Resource                    | optional    | A related resource, such as a publication, that references, cites, or otherwise points to the documented resource.                           |                                                                                                                                                                                                                                                                                                                                                                                                                | | releaseDate           | rdfs:Literal(xsd:date)       | optional    | The date of formal issuance (e.g., publication) of the resource.                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                | | language              | rdfs:Literal(xsd:string)     | optional    | A language of the resource.                                                                                                                  | Recommended practice is to use either a non-literal value representing a language from a controlled vocabulary such as ISO 639-2 or ISO 639-3, or a literal value consisting of an IETF Best Current Practice 47 [IETF-BCP47] language tag.</p> <p>Example values: \"en\", \"it\", \"no\", \"da\"</p> <p>This property can be repeated if the resource is expressed with multiple languages.  | | license               | dcterms:LicenseDocument          | optional    | A licence under which the resource is made available.                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                                                | | modificationDate      | rdfs:Literal(xsd:date)       | optional    | The most recent date on which the resource was changed or modified.                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                | | documentation         | foaf:Document                    | optional    | A page or document about this resource.                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                | | qualifiedAttribution  | prov:Attribution                 | optional    | An Agent having some form of responsibility for the resource.                                                                                |                                                                                                                                                                                                                                                                                                                                                                                                                | | abstract              | rdfs:Literal(rdf:langString) |             | A summary of the resource.                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                | | bibliographicCitation | rdfs:Literal                     |             | A bibliographic reference for the resource.                                                                                                  | Recommended practice is to include sufficient bibliographic detail to identify the resource as unambiguously as possible.                                                                                                                                                                                                                                                                                      | | conformance           | ddoc:ConformanceLevel            |             | Whether the annotation is mandatory, recommended or optional.                                                                                | It MUST take one of the values 'ddoc:mandatory', 'ddoc:recommended' or 'ddoc:optional'.                                                                                                                                                                                                                                                                                                                        | | curationDate          | xsd:date                         |             | Date of curation.                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                | | curator               | foaf:Agent                       |             | The agent that curated the resource.                                                                                                         | Use <code>ddoc:curationDate</code> to refer to the date of curation.                                                                                                                                                                                                                                                                                                                                                      | | statements            | rdfs:Literal(rdf:JSON)       |             | A list of subject-predicate-object triples with additional RDF statements documenting the resource.                                          |                                                                                                                                                                                                                                                                                                                                                                                                                | | deprecated            | rdfs:Literal(xsd:boolean)    |             | The annotation property that indicates that a given entity has been deprecated. It should equal to <code>\"true\"^^xsd:boolean</code>.                    |                                                                                                                                                                                                                                                                                                                                                                                                                | | comment               | rdfs:Literal                     |             | A description of the subject resource. Use <code>description</code> instead.                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                | | domain                | rdfs:Resource                    |             | A domain of the subject property.                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                | | isDefinedBy           | skos:Concept                     |             | Indicate a resource defining the subject resource. This property may be used to indicate an RDF vocabulary in which a resource is described. |                                                                                                                                                                                                                                                                                                                                                                                                                | | label                 | rdfs:Literal                     |             | Provides a human-readable version of a resource's name.                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                | | range                 | rdfs:Resource                    |             | A range of the subject property.                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                | | seeAlso               | skos:Concept                     |             | Indicates a resource that might provide additional information about the subject resource.                                                   |                                                                                                                                                                                                                                                                                                                                                                                                                | | definition            | rdfs:Literal(rdf:langString) |             | A statement or formal explanation of the meaning of a concept.                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                |</p>"},{"location":"datadoc/keywords/#properties-on-licensedocument","title":"Properties on LicenseDocument","text":"<p>A legal document giving official permission to do something with a resource.</p> Keyword Range Conformance Definition Usage note type skos:Concept optional A type of the resource. A recommended controlled vocabulary data-type is foreseen. For agents, the value should be chosen from ADMS publisher type."},{"location":"datadoc/keywords/#properties-on-location","title":"Properties on Location","text":"<p>A spatial region or named place.</p> Keyword Range Conformance Definition Usage note bbox rdfs:Literal recommended The geographic bounding box of a resource. centroid rdfs:Literal recommended The geographic center (centroid) of a resource. geometry locn:Geometry optional The geographic center (centroid) of a resource."},{"location":"datadoc/keywords/#properties-on-agent","title":"Properties on Agent","text":"<p>An agent (eg. person, group, software or physical artifact).</p> <ul> <li>subClassOf: dcterms:Agent, emmo:EMMO_2480b72b_db8d_460f_9a5f_c2912f979046</li> </ul> Keyword Range Conformance Definition Usage note name rdfs:Literal(xsd:string) mandatory A name of the agent. identifier rdfs:Literal optional URI or other unique identifier of the resource being described. Recommended practice is to identify the resource by means of a string conforming to an identification system. Examples include International Standard Book Number (ISBN), Digital Object Identifier (DOI), and Uniform Resource Name (URN).  Persistent identifiers should be provided as HTTP URIs."},{"location":"datadoc/keywords/#properties-on-generator","title":"Properties on Generator","text":"<p>A generator that can serialise an instance of a datamodel into a distribution.</p> <ul> <li>subClassOf: oteio:Parser</li> </ul> Keyword Range Conformance Definition Usage note generatorType rdfs:Literal Generator type. Ex: <code>application/vnd.dlite-generate</code>."},{"location":"datadoc/keywords/#properties-on-parser","title":"Properties on Parser","text":"<p>A parser that can parse a distribution into an instance of a datamodel.</p> Keyword Range Conformance Definition Usage note configuration rdfs:Literal(rdf:JSON) A JSON string with configurations specific to the parser or generator. parserType rdfs:Literal Parser type. Ex: <code>application/vnd.dlite-parse</code>."},{"location":"datadoc/keywords/#properties-on-restriction","title":"Properties on Restriction","text":"<p>The class of property restrictions.</p> Keyword Range Conformance Definition Usage note maxQualifiedCardinality rdfs:Literal(xsd:nonNegativeInteger) The property that determines the cardinality of a maximum qualified cardinality restriction. minQualifiedCardinality rdfs:Literal(xsd:nonNegativeInteger) The property that determines the cardinality of a minimum qualified cardinality restriction. qualifiedCardinality rdfs:Literal(xsd:nonNegativeInteger) The property that determines the cardinality of an exact qualified cardinality restriction."},{"location":"datadoc/keywords/#properties-on-class","title":"Properties on Class","text":"<p>The class of classes.</p> Keyword Range Conformance Definition Usage note conceptualisation rdfs:Literal(rdf:langString) A comment that helps the reader to understand how the world has been conceptualised by the ontology authors. elucidation rdfs:Literal(rdf:langString) Short enlightening explanation aimed to facilitate the user in drawing the connection (interpretation) between a OWL entity and the real world object(s) for which it stands.  It should address the real world entities using the concepts introduced by the conceptualisation. subClassOf rdfs:Class The subject is a subclass of a class. subPropertyOf rdf:Property The subject is a subproperty of a property. altLabel rdfs:Literal(rdf:langString) An alternative lexical label for a resource. hiddenLabel rdfs:Literal(rdf:langString) A lexical label for a resource that should be hidden when generating visual displays of the resource, but should still be accessible to free text search operations. prefLabel rdfs:Literal(rdf:langString) A preferred label of the concept."},{"location":"datadoc/keywords/#properties-on-resource_1","title":"Properties on Resource","text":"<p>Resource published or curated by an agent.</p> <ul> <li>subClassOf: rdfs:Resource</li> </ul> Keyword Range Conformance Definition Usage note datatype rdfs:Datatype A datatype for annotations and data properties. usageNote rdfs:Literal(rdf:langString) A reference that provides information on how this resource is to be used."},{"location":"datadoc/keywords/#properties-on-checksum","title":"Properties on Checksum","text":"<p>A preferred label of the concept.</p> Keyword Range Conformance Definition Usage note algorithm spdx:ChecksumAlgorithm The algorithm used to produce the subject Checksum. checksumValue rdfs:Literal(xsd:hexBinary) A lower case hexadecimal encoded digest value produced using a specific algorithm."},{"location":"datadoc/keywords/#properties-on-kind","title":"Properties on Kind","text":"<p>A description following the vCard specification, e.g. to provide telephone number and e-mail address for a contact point.</p> Keyword Range Conformance Definition Usage note hasAddress rdfs:Literal hasCountryName rdfs:Literal hasEmail rdfs:Literal hasFamilyName rdfs:Literal hasGender rdfs:Literal hasGeo rdfs:Literal hasGivenName rdfs:Literal hasHonorificPrefix rdfs:Literal hasHonorificSuffix rdfs:Literal hasInstantMessage rdfs:Literal hasKey rdfs:Literal hasLanguage rdfs:Literal hasLogo rdfs:Literal hasMember rdfs:Literal hasName rdfs:Literal hasNickname rdfs:Literal hasNote rdfs:Literal hasOrganizationName rdfs:Literal hasOrganizationUnit rdfs:Literal hasPhoto rdfs:Literal hasPostalCode rdfs:Literal hasRegion rdfs:Literal hasStreetAddress rdfs:Literal hasTelephone rdfs:Literal hasUID rdfs:Literal hasURL rdfs:Literal"},{"location":"datadoc/prefixes-process/","title":"Predefined prefixes","text":"<p>All namespace prefixes listed on this page are defined in the default JSON-LD context. See User-defined prefixes for how to extend this list with additional namespace prefixes.</p> Prefix Namespace ddoc https://w3id.org/emmo/application/datadoc# accr http://publications.europa.eu/resource/authority/access-right/ adms http://www.w3.org/ns/adms# dcat http://www.w3.org/ns/dcat# dcatap http://data.europa.eu/r5r/ dcterms http://purl.org/dc/terms/ dctype http://purl.org/dc/dcmitype/ eli http://data.europa.eu/eli/ontology# foaf http://xmlns.com/foaf/0.1/ iana https://www.iana.org/assignments/media-types/ locn http://www.w3.org/ns/locn# odrl http://www.w3.org/ns/odrl/2/ owl http://www.w3.org/2002/07/owl# prov http://www.w3.org/ns/prov# rdf http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs http://www.w3.org/2000/01/rdf-schema# schema https://schema.org/ skos http://www.w3.org/2004/02/skos/core# spdx http://spdx.org/rdf/terms# stat http://purl.org/adms/status/ vann http://purl.org/vocab/vann/ vcard http://www.w3.org/2006/vcard/ns# xsd http://www.w3.org/2001/XMLSchema# emmo https://w3id.org/emmo# oteio https://w3id.org/emmo/domain/oteio# chameo https://w3id.org/emmo/domain/characterisation-methodology/chameo#"},{"location":"datadoc/prefixes/","title":"Predefined prefixes","text":"<p>All namespace prefixes listed on this page are defined in the default JSON-LD context. See User-defined prefixes for how to extend this list with additional namespace prefixes.</p> Prefix Namespace ddoc https://w3id.org/emmo/application/datadoc# accr http://publications.europa.eu/resource/authority/access-right/ adms http://www.w3.org/ns/adms# dcat http://www.w3.org/ns/dcat# dcatap http://data.europa.eu/r5r/ dcterms http://purl.org/dc/terms/ dctype http://purl.org/dc/dcmitype/ eli http://data.europa.eu/eli/ontology# foaf http://xmlns.com/foaf/0.1/ iana https://www.iana.org/assignments/media-types/ locn http://www.w3.org/ns/locn# odrl http://www.w3.org/ns/odrl/2/ owl http://www.w3.org/2002/07/owl# prov http://www.w3.org/ns/prov# rdf http://www.w3.org/1999/02/22-rdf-syntax-ns# rdfs http://www.w3.org/2000/01/rdf-schema# schema https://schema.org/ skos http://www.w3.org/2004/02/skos/core# spdx http://spdx.org/rdf/terms# stat http://purl.org/adms/status/ vann http://purl.org/vocab/vann/ vcard http://www.w3.org/2006/vcard/ns# xsd http://www.w3.org/2001/XMLSchema# emmo https://w3id.org/emmo# oteio https://w3id.org/emmo/domain/oteio# chameo https://w3id.org/emmo/domain/characterisation-methodology/chameo#"},{"location":"tools/datadoc/","title":"Datadoc","text":"<p><code>datadoc</code> is a command-line tool for data documentation which comes with Tripper. It provides functionality for both populating a triplestore with data documentation and to search for and find the documentation at a later stage.</p> <p>Running <code>datadoc --help</code> from the shell will show the following help message:</p> <pre><code>usage: datadoc [-h] [--config CONFIG] [--triplestore TRIPLESTORE]\n               [--backend BACKEND] [--base-iri BASE_IRI] [--database DATABASE]\n               [--package PACKAGE] [--parse LOCATION]\n               [--parse-format PARSE_FORMAT] [--prefix PREFIX=URL]\n               {add,find,fetch} ...\n\nTool for data documentation. It allows populating and searching a triplestore\nfor existing documentation.\n\npositional arguments:\n  {add,find,fetch}      Subcommands:\n    add                 Populate the triplestore with data documentation.\n    find                Find documented resources in the triplestore.\n    fetch               Fetch documented dataset from a storage.\n\noptions:\n  -h, --help            show this help message and exit\n  --config CONFIG, -c CONFIG\n                        Session configuration file.\n  --triplestore TRIPLESTORE, -t TRIPLESTORE\n                        Name of triplestore to connect to. The name should be\n                        defined in the session configuration file.\n  --backend BACKEND, -b BACKEND\n                        Triplestore backend to use. Defaults to \"rdflib\" - an\n                        in-memory rdflib triplestore, that can be pre-loaded\n                        with --parse.\n  --base-iri BASE_IRI, -B BASE_IRI\n                        Base IRI of the triplestore.\n  --database DATABASE, -d DATABASE\n                        Name of database to connect to (for backends\n                        supporting it).\n  --package PACKAGE     Only needed when `backend` is a relative module.\n  --parse LOCATION, -p LOCATION\n                        Load triplestore from this location.\n  --parse-format PARSE_FORMAT, -F PARSE_FORMAT\n                        Used with `--parse`. Format to use when parsing\n                        triplestore.\n  --prefix PREFIX=URL, -P PREFIX=URL\n                        Namespace prefix to bind to the triplestore. This\n                        option can be given multiple times.\n</code></pre> <p>Currently, <code>datadoc</code> has currently three sub-commands, <code>add</code>, <code>find</code> and <code>load</code> for populating  the triplestore, searching  the triplestore and accessing a dataset documented the triplestore, respectively.</p>"},{"location":"tools/datadoc/#general-options","title":"General options","text":"<ul> <li> <p>The <code>--config</code>, <code>--triplestore</code>, <code>--backend</code>, <code>--base-iri</code>, <code>--database</code> and <code>--package</code> options are all for connecting to a triplestore.   The most convenient method is to configure a session and use the <code>--triplestore</code> argument to select the triplestore to connect to.</p> </li> <li> <p>The <code>--parse</code>, <code>--parse-format</code> and <code>--prefix</code> options are for pre-loading the triplestore with triples from an external source, like a ntriples or turtle file, and for adding namespace prefixes. They are typically used with the default \"rdflib\" in-memory backend.</p> </li> </ul>"},{"location":"tools/datadoc/#subcommands","title":"Subcommands","text":""},{"location":"tools/datadoc/#add","title":"add","text":"<p>Populates the triplestore with data documentation. Running <code>datadoc add --help</code> will show the following help message:</p> <pre><code>usage: datadoc add [-h] [--input-format {yaml,csv}]\n                   [--csv-options OPTION=VALUE [OPTION=VALUE ...]]\n                   [--context CONTEXT] [--dump FILENAME] [--format FORMAT]\n                   input\n\npositional arguments:\n  input                 Path or URL to the input the triplestore should be\n                        populated from.\n\noptions:\n  -h, --help            show this help message and exit\n  --input-format {yaml,csv}, -i {yaml,csv}\n                        Input format. By default it is inferred from the file\n                        extension of the `input` argument.\n  --csv-options OPTION=VALUE [OPTION=VALUE ...]\n                        Options describing the CSV dialect for --input-\n                        format=csv. Common options are 'dialect', 'delimiter'\n                        and 'quotechar'.\n  --context CONTEXT     Path or URL to custom JSON-LD context for the input.\n  --dump FILENAME, -d FILENAME\n                        File to dump the populated triplestore to.\n  --format FORMAT, -f FORMAT\n                        Format to use with `--dump`. Default is \"turtle\".\n</code></pre> <p>The positional <code>input</code> argument is the path or URL to the source from which to populate the triplestore. The <code>--input-format</code>, <code>--csv-options</code> and <code>--context</code> options provide more details about the input.</p> <p>The <code>--dump</code> and <code>--format</code> options allow to dump the populated triplestore to file. This is useful if you are working with an in-memory triplestore.</p> <p>Example</p> <p>The <code>tests/input/</code> folder in the source code contain the <code>semdata.csv</code> CSV file documenting four datasets, a SEM image, two nested dataset series and the sample the image was acquired from as shown in the figure below (click on it to see it in full size).</p> <p></p> <p>Running the following command from the root folder of the source code will populate an in-memory rdflib store with the data documented in the <code>semdata.csv</code> file.</p> <pre><code>datadoc add tests/input/semdata.csv --context tests/input/semdata-context.json --dump kb.ttl\n</code></pre> <p>The <code>--context</code> option provides a user-defined context defining prefixes and keywords used by the input.</p> <p>The <code>--dump</code> option dumps the in-memory triplestore to the file <code>kb.ttl</code>. If you open the file, you will notice that it in addition to the four datasets listed in the input, also include the <code>SEMImage</code> class and its properties, providing structural documentation of the <code>SEMImage</code> individuals.</p> Generated turtle file <pre><code>@prefix chameo: &lt;https://w3id.org/emmo/domain/characterisation-methodology/chameo#&gt; .\n@prefix dcat: &lt;http://www.w3.org/ns/dcat#&gt; .\n@prefix dcterms: &lt;http://purl.org/dc/terms/&gt; .\n@prefix emmo: &lt;https://w3id.org/emmo#&gt; .\n@prefix oteio: &lt;https://w3id.org/emmo/domain/oteio#&gt; .\n@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix skos: &lt;http://www.w3.org/2004/02/skos/core#&gt; .\n@prefix xsd: &lt;http://www.w3.org/2001/XMLSchema#&gt; .\n\n&lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage&gt; a owl:Class ;\n    rdfs:subClassOf [ a owl:Restriction ;\n            owl:onClass &lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#data&gt; ;\n            owl:onProperty emmo:EMMO_b19aacfc_5f73_4c33_9456_469c1e89a53e ;\n            owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ],\n        [ a owl:Restriction ;\n            owl:onClass &lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#labels&gt; ;\n            owl:onProperty emmo:EMMO_b19aacfc_5f73_4c33_9456_469c1e89a53e ;\n            owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ],\n        [ a owl:Restriction ;\n            owl:onClass &lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#metadata&gt; ;\n            owl:onProperty emmo:EMMO_b19aacfc_5f73_4c33_9456_469c1e89a53e ;\n            owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ],\n        [ a owl:Restriction ;\n            owl:onClass &lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#pixelheight&gt; ;\n            owl:onProperty emmo:EMMO_b19aacfc_5f73_4c33_9456_469c1e89a53e ;\n            owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ],\n        [ a owl:Restriction ;\n            owl:onClass &lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#pixelwidth&gt; ;\n            owl:onProperty emmo:EMMO_b19aacfc_5f73_4c33_9456_469c1e89a53e ;\n            owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ],\n        emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a ;\n    skos:prefLabel \"SEMImage\"@en ;\n    emmo:EMMO_967080e5_2f42_4eb2_a3a9_c58143e835f9 \"SEM image with elemental mappings. Represented as a stack of elemental mapping\\\\nfollowed by the image formed by the back-scattered electrons (BSE).\\\\nSet `nelements=0` if you only have the back-scattered image.\\\\n\"@en ;\n    oteio:hasURI \"http://onto-ns.com/meta/matchmaker/0.2/SEMImage\"^^xsd:anyURI .\n\n&lt;https://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001/77600-23-001_5kV_400x_m001&gt; a dcat:Dataset,\n        &lt;https://w3id.com/emmo/domain/sem/0.1#SEMImage&gt;,\n        emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a ;\n    dcterms:creator \"Sigurd Wenner\" ;\n    dcterms:description \"Back-scattered SEM image of cement sample 77600 from Heidelberg, polished with 1 \u00b5m diamond compound.\" ;\n    dcterms:title \"SEM image of cement\" ;\n    dcat:contactPoint \"Sigurd Wenner &lt;Sigurd.Wenner@sintef.no&gt;\" ;\n    dcat:distribution [ a dcat:Distribution ;\n            &lt;http://sintef.no/dlite/parser#&gt; \"http://sintef.no/dlite/parser#sem_hitachi\" ;\n            dcat:downloadURL \"https://github.com/EMMC-ASBL/tripper/raw/refs/heads/master/tests/input/77600-23-001_5kV_400x_m001.tif\" ;\n            dcat:mediaType \"image/tiff\" ] ;\n    dcat:inSeries &lt;https://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001&gt; ;\n    emmo:EMMO_f702bad4_fc77_41f0_a26d_79f6444fd4f3 &lt;https://he-matchmaker.eu/material/concrete1&gt; ;\n    oteio:hasDatamodel \"http://onto-ns.com/meta/matchmaker/0.2/SEMImage\" ;\n    oteio:hasDatamodelStorage \"https://github.com/HEU-MatCHMaker/DataDocumentation/blob/master/SEM/datamodels/SEMImage.yaml\" .\n\n&lt;https://he-matchmaker.eu/sample/SEM_cement_batch2/77600-23-001&gt; a dcat:Dataset,\n        emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a,\n        chameo:Sample ;\n    dcterms:title \"Series for SEM images for sample 77600-23-001.\" .\n\n&lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#data&gt; a owl:Class ;\n    rdfs:subClassOf [ a owl:Restriction ;\n            owl:hasValue &lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#data_dimension0&gt; ;\n            owl:onProperty emmo:EMMO_0a9ae0cb_526d_4377_9a11_63d1ce5b3499 ],\n        [ a owl:Restriction ;\n            owl:onProperty emmo:EMMO_e5a34647_a955_40bc_8d81_9b784f0ac527 ;\n            owl:someValuesFrom emmo:EMMO_ac9e518d_b403_4d8b_97e2_06f9d40bac01 ],\n        emmo:EMMO_28fbea28_2204_4613_87ff_6d877b855fcd,\n        emmo:EMMO_50d6236a_7667_4883_8ae1_9bb5d190423a ;\n    skos:prefLabel \"Data\"@en ;\n    emmo:EMMO_967080e5_2f42_4eb2_a3a9_c58143e835f9 \"Image data - a stack of images for each channel\"@en ;\n    oteio:datasize \"4\"^^xsd:nonNegativeInteger .\n\n&lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#data_dimension0&gt; a emmo:EMMO_b4c97fa0_d82c_406a_bda7_597d6e190654 ;\n    skos:prefLabel \"data_dimension0\"@en ;\n    emmo:EMMO_23b579e1_8088_45b5_9975_064014026c42 \"channels\"^^xsd:string ;\n    emmo:EMMO_499e24a5_5072_4c83_8625_fe3f96ae4a8d &lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#data_dimension1&gt; ;\n    emmo:EMMO_967080e5_2f42_4eb2_a3a9_c58143e835f9 \"Number of channels.\"@en .\n\n&lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#data_dimension1&gt; a emmo:EMMO_b4c97fa0_d82c_406a_bda7_597d6e190654 ;\n    skos:prefLabel \"data_dimension1\"@en ;\n    emmo:EMMO_23b579e1_8088_45b5_9975_064014026c42 \"height\"^^xsd:string ;\n    emmo:EMMO_499e24a5_5072_4c83_8625_fe3f96ae4a8d &lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#data_dimension2&gt; ;\n    emmo:EMMO_967080e5_2f42_4eb2_a3a9_c58143e835f9 \"Number of pixels along the image height.\"@en .\n\n&lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#data_dimension2&gt; a emmo:EMMO_b4c97fa0_d82c_406a_bda7_597d6e190654 ;\n    skos:prefLabel \"data_dimension2\"@en ;\n    emmo:EMMO_23b579e1_8088_45b5_9975_064014026c42 \"width\"^^xsd:string ;\n    emmo:EMMO_967080e5_2f42_4eb2_a3a9_c58143e835f9 \"Number of pixels along the image width.\"@en .\n\n&lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#labels&gt; a owl:Class ;\n    rdfs:subClassOf [ a owl:Restriction ;\n            owl:hasValue &lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#labels_dimension0&gt; ;\n            owl:onProperty emmo:EMMO_0a9ae0cb_526d_4377_9a11_63d1ce5b3499 ],\n        [ a owl:Restriction ;\n            owl:onProperty emmo:EMMO_e5a34647_a955_40bc_8d81_9b784f0ac527 ;\n            owl:someValuesFrom emmo:EMMO_5f334606_f67d_4f0e_acb9_eeb21cb10c66 ],\n        emmo:EMMO_28fbea28_2204_4613_87ff_6d877b855fcd,\n        emmo:EMMO_50d6236a_7667_4883_8ae1_9bb5d190423a ;\n    skos:prefLabel \"Labels\"@en ;\n    emmo:EMMO_967080e5_2f42_4eb2_a3a9_c58143e835f9 \"The label of each channel. For elemental mapping this should be the chemical symbol of the element or BSE for the back-scattered image.\"@en .\n\n&lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#labels_dimension0&gt; a emmo:EMMO_b4c97fa0_d82c_406a_bda7_597d6e190654 ;\n    skos:prefLabel \"labels_dimension0\"@en ;\n    emmo:EMMO_23b579e1_8088_45b5_9975_064014026c42 \"channels\"^^xsd:string ;\n    emmo:EMMO_967080e5_2f42_4eb2_a3a9_c58143e835f9 \"Number of channels.\"@en .\n\n&lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#metadata&gt; a owl:Class ;\n    rdfs:subClassOf emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a,\n        emmo:EMMO_50d6236a_7667_4883_8ae1_9bb5d190423a ;\n    skos:prefLabel \"Metadata\"@en ;\n    emmo:EMMO_967080e5_2f42_4eb2_a3a9_c58143e835f9 \"Reference to data model for SEM metadata.\"@en .\n\n&lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#pixelheight&gt; a owl:Class ;\n    rdfs:subClassOf [ a owl:Restriction ;\n            owl:onClass emmo:Metre ;\n            owl:onProperty emmo:EMMO_bed1d005_b04e_4a90_94cf_02bc678a8569 ;\n            owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ],\n        emmo:EMMO_50d6236a_7667_4883_8ae1_9bb5d190423a,\n        emmo:EMMO_52fa9c76_fc42_4eca_a5c1_6095a1c9caab ;\n    skos:prefLabel \"Pixelheight\"@en ;\n    emmo:EMMO_967080e5_2f42_4eb2_a3a9_c58143e835f9 \"Height of each pixel.\"@en ;\n    oteio:datasize \"8\"^^xsd:nonNegativeInteger .\n\n&lt;http://onto-ns.com/meta/matchmaker/0.2/SEMImage#pixelwidth&gt; a owl:Class ;\n    rdfs:subClassOf [ a owl:Restriction ;\n            owl:onClass emmo:Metre ;\n            owl:onProperty emmo:EMMO_bed1d005_b04e_4a90_94cf_02bc678a8569 ;\n            owl:qualifiedCardinality \"1\"^^xsd:nonNegativeInteger ],\n        emmo:EMMO_50d6236a_7667_4883_8ae1_9bb5d190423a,\n        emmo:EMMO_52fa9c76_fc42_4eca_a5c1_6095a1c9caab ;\n    skos:prefLabel \"Pixelwidth\"@en ;\n    emmo:EMMO_967080e5_2f42_4eb2_a3a9_c58143e835f9 \"Width of each pixel.\"@en ;\n    oteio:datasize \"8\"^^xsd:nonNegativeInteger .\n\n&lt;https://he-matchmaker.eu/data/sem/SEM_cement_batch2&gt; a dcat:Dataset,\n        &lt;https://w3id.com/emmo/domain/sem/0.1#SEMImageSeries&gt;,\n        emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a ;\n    dcterms:creator \"Sigurd Wenner\" ;\n    dcterms:description \"...\" ;\n    dcterms:title \"Nested series of SEM images of cement batch2\" ;\n    dcat:contactPoint \"Sigurd Wenner &lt;Sigurd.Wenner@sintef.no&gt;\" ;\n    dcat:distribution [ a dcat:Distribution ;\n            dcat:downloadURL \"sftp://nas.aimen.es/P_MATCHMAKER_SHARE_SINTEF/SEM_cement_batch2\" ;\n            dcat:mediaType \"inode/directory\" ] .\n\n&lt;https://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001&gt; a dcat:Dataset,\n        &lt;https://w3id.com/emmo/domain/sem/0.1#SEMImageSeries&gt;,\n        emmo:EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a ;\n    dcterms:creator \"Sigurd Wenner\" ;\n    dcterms:description \"Back-scattered SEM image of cement sample 77600, polished with 1 \u00b5m diamond compound.\" ;\n    dcterms:title \"Series of SEM image of cement sample 77600\" ;\n    dcat:contactPoint \"Sigurd Wenner &lt;Sigurd.Wenner@sintef.no&gt;\" ;\n    dcat:distribution [ a dcat:Distribution ;\n            dcat:downloadURL \"sftp://nas.aimen.es/P_MATCHMAKER_SHARE_SINTEF/SEM_cement_batch2/77600-23-001\" ;\n            dcat:mediaType \"inode/directory\" ] ;\n    dcat:inSeries &lt;https://he-matchmaker.eu/data/sem/SEM_cement_batch2&gt; .\n</code></pre>"},{"location":"tools/datadoc/#find","title":"find","text":"<p>Search the triplestore for documented resources. Running <code>datadoc find --help</code> will show the following help message:</p> <pre><code>usage: datadoc find [-h] [--type TYPE]\n                    [--criteria KEYWORD=VALUE [KEYWORD=VALUE ...]]\n                    [--output FILENAME] [--format {iris,json,turtle,csv}]\n\noptions:\n  -h, --help            show this help message and exit\n  --type TYPE, -t TYPE  Either a resource type (ex: \"Dataset\", \"Distribution\",\n                        ...) or the IRI of a class to limit the search to.\n  --criteria IRI=VALUE, -c IRI=VALUE\n                        Matching criteria for resources to find. The IRI may\n                        be written using a namespace prefix, like\n                        `tcterms:title=\"My title\"`. Currently only exact\n                        matching is supported. This option can be given\n                        multiple times.\n  --output FILENAME, -o FILENAME\n                        Write matching output to the given file. The default\n                        is to write to standard output.\n  --format {iris,json,turtle,csv}, -f {iris,json,turtle,csv}\n                        Output format to list the matched resources. The\n                        default is to infer from the file extension if\n                        --output is given. Otherwise it defaults to \"iris\".\n</code></pre> <p>The <code>--type</code> and <code>--criteria</code> options provide search criteria. The <code>--type</code> option can be any of the classes in the domain to limit the search to. See Predefined keywords for a list of available classes in the \"default\" domain. Alternatively, it may be the IRI of a class. This limits the search to only resources that are individuals of this class.</p> <p>The <code>--output</code> option allows to write the matching output to file instead of standard output.</p> <p>The <code>--format</code> option controls how the search result should be presented. The following formats are currently available:</p> <ul> <li>iris: Return the IRIs of matching resources.</li> <li>json: Return a JSON array with documentation of matching resources.</li> <li> <p>turtle: Return a turtle representation of matching resources.</p> <p>Note: In case the matching resources are datasets with a <code>datamodel</code> keyword, the serialised data model will also be included in the turtle output.</p> </li> <li> <p>csv: Return a CSV table with the matching resources.</p> </li> </ul> <p>Examples</p> <p>For all the examples below, we use <code>--parse</code> option to pre-load the triplestore from the <code>kb.ttl</code> file that we generated in the previous example.</p> <p>Ex 1: List IRIs of all datasets:</p> <pre><code>$ datadoc --parse=kb.ttl find --type=dataset\nhttps://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001/77600-23-001_5kV_400x_m001\nhttps://he-matchmaker.eu/sample/SEM_cement_batch2/77600-23-001\nhttps://he-matchmaker.eu/data/sem/SEM_cement_batch2\nhttps://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001\n</code></pre> <p>Ex 2: List IRIs of all samples (individuals of <code>chameo:Sample</code>):</p> <pre><code>$ datadoc --parse=kb.ttl find --type=chameo:Sample\nhttps://he-matchmaker.eu/sample/SEM_cement_batch2/77600-23-001\n</code></pre> <p>Ex 3: List IRIs of all resources with a given title:</p> <pre><code>$ datadoc --parse=kb.ttl find --criteria dcterms:title=\"Series of SEM image of cement sample 77600\"\nhttps://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001\n</code></pre> <p>Ex 4: List all sample individuals as JSON:</p> <pre><code>$ datadoc --parse=kb.ttl find --type=chameo:Sample --format=json\n</code></pre> <pre><code>[\n  {\n    \"@id\": \"https://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001\",\n    \"@type\": [\n      \"http://www.w3.org/ns/dcat#Dataset\",\n      \"https://w3id.com/emmo/domain/sem/0.1#SEMImageSeries\",\n      \"https://w3id.org/emmo#EMMO_194e367c_9783_4bf5_96d0_9ad597d48d9a\"\n    ],\n    \"creator\": \"Sigurd Wenner\",\n    \"description\": \"Back-scattered SEM image of cement sample 77600, polished with 1 \\u00b5m diamond compound.\",\n    \"title\": \"Series of SEM image of cement sample 77600\",\n    \"contactPoint\": \"Sigurd Wenner &lt;Sigurd.Wenner@sintef.no&gt;\",\n    \"distribution\": {\n      \"@id\": \"_:nac2552b949a94ef391080807ca2a02e4b14\",\n      \"@type\": \"http://www.w3.org/ns/dcat#Distribution\",\n      \"downloadURL\": \"sftp://nas.aimen.es/P_MATCHMAKER_SHARE_SINTEF/SEM_cement_batch2/77600-23-001\",\n      \"mediaType\": \"inode/directory\"\n    },\n    \"inSeries\": \"https://he-matchmaker.eu/data/sem/SEM_cement_batch2\"\n  }\n]\n</code></pre> <p>Ex 5: Show the documentation of a resource with a given IRI as JSON:</p> <pre><code>$ datadoc --parse=kb.ttl find --criteria @id=https://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001 --format=json\n</code></pre> <p>This will show the same output as in Ex 4.</p>"},{"location":"tools/datadoc/#fetch","title":"fetch","text":"<p>Fetch documented dataset from a storage. Running <code>datadoc fetch --help</code> will show the following help message:</p> <pre><code>usage: datadoc fetch [-h] [--output FILENAME] iri\n\npositional arguments:\n  iri                   IRI of dataset to fetch.\n\noptions:\n  -h, --help            show this help message and exit\n  --output FILENAME, -o FILENAME\n                        Write the dataset to the given file. The default is to\n                        write to standard output.\n</code></pre> <p>Note</p> <p>The <code>fetch</code> subcommand is specific for datasets since it uses DCAT documentation of how to fetch the dataset.</p> <p>The positional <code>iri</code> argument is the IRI of the documented dataset to fetch.</p> <p>The <code>--output</code> option allows to write the dataset to a local file.</p> <p>Example</p> <p>Save the dataset <code>https://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001/77600-23-001_5kV_400x_m001</code> documented in <code>kb.ttl</code> to file:</p> <pre><code>$ datadoc -p kb.ttl fetch https://he-matchmaker.eu/data/sem/SEM_cement_batch2/77600-23-001/77600-23-001_5kV_400x_m001 -o cement.tif\n</code></pre> <p>This should create the file <code>cement.tif</code> containing the image data.</p>"},{"location":"units/units/","title":"Units and quantities","text":"<p>The tripper.units subpackage provides support for using Pint to work with units and quantites defined in ontologies.</p> <p>Currently, only EMMO and EMMO-based ontologies can be used as a source for units and quantities. However, since EMMO includes references to the QUDT and OM ontologies, it is also possible to work with IRIs for these ontologies.</p> <p>Note</p> <p>Currently the support for OM is weak.  Improvements are planned.</p>"},{"location":"units/units/#unit-registry","title":"Unit registry","text":"<p>The UnitRegistry in tripper.units is a subclass of the Pint unit registry. By default it is populated with units from EMMO.</p> <pre><code>&gt;&gt;&gt; from tripper.units import UnitRegistry\n&gt;&gt;&gt; ureg = UnitRegistry()\n</code></pre> <p>The registry provides attribute and item access to units based on their EMMO prefLabel or symbol.</p> <pre><code>&gt;&gt;&gt; ureg.Pascal\n&lt;Unit('Pascal')&gt;\n\n&gt;&gt;&gt; ureg.Pa\n&lt;Unit('Pascal')&gt;\n</code></pre> <p>By convention, EMMO units are written in \"CamelCase\". However, unit access also works with \"snake_case\":</p> <pre><code>&gt;&gt;&gt; ureg.pascal\n&lt;Unit('Pascal')&gt;\n\n&gt;&gt;&gt; ureg.newton_square_metre\n&lt;Unit('NewtonSquareMetre')&gt;\n</code></pre> <p>Item access creates a subclass of a Pint quantity representation (see Working with quantities):</p> <pre><code>&gt;&gt;&gt; ureg[\"Pa\"]\n&lt;Quantity(1, 'Pascal')&gt;\n\n&gt;&gt;&gt; ureg[\"A/m\u00b2\"]\n&lt;Quantity(1.0, 'Ampere / Metre ** 2')&gt;\n</code></pre>"},{"location":"units/units/#extra-unit-registry-methods","title":"Extra unit registry methods","text":"<p>Tripper adds some extra methods to the unit registry on top of what is already provided by Pint, including:</p> <ul> <li>get_unit(): Returns a Pint unit object derived from unit name, symbol, IRI (supporting EMMO, QUDT and OM), or unit code defined in the ontology.</li> <li>get_unit_info(): Returns a dict with attribute access providing additional information about the unit.</li> <li>get_quantity(): Returns the Pint quantity (i.e. value and unit) of a quantity in the ontology.</li> <li>load_quantity(): Loads a quantity from a triplestore.</li> <li>save_quantity(): Saves a quantity to a triplestore.</li> <li>set_as_default(): Set the current unit registry as the default. This allows to access the registry with the get_ureg() method.</li> <li>clear_cache(): Clear caches.</li> </ul> <p>Here we will only discuss get_unit() and get_unit_info() methods. See Accessing quantities in a triplestore and Setting up custom unit registry for the rest.</p> <p>For example:</p> <pre><code>&gt;&gt;&gt; ureg.get_unit(name=\"Metre\")  # name\n&lt;Unit('Metre')&gt;\n\n&gt;&gt;&gt; ureg.get_unit(symbol=\"H/\u03a9\")  # EMMO symbol\n&lt;Unit('HenryPerOhm')&gt;\n\n&gt;&gt;&gt; ureg.get_unit(symbol=\"H.Ohm-1\")  # UCUM symbol\n&lt;Unit('HenryPerOhm')&gt;\n\n&gt;&gt;&gt; ureg.get_unit(iri=\"https://w3id.org/emmo#Metre\")  # EMMO IRI\n&lt;Unit('Metre')&gt;\n\n&gt;&gt;&gt; ureg.get_unit(iri=\"http://qudt.org/vocab/unit/HR\")  # QUDT IRI\n&lt;Unit('Hour')&gt;\n\n&gt;&gt;&gt; ureg.get_unit(iri=\"http://www.ontology-of-units-of-measure.org/resource/om-2/cubicMetre\")  # OM IRI\n&lt;Unit('CubicMetre')&gt;\n\n&gt;&gt;&gt; ureg.get_unit(unitCode=\"HUR\")\n&lt;Unit('Hour')&gt;\n</code></pre> <p>When you have a unit, you can also ask it for its IRI using its <code>emmoIRI</code>, <code>qudtIRI</code> and <code>omIRI</code> properties:</p> <pre><code>&gt;&gt;&gt; ureg.CubicMetre.emmoIRI\n'https://w3id.org/emmo#CubicMetre'\n\n&gt;&gt;&gt; ureg.CubicMetre.qudtIRI\n'http://qudt.org/vocab/unit/M3'\n\n&gt;&gt;&gt; ureg.CubicMetre.omIRI\n'http://www.ontology-of-units-of-measure.org/resource/om-2/cubicMetre'\n</code></pre> <p>Units have an info property providing a dict with attribute access with description of the unit provided by the ontology. It contains the following fields:</p> <ul> <li>name: Preferred label.</li> <li>description: Unit description.</li> <li>aliases: List of alternative labels.</li> <li>symbols: List with unit symbols.</li> <li>dimension: Named tuple with quantity dimension.</li> <li>emmoIRI: IRI of the unit in the EMMO ontology.</li> <li>qudtIRI: IRI of the unit in the QUDT ontology.</li> <li>omIRI: IRI of the unit in the OM ontology.</li> <li>ucumCodes: List of UCUM codes for the unit.</li> <li>unitCode: UNECE common code for the unit.</li> <li>multiplier: Unit multiplier.</li> <li>offset: Unit offset.</li> </ul> <p>This dict can also be accessed with the get_unit() method.</p> <p>For example:</p> <pre><code>&gt;&gt;&gt; info = ureg.CubicMetre.info\n&gt;&gt;&gt; info.name\n'CubicMetre'\n\n&gt;&gt;&gt; info.dimension\nDimension(T=0, L=3, M=0, I=0, H=0, N=0, J=0)\n</code></pre> <p>The same dict can also be accessed from the unit registry with the <code>get_unit_info()</code> method.</p> <pre><code>&gt;&gt;&gt; info = ureg.get_unit_info(\"CubicMetre\")\n&gt;&gt;&gt; info.name\n'CubicMetre'\n</code></pre>"},{"location":"units/units/#working-with-quantities","title":"Working with quantities","text":"<p>Physical quantities consisting of a numerical value and a unit, can be constructed in several ways.</p> <p>For example by using ureg.Quantity() with two arguments</p> <pre><code>&gt;&gt;&gt; length = ureg.Quantity(6, \"km\")\n&gt;&gt;&gt; length\n&lt;Quantity(6, 'KiloMetre')&gt;\n</code></pre> <p>or with a single string argument</p> <pre><code>&gt;&gt;&gt; time = ureg.Quantity(\"1.2 h\")\n&gt;&gt;&gt; time\n&lt;Quantity(1.2, 'Hour')&gt;\n</code></pre> <p>or by multiplying a numerical value with a unit</p> <pre><code>&gt;&gt;&gt; pressure = 101325 * ureg.Pa\n&gt;&gt;&gt; pressure\n&lt;Quantity(101325, 'Pascal')&gt;\n</code></pre> <p>The magnitude and unit of a quantity can be accessed individually with the properties <code>m</code> and <code>u</code>:</p> <pre><code>&gt;&gt;&gt; pressure.m\n101325\n\n&gt;&gt;&gt; pressure.u\n&lt;Unit('Pascal')&gt;\n</code></pre> <p>By default, EMMO does not include prefixed units (with a few exceptions). It is therefore not uncommon to have a quantity with a unit not in the ontology. For example:</p> <pre><code>&gt;&gt;&gt; duration = 1.2 * ureg.kh\n&gt;&gt;&gt; duration\n&lt;Quantity(1.2, 'KiloHour')&gt;\n\n# This raises an exception since KiloHour is not in the ontology\n&gt;&gt;&gt; duration.u.info\nTraceback (most recent call last):\n...\ntripper.units.units.MissingUnitError: name=KiloHour\n</code></pre> <p>You can use to to_ontology_units() method (or its in-place variant ito_ontology_units()) to rescale the quantity to a unit that exists in the ontology:</p> <pre><code>&gt;&gt;&gt; duration.ito_ontology_units()\n&gt;&gt;&gt; f\"{duration:.1f}\"  # avoid rounding errors\n'50.0 Day'\n\n# The unit now has a `info` property\n&gt;&gt;&gt; duration.u.info.qudtIRI\n'http://qudt.org/vocab/unit/DAY'\n</code></pre> <p>The get_quantity() method allows to represent a physical quantity in the ontology as a Pint quantity. By default access is by name (prefLabel):</p> <pre><code>&gt;&gt;&gt; ureg.get_quantity(\"Energy\")\n&lt;Quantity(1.0, 'Joule')&gt;\n\n# A value can also be provided (in units of the base SI units)\n&gt;&gt;&gt; ureg.get_quantity(\"Energy\", value=2.5)\n&lt;Quantity(2.5, 'Joule')&gt;\n\n&gt;&gt;&gt; q = ureg.get_quantity(\"Energy\", value=1e-19)\n&gt;&gt;&gt; f\"{q:.4~P}\"\n'0.6242 eV'\n</code></pre> <p>Access by \"emmoIRI\", \"qudtIRI\", \"omIRI\", \"iupacIRI\" or \"iso80000Ref\" is also possible:</p> <pre><code>&gt;&gt;&gt; from tripper import EMMO\n&gt;&gt;&gt; ureg.get_quantity(iri=EMMO.Acceleration)\n&lt;Quantity(1.0, 'MetrePerSquareSecond')&gt;\n\n&gt;&gt;&gt; ureg.get_quantity(iso80000Ref=\"3-9.1\")  # also acceleration\n&lt;Quantity(1.0, 'MetrePerSquareSecond')&gt;\n</code></pre>"},{"location":"units/units/#quantities-as-literals","title":"Quantities as literals","text":"<p>Quantities are also understood by the Literal constructor</p> <pre><code>&gt;&gt;&gt; from tripper import Literal\n&gt;&gt;&gt; literal = Literal(pressure)\n&gt;&gt;&gt; literal\nLiteral('101325 Pa', datatype='https://w3id.org/emmo#EMMO_799c067b_083f_4365_9452_1f1433b03676')\n\n# Check the label of the datatype\n&gt;&gt;&gt; from tripper import EMMO\n&gt;&gt;&gt; EMMO._get_labels(literal.datatype)\n['SIQuantityDatatype']\n</code></pre> <p>which uses the <code>emmo:SIQuantityDatatype</code> datatype. The Literal.value property and Literal.n3() method can be used to convert back to a quantity or represent it in N3 notation:</p> <pre><code>&gt;&gt;&gt; literal = Literal(pressure)\n&gt;&gt;&gt; literal.value\n&lt;Quantity(101325, 'Pascal')&gt;\n\n&gt;&gt;&gt; literal.n3()\n'\"101325 Pa\"^^&lt;https://w3id.org/emmo#EMMO_799c067b_083f_4365_9452_1f1433b03676&gt;'\n</code></pre>"},{"location":"units/units/#accessing-quantities-in-a-triplestore","title":"Accessing quantities in a triplestore","text":"<p>Lets do a small calculation using the quantities constructed above:</p> <pre><code>&gt;&gt;&gt; mean_speed = length / time\n&gt;&gt;&gt; mean_speed\n&lt;Quantity(5.0, 'KiloMetre / Hour')&gt;\n</code></pre> <p>and store our calculated <code>mean_speed</code> to a triplestore:</p> <pre><code>&gt;&gt;&gt; from tripper import EMMO, Triplestore\n&gt;&gt;&gt; ts = Triplestore(backend=\"rdflib\")\n&gt;&gt;&gt; NS = ts.bind(\"\", \"http://example.com#\")\n&gt;&gt;&gt; ureg.save_quantity(ts, mean_speed, iri=NS.MeanSpeed, type=EMMO.Speed)\n</code></pre> <p>Above we have created a new triplestore, bound empty prefix to the namespace <code>http://example.com#</code> and saved the calculated <code>mean_speed</code> to a new individual with IRI <code>http://example.com#MeanSpeed</code>. The final <code>type</code> argument to ureg.save_quantity() states that our new individual will be an individual of the class <code>emmo:Speed</code>.</p> <p>The content of the triplestore is now</p> <pre><code>&gt;&gt;&gt; print(ts.serialize())\n@prefix : &lt;http://example.com#&gt; .\n@prefix emmo: &lt;https://w3id.org/emmo#&gt; .\n&lt;BLANKLINE&gt;\n:MeanSpeed a emmo:EMMO_81369540_1b0e_471b_9bae_6801af22800e ;\n    emmo:EMMO_42806efc_581b_4ff8_81b0_b4d62153458b \"5.0 km/h\"^^emmo:EMMO_799c067b_083f_4365_9452_1f1433b03676 .\n&lt;BLANKLINE&gt;\n&lt;BLANKLINE&gt;\n</code></pre> <p>By default ureg.save_quantity() saves the quantity as an individual using the <code>emmo:SIQuantityDatatype</code> datatype. But, the ureg.save_quantity() method has also options for saving the quantity as a class (argument <code>tbox=True</code>) or to represent the quantity using the <code>emmo:hasNumericalPart</code> and <code>emmo:hasReferencePart</code> properties (argument <code>si_datatype=False</code>).</p> <p>Loading a quantity from the triplestore can be with ureg.load_quantity():</p> <pre><code>&gt;&gt;&gt; q = ureg.load_quantity(ts, iri=NS.MeanSpeed)\n&gt;&gt;&gt; q\n&lt;Quantity(5.0, 'KiloMetre / Hour')&gt;\n</code></pre>"},{"location":"units/units/#setting-up-custom-unit-registry","title":"Setting up custom unit registry","text":"<p>You can extend the default set of units by creating a domain or application ontology with additional custom units. It should import EMMO to get the default units.</p> <p>Use the <code>url</code> and <code>name</code> options when instantiating the unit registry</p> <pre><code>&gt;&gt;&gt; ureg = UnitRegistry(url=\"http://custom.org/myunits.ttl\", name=\"myunits-0.3.2\")  # doctest: +SKIP\n</code></pre> <p>where <code>url</code> is the URL or file path to the ontology and <code>name</code> is a, preferred versioned, name for the custom ontology used for caching.</p> <p>Typically you create the unit registry when initialising your application. After creating it, you can call the set_as_default() method.</p> <pre><code>&gt;&gt;&gt; ureg.set_as_default()  # doctest: +ELLIPSIS\n&lt;tripper.units.units.UnitRegistry object at 0x...&gt;\n</code></pre> <p>This will allow to retrieve the custom unit register from anywhere in your application using the get_ureg() function</p> <pre><code>&gt;&gt;&gt; from tripper.units import get_ureg\n&gt;&gt;&gt; ureg = get_ureg()\n</code></pre>"},{"location":"units/units/#tips-tricks","title":"Tips &amp; tricks","text":"<p>Tripper caches the ontology and Pint units definition file for performance reasons. If the ontology has been updated, you may need to clear the cache. That can either be done manually or by calling the ureg.clear_cache() method.</p> <p>For manual deletion of the cache files, the cache directory can be found using the <code>ureg._tripper_cachedir</code> attribute.</p>"}]}